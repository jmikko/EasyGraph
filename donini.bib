Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Maurer2012,
abstract = {We present a data dependent generalization bound for a large class of regularized algorithms which implement structured sparsity constraints. The bound can be applied to standard squared-norm regularization, the Lasso, the group Lasso, some versions of the group Lasso with overlapping groups, multiple kernel learning and other regularization schemes. In all these cases competitive results are obtained. A novel feature of our bound is that it can be applied in an infinite dimensional setting such as the Lasso in a separable Hilbert space or multiple kernel learning with a countable number of kernels.},
archivePrefix = {arXiv},
arxivId = {1108.3476},
author = {Maurer, Andreas and Pontil, Massimiliano},
eprint = {1108.3476},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maurer, Pontil - 2012 - Structured sparsity and generalization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {empirical processes,rademacher average,sparse estimation},
pages = {671--690},
title = {{Structured sparsity and generalization}},
url = {http://arxiv.org/abs/1108.3476$\backslash$nhttp://dl.acm.org/citation.cfm?id=2188408},
volume = {13},
year = {2012}
}
@article{Cortes2009c,
abstract = {This paper presents several novel generalization bounds for the problem of learning kernels based on the analysis of the Rademacher complexity of the corresponding hypothesis sets. Our bound for learning kernels with a convex combination of p base kernels has only a log(p) dependency on the number of kernels, p, which is considerably more favorable than the previous best bound given for the same problem. We also give a novel bound for learning with a linear combination of p base kernels with an L{\_}2 regularization whose dependency on p is only in p{\^{}}{\{}1/4{\}}.},
archivePrefix = {arXiv},
arxivId = {0912.3309},
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
eprint = {0912.3309},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0912.3309v1.pdf:pdf;:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes, Mohri, Rostamizadeh - 2009 - New Generalization Bounds for Learning Kernels.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
pages = {247----254},
title = {{Generalization bounds for learning kernels}},
url = {http://arxiv.org/abs/0912.3309},
year = {2010}
}
@article{Yu2015,
abstract = {Conventional vision algorithms adopt a single type of feature or a simple concatenation of multiple features, which is always represented in a high-dimensional space. In this paper, we propose a novel unsupervised spectral embedding algorithm called Kernelized Multiview Projection (KMP) to better fuse and embed different feature representations. Computing the kernel matrices from different features/views, KMP can encode them with the corresponding weights to achieve a low-dimensional and semantically meaningful subspace where the distribution of each view is sufficiently smooth and discriminative. More crucially, KMP is linear for the reproducing kernel Hilbert space (RKHS) and solves the out-of-sample problem, which allows it to be competent for various practical applications. Extensive experiments on three popular image datasets demonstrate the effectiveness of our multiview embedding algorithm.},
archivePrefix = {arXiv},
arxivId = {1508.00430},
author = {Yu, Mengyang and Liu, Li and Shao, Ling},
eprint = {1508.00430},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1508.00430.pdf:pdf},
title = {{Kernelized Multiview Projection}},
url = {http://arxiv.org/abs/1508.00430},
year = {2015}
}
@article{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
doi = {10.1145/1273496.1273598},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/PegasosMPB.pdf:pdf},
isbn = {9781595937933},
journal = {Proceedings of the 24th international conference on Machine learning - ICML '07},
keywords = {2000,first,mathematics subject classification,more,second,stochastic gradient descent,svm},
pages = {807--814},
title = {{Pegasos: Primal Estimated sub-GrAdient SOlver for SVM}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273598},
year = {2007}
}
@article{Suzuki2011,
abstract = {We propose a new optimization algorithm for Multiple Kernel Learning (MKL) called SpicyMKL, which is applicable to general convex loss functions and general types of regularization. The proposed SpicyMKL iteratively solves smooth minimization problems. Thus, there is no need of solving SVM, LP, or QP internally. SpicyMKL can be viewed as a proximal minimization method and converges super-linearly. The cost of inner minimization is roughly proportional to the number of active kernels. Therefore, when we aim for a sparse kernel combination, our algorithm scales well against increasing number of kernels. Moreover, we give a general block-norm formulation of MKL that includes non-sparse regularizations, such as elastic-net and ℓ p -norm regularizations. Extending SpicyMKL, we propose an efficient optimization method for the general regularization framework. Experimental results show that our algorithm is faster than existing methods especially when the number of kernels is large ({\textgreater}1000).},
author = {Suzuki, Taiji and Tomioka, Ryota},
doi = {10.1007/s10994-011-5252-9},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/SuzTom11.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Multiple kernel learning,Non-smooth optimization,Proximal minimization,Sparsity,Super-linear convergence},
number = {1-2},
pages = {77--108},
title = {{SpicyMKL: A fast algorithm for Multiple Kernel Learning with thousands of kernels}},
volume = {85},
year = {2011}
}
@article{Diaz-Morales2016a,
author = {D{\'{i}}az-Morales, Roberto and Navia-V{\'{a}}zquez, {\'{A}}ngel},
doi = {10.1016/j.neucom.2015.11.097},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0925231216000837-main.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Gaussian Process,Kernel methods,Matrix inversion,OpenMP,Parallel,Quadtree,Support Vector Machine},
publisher = {Elsevier},
title = {{Efficient Parallel Implementation of Kernel Methods}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231216000837},
year = {2016}
}
@article{Wauthier2013,
abstract = {The ranking of n objects based on pair- wise comparisons is a core machine learning problem, arising in recommender systems, ad placement, player ranking, biological appli- cations and others. In many practical sit- uations the true pairwise comparisons can- not be actively measured, but a subset of all n(n−1)/2 comparisons is passively and nois- ily observed. Optimization algorithms (e.g., the SVM) could be used to predict a rank- ing with fixed expected Kendall tau distance, while achieving an Ω(n) lower bound on the corresponding sample complexity. However, due to their centralized structure they are difficult to extend to online or distributed settings. In this paper we show that much simpler algorithms can match the same Ω(n) lower bound in expectation. Furthermore, if an average of O(nlog(n)) binary comparisons are measured, then one algorithm recovers the true ranking in a uniform sense, while the other predicts the ranking more accurately near the top than the bottom. We discuss extensions to online and distributed ranking, with benefits over traditional alternatives. 1.},
author = {Wauthier, Fabian L and Jordan, Michael I},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/wauthier13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {1--9},
title = {{Efficient Ranking from Pairwise Comparisons}},
volume = {28},
year = {2013}
}
@article{Hsu2002a,
abstract = {Support vector machines (SVMs) were originally designed for binary$\backslash$nclassification. How to effectively extend it for multiclass classification$\backslash$nis still an ongoing research issue. Several methods have been proposed$\backslash$nwhere typically we construct a multiclass classifier by combining$\backslash$nseveral binary classifiers. Some authors also proposed methods that$\backslash$nconsider all classes at once. As it is computationally more expensive$\backslash$nto solve multiclass problems, comparisons of these methods using$\backslash$nlarge-scale problems have not been seriously conducted. Especially$\backslash$nfor methods solving multiclass SVM in one step, a much larger optimization$\backslash$nproblem is required so up to now experiments are limited to small$\backslash$ndata sets. In this paper we give decomposition implementations for$\backslash$ntwo such "all-together" methods. We then compare their performance$\backslash$nwith three methods based on binary classifications: "one-against-all,"$\backslash$n"one-against-one," and directed acyclic graph SVM (DAGSVM). Our experiments$\backslash$nindicate that the "one-against-one" and DAG methods are more suitable$\backslash$nfor practical use than the other methods. Results also show that$\backslash$nfor large problems methods by considering all data at once in general$\backslash$nneed fewer support vectors},
author = {Hsu, Chih-Wei and Lin, Chih-Jen},
doi = {10.1109/72.991427},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/multisvm.pdf:pdf},
isbn = {3-540-32026-1},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {DAGSVM,SVMs,binary classifiers,decomposition,direc},
number = {2},
pages = {415--425},
pmid = {18244499},
title = {{A comparison of methods for multiclass support vector machines}},
volume = {13},
year = {2002}
}
@article{Pasa2014,
author = {Pasa, Luca and Testolin, Alberto and Sperduti, Alessandro},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/An{\_}HMM{\_}based{\_}PreTraining{\_}approach.pdf:pdf},
pages = {1--6},
title = {{An HMM-based Pre-training Approach for Sequential Data}},
year = {2014}
}
@article{Hahn2011,
abstract = {Context: Although psychiatric disorders are, to date, di-agnosed on the basis of behavioral symptoms and course of illness, the interest in neurobiological markers of psy-chiatric disorders has grown substantially in recent years. However, current classification approaches are mainly based on data from a single biomarker, making it diffi-cult to predict disorders characterized by complex pat-terns of symptoms. Objective: To integrate neuroimaging data associated with multiple symptom-related neural processes and dem-onstrate their utility in the context of depression by de-riving a predictive model of brain activation.},
author = {Hahn, Tim and Marquand, Andre F and Ehlis, Ann-Christine and Dresler, Thomas and Kittel-Schneider, Sarah and Jarczok, Tomasz A and Lesch, Klaus-Peter and Jakob, Peter M and Mourao-Miranda, Janaina and Brammer, Michael J and Fallgatter, Andreas J},
doi = {10.1001/archgenpsychiatry.2010.178},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Hahn{\_}et{\_}al{\_}2010.pdf:pdf},
issn = {0003-990X},
number = {4},
pages = {361--368},
title = {{Integrating Neurobiological Markers of Depression}},
volume = {68},
year = {2011}
}
@article{Cortes2013,
abstract = {We use the notion of local Rademacher complexity to design new algorithms for learning kernels. Our algorithms thereby benefit from the sharper learning bounds based on that notion which, under certain general conditions, guarantee a faster convergence rate. We devise two new learning kernel algorithms: one based on a convex optimization problem for which we give an efficient solution using ex- isting learning kernel techniques, and another one that can be formulated as a DC-programming problem for which we describe a solution in detail. We also re- port the results of experiments with both algorithms in both binary and multi-class classification tasks.},
author = {Cortes, Corinna and Kloft, Marius and Mohri, Mehryar},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/42029.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/nips2013.pdf:pdf;:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes - 2013 - Learning Kernels Using Local Rademacher Complexity.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/nips2013.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Learning Kernels Using Local Rademacher Complexity}},
year = {2013}
}
@article{Feng2015,
abstract = {The paper analyses the multiple kernel learning-based face recognition in pattern matching area. Based on the analysis of the basic theory of multiple kernel SVM, this thesis focuses on the multiple kernel SVM algorithm based on semi-infinite linear program (SILP), including SILP based on column generation (CG) and SILP based on chunking algo- rithm (CA). The two SILP improved algorithms are applied to several classification problems, including UCI binary clas- sification problem datasets and multi-classification problem datasets. Furthermore, the two SILP improved algorithms are applied to the actual problems of face recognition. The experiment data shows that with the multiple kernel learning-based method, the performance of face recognition can be obviously improved.},
author = {Feng, Shan},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/TOAUTOCJ-7-1796.pdf:pdf},
journal = {The Open Automation and Control Systems Journal},
keywords = {face recognition,kernel functions,multiple kernel learning-based,pattern recognition,semi-infinite linear program (SILP).},
pages = {1796--1801},
title = {{The Research on the Multiple Kernel Learning-based Face Recognition in Pattern Matching}},
year = {2015}
}
@article{Davis2007,
abstract = {In this paper,we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under con- straints on the distance function. We express this problem as a particular Bregman optimiza- tion problem—that ofminimizing the LogDet di- vergence subject to linear constraints. Our result- ing algorithm has several advantages over exist- ingmethods. First, ourmethod can handle awide variety of constraints and can optionally incorpo- rate a prior on the distance function. Second, it is fast and scalable. Unlike most existing meth- ods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of met- ric learning for nearest neighbor classification, as well as on standard data sets.},
author = {Davis, Jason V. and Kulis, Brian and Jain, Prateek and Sra, Suvrit and Dhillon, Inderjit S.},
doi = {10.1145/1273496.1273523},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/metriclearning{\_}icml.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/p209-davis.pdf:pdf},
isbn = {9781595937933},
issn = {07413335},
journal = {Icml},
pages = {209--216},
title = {{Information-theoretic metric learning}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273523},
year = {2007}
}
@inproceedings{Aiolli2008,
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-87536-9_32},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aiolli, Da San Martino, Sperduti - 2008 - A kernel method for the optimization of the margin distribution.pdf:pdf},
isbn = {3540875352},
issn = {03029743},
keywords = {Game theory,Kernel methods,Margin distribution},
pages = {305--314},
title = {{A kernel method for the optimization of the margin distribution}},
volume = {5163 LNCS},
year = {2008}
}
@article{Mika1999,
abstract = {Kernel PCA as a nonlinear feature extractor has proven powerful as a preprocessing step for classification algorithms. But it can also be con- sidered as a natural generalization of linear principal component anal- ysis. This gives rise to the question how to use nonlinear features for data compression, reconstruction, and de-noising, applications common in linear PCA. This is a nontrivial task, as the results provided by ker- nel PCA live in some high dimensional feature space and need not have pre-images in input space. This work presents ideas for finding approxi- mate pre-images, focusing on Gaussian kernels, and shows experimental results using these pre-images in data reconstruction and de-noising on toy examples as well as on real world data.},
author = {Mika, Sebastian and Sch{\"{o}}lkopf, Bernhard and Smola, Alex and M{\"{u}}ller, Klaus-robert and Scholz, Matthias and R{\"{a}}tsch, Gunnar},
doi = {10.1.1.88.5268},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mika et al. - 1999 - Kernel PCA and De-Noising in Feature Spaces.pdf:pdf},
isbn = {0262112450},
issn = {10495258},
journal = {Analysis},
number = {i},
pages = {536--542},
title = {{Kernel PCA and De-Noising in Feature Spaces}},
url = {http://trac.assembla.com/mlea2{\_}cpa/export/56/doc/kpca-de-noising-feature-space.pdf},
volume = {11},
year = {1999}
}
@article{Bottou2012,
abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
author = {Bottou, Leon},
doi = {10.1007/978-3-642-35289-8_25},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/tricks-2012.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {2045-2322},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {421--436},
pmid = {25382349},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
volume = {1},
year = {2012}
}
@article{Goldstein2014,
abstract = {Alternating direction methods are a common tool for general mathematical programming and optimization. These methods have become particularly important in the field of variational image processing, which frequently requires the minimization of nondifferentiable objectives. This paper considers accelerated (i.e., fast) variants of two common alternating direction methods: the alternating direction method of multipliers (ADMM) and the alternating minimization algorithm (AMA). The proposed acceleration is of the form first proposed by Nesterov for gradient descent methods. In the case that the objective function is strongly convex, global convergence bounds are provided for both classical and accelerated variants of the methods. Numerical examples are presented to demonstrate the superior performance of the fast methods for a wide variety of problems. Read More: http://epubs.siam.org/doi/abs/10.1137/120896219},
author = {Goldstein, T O M and Donoghue, Brendan O and Setzer, Simon and Baraniuk, Richard},
doi = {10.1137/120896219},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/cam12-35.pdf:pdf},
issn = {1936-4954},
journal = {SIAM Journal of Imaging Sciences},
number = {3},
pages = {1588--1623},
title = {{Fast alternating direction optimization methods}},
volume = {7},
year = {2014}
}
@article{Chu2012,
abstract = {There are growing numbers of studies using machine learning approaches to characterize patterns of anatomical difference discernible from neuroimaging data. The high-dimensionality of image data often raises a concern that feature selection is needed to obtain optimal accuracy. Among previous studies, mostly using fixed sample sizes, some show greater predictive accuracies with feature selection, whereas others do not. In this study, we compared four common feature selection methods. 1) Pre-selected region of interests (ROIs) that are based on prior knowledge. 2) Univariate t-test filtering. 3) Recursive feature elimination (RFE), and 4) t-test filtering constrained by ROIs. The predictive accuracies achieved from different sample sizes, with and without feature selection, were compared statistically. To demonstrate the effect, we used grey matter segmented from the T1-weighted anatomical scans collected by the Alzheimer's disease Neuroimaging Initiative (ADNI) as the input features to a linear support vector machine classifier. The objective was to characterize the patterns of difference between Alzheimer's disease (AD) patients and cognitively normal subjects, and also to characterize the difference between mild cognitive impairment (MCI) patients and normal subjects. In addition, we also compared the classification accuracies between MCI patients who converted to AD and MCI patients who did not convert within the period of 12. months. Predictive accuracies from two data-driven feature selection methods (t-test filtering and RFE) were no better than those achieved using whole brain data. We showed that we could achieve the most accurate characterizations by using prior knowledge of where to expect neurodegeneration (hippocampus and parahippocampal gyrus). Therefore, feature selection does improve the classification accuracies, but it depends on the method adopted. In general, larger sample sizes yielded higher accuracies with less advantage obtained by using knowledge from the existing literature. ?? 2011 Elsevier Inc.},
author = {Chu, Carlton and Hsu, Ai Ling and Chou, Kun Hsien and Bandettini, Peter and Lin, ChingPo},
doi = {10.1016/j.neuroimage.2011.11.066},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S1053811911013486-main.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Diseases classification,Feature selection,Recursive feature elimination (RFE),Support vector machine (SVM)},
number = {1},
pages = {59--70},
pmid = {22166797},
publisher = {Elsevier Inc.},
title = {{Does feature selection improve classification accuracy? Impact of sample size and feature selection on classification using anatomical magnetic resonance images}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2011.11.066},
volume = {60},
year = {2012}
}
@article{Yang2012a,
author = {Yang, Tianbao and Mahdavi, Mehrdad and Jin, Rong and Zhang, Lijun and Zhou, Yang},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/127.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {29th International Conference on Machine Learning},
pages = {233--240},
title = {{Multiple kernel learning from noisy labels by stochastic programming}},
url = {http://arxiv.org/abs/1206.4629},
year = {2012}
}
@article{Hinton2006,
abstract = {We show how to use " complementary priors " to eliminate the explaining away effects that make inference difficult in densely-connected belief nets that have many hidden layers. Using com-plementary priors, we derive a fast, greedy algo-rithm that can learn deep, directed belief networks one layer at a time, provided the top two lay-ers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights us-ing a contrastive version of the wake-sleep algo-rithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit im-ages and their labels. This generative model gives better digit classification than the best discrimi-native learning algorithms. The low-dimensional manifolds on which the digits lie are modelled by long ravines in the free-energy landscape of the top-level associative memory and it is easy to ex-plore these ravines by using the directed connec-tions to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/fastnc.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
pages = {1527--1554},
pmid = {16764513},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
volume = {1554},
year = {2006}
}
@article{Renyi1961,
abstract = {We shall investigate the relationships between the thermodynamic entropy and information theory and the implications that can be drawn for the arrow of time. This demands a careful study of classical thermodynamics and a review of its fundamental concepts. The statistical mechanical properties of time-dependent systems will be carefully studied, and the point at which the arrow of time appears will be described.},
archivePrefix = {arXiv},
arxivId = {1101.3070},
author = {R{\'{e}}nyi, a},
doi = {10.1021/jp106846b},
eprint = {1101.3070},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/euclid.bsmsp.1200512181.pdf:pdf},
isbn = {0097-0433},
issn = {15205207},
journal = {Entropy},
number = {c},
pages = {547--561},
pmid = {20961075},
title = {{On measures of entropy and information}},
url = {http://www.maths.gla.ac.uk/{~}tl/Renyi.pdf},
volume = {547},
year = {1961}
}
@article{Xu2013a,
abstract = {In recent years, a great many methods of learning from multi-view data by considering the diversity of different views have been proposed. These views may be obtained from multiple sources or different feature subsets. In trying to organize and highlight similarities and differences between the variety of multi-view learning approaches, we review a number of representative multi-view learning algorithms in different areas and classify them into three groups: 1) co-training, 2) multiple kernel learning, and 3) subspace learning. Notably, co-training style algorithms train alternately to maximize the mutual agreement on two distinct views of the data; multiple kernel learning algorithms exploit kernels that naturally correspond to different views and combine kernels either linearly or non-linearly to improve learning performance; and subspace learning algorithms aim to obtain a latent subspace shared by multiple views by assuming that the input views are generated from this latent subspace. Though there is significant variance in the approaches to integrating multiple views to improve learning performance, they mainly exploit either the consensus principle or the complementary principle to ensure the success of multi-view learning. Since accessing multiple views is the fundament of multi-view learning, with the exception of study on learning a model from multiple views, it is also valuable to study how to construct multiple views and how to evaluate these views. Overall, by exploring the consistency and complementary properties of different views, multi-view learning is rendered more effective, more promising, and has better generalization ability than single-view learning.},
archivePrefix = {arXiv},
arxivId = {1304.5634},
author = {Xu, Chang and Tao, Dacheng and Xu, Chao},
eprint = {1304.5634},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/A Survey on Multi-view Learning.pdf:pdf},
pages = {1--59},
title = {{A Survey on Multi-view Learning}},
url = {http://arxiv.org/abs/1304.5634},
year = {2013}
}
@article{Maurer2006,
abstract = {We give dimension-free and data-dependent bounds for linear multi-task learning where a common linear operator is chosen to preprocess data for a vector of task specific linear-thresholding classifiers. The complexity penalty of multi-task learning is bounded by a simple expression involving the margins of the task-specific classifiers, the Hilbert-Schmidt norm of the selected preprocessor and the Hilbert-Schmidt norm of the covariance operator for the total mixture of all task distributions, or, alternatively, the Frobenius norm of the total Gramian matrix for the data-dependent version. The results can be compared to state-of-the-art results on linear single-task learning.},
author = {Maurer, a},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/maurer06a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {learning to learn,multi-task learning,transfer learning},
pages = {117--139},
title = {{Bounds for linear multi-task learning}},
url = {{\textless}Go to ISI{\textgreater}://000236331400005},
volume = {7},
year = {2006}
}
@article{Wang2015,
abstract = {Kernel Canonical correlation analysis (KCCA) is a fundamental method with broad applicability in statistics and machine learning. Although there exist closed-form solution to the KCCA objective by solving an {\$}N\backslashtimes N{\$} eigenvalue system where {\$}N{\$} is the training set size, the computational requirements of this approach in both memory and time prohibit its usage in the large scale. Various approximation techniques have been developed for KCCA. A recently proposed approach is to first transform original inputs to a {\$}M{\$}-dimensional feature space using random kitchen sinks so that inner product in the feature space approximates the kernel function, and then apply linear CCA to the transformed inputs. In challenging applications, however, the dimensionality {\$}M{\$} of the feature space may need to be very large in order to reveal the nonlinear correlations, and then it becomes non-trivial to solve linear CCA for data matrices of very high dimensionality. We propose to use the recently proposed stochastic optimization algorithm for linear CCA and its neural-network extension to further alleviate the computation requirements of approximate KCCA. This approach allows us to run approximate KCCA on a speech dataset with {\$}1.4{\$} million training samples and random feature space of dimensionality {\$}M=100000{\$} on a normal workstation.},
archivePrefix = {arXiv},
arxivId = {1511.04773},
author = {Wang, Weiran and Livescu, Karen},
eprint = {1511.04773},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1511.04773.pdf:pdf},
number = {1},
pages = {1--13},
title = {{Large-Scale Approximate Kernel Canonical Correlation Analysis}},
url = {http://arxiv.org/abs/1511.04773},
year = {2015}
}
@article{Deng2015,
abstract = {We analyze the convergence rate of the alternating direction method of multipliers (ADMM) for minimizing the sum of two or more nonsmooth convex separable functions subject to linear constraints. Previous analysis of the ADMM typically assumes that the objective function is the sum of only two convex functions defined on two separable blocks of variables even though the algorithm works well in numerical experiments for three or more blocks. Moreover, there has been no rate of convergence analysis for the ADMM without strong convexity in the objective function. In this paper we establish the global linear convergence of the ADMM for minimizing the sum of any number of convex separable functions. This result settles a key question regarding the convergence of the ADMM when the number of blocks is more than two or if the strong convexity is absent. It also implies the linear convergence of the ADMM for several contemporary applications including LASSO, Group LASSO and Sparse Group LASSO without any strong convexity assumption. Our proof is based on estimating the distance from a dual feasible solution to the optimal dual solution set by the norm of a certain proximal residual, and by requiring the dual stepsize to be sufficiently small.},
archivePrefix = {arXiv},
arxivId = {1208.3922},
author = {Deng, Wei and Yin, Wotao},
doi = {10.1007/s10915-015-0048-x},
eprint = {1208.3922},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/cam12-52.pdf:pdf},
issn = {1573-7691},
journal = {Journal of Scientific Computing$\backslash$},
keywords = {49,90,alternating directions of multipliers,ams,by the national science,computer engineering,department of electrical and,dual ascent,error bound,foundation,grant number dms-1015346,linear convergence,minneapolis,mn 55455,mos,subject classifications,the research is supported,university of minnesota,usa},
title = {{On the Global and Linear Convergence of the Generalized Alternating Direction Method of Multipliers}},
url = {http://arxiv.org/abs/1208.3922},
volume = {77005},
year = {2015}
}
@article{Young2014,
abstract = {We demonstrate the use of a probabilistic generative model to explore the biomarker changes occurring as Alzheimer's disease develops and progresses. We enhanced the recently introduced event-based model for use with a multi-modal sporadic disease data set. This allows us to determine the sequence in which Alzheimer's disease biomarkers become abnormal without reliance on a priori clinical diagnostic information or explicit biomarker cut points. The model also characterizes the uncertainty in the ordering and provides a natural patient staging system. Two hundred and eighty-five subjects (92 cognitively normal, 129 mild cognitive impairment, 64 Alzheimer's disease) were selected from the Alzheimer's Disease Neuroimaging Initiative with measurements of 14 Alzheimer's disease-related biomarkers including cerebrospinal fluid proteins, regional magnetic resonance imaging brain volume and rates of atrophy measures, and cognitive test scores. We used the event-based model to determine the sequence of biomarker abnormality and its uncertainty in various population subgroups. We used patient stages assigned by the event-based model to discriminate cognitively normal subjects from those with Alzheimer's disease, and predict conversion from mild cognitive impairment to Alzheimer's disease and cognitively normal to mild cognitive impairment. The model predicts that cerebrospinal fluid levels become abnormal first, followed by rates of atrophy, then cognitive test scores, and finally regional brain volumes. In amyloid-positive (cerebrospinal fluid amyloid-$\beta$1-42 {\textless} 192 pg/ml) or APOE-positive (one or more APOE4 alleles) subjects, the model predicts with high confidence that the cerebrospinal fluid biomarkers become abnormal in a distinct sequence: amyloid-$\beta$1-42, phosphorylated tau, total tau. However, in the broader population total tau and phosphorylated tau are found to be earlier cerebrospinal fluid markers than amyloid-$\beta$1-42, albeit with more uncertainty. The model's staging system strongly separates cognitively normal and Alzheimer's disease subjects (maximum classification accuracy of 99{\%}), and predicts conversion from mild cognitive impairment to Alzheimer's disease (maximum balanced accuracy of 77{\%} over 3 years), and from cognitively normal to mild cognitive impairment (maximum balanced accuracy of 76{\%} over 5 years). By fitting Cox proportional hazards models, we find that baseline model stage is a significant risk factor for conversion from both mild cognitive impairment to Alzheimer's disease (P = 2.06 × 10(-7)) and cognitively normal to mild cognitive impairment (P = 0.033). The data-driven model we describe supports hypothetical models of biomarker ordering in amyloid-positive and APOE-positive subjects, but suggests that biomarker ordering in the wider population may diverge from this sequence. The model provides useful disease staging information across the full spectrum of disease progression, from cognitively normal to mild cognitive impairment to Alzheimer's disease. This approach has broad application across neurodegenerative disease, providing insights into disease biology, as well as staging and prognostication.},
author = {Young, Alexandra L. and Oxtoby, Neil P. and Daga, Pankaj and Cash, David M. and Fox, Nick C. and Ourselin, Sebastien and Schott, Jonathan M. and Alexander, Daniel C.},
doi = {10.1093/brain/awu176},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2564.full.pdf:pdf},
isbn = {0006-8950},
issn = {14602156},
journal = {Brain},
keywords = {Alzheimer's disease,Biomarker ordering,Biomarkers,Disease progression,Event-based model},
number = {9},
pages = {2564--2577},
pmid = {25012224},
title = {{A data-driven model of biomarker changes in sporadic Alzheimer's disease}},
volume = {137},
year = {2014}
}
@article{Demmel2007,
abstract = {In an earlier paper, we showed that a large class of fast recursive matrix multiplication algorithms is stable in a normwise sense, and that in fact if multiplication of {\$}n{\$}-by-{\$}n{\$} matrices can be done by any algorithm in {\$}O(n{\^{}}{\{}\backslashomega + \backslasheta{\}}){\$} operations for any {\$}\backslasheta {\textgreater} 0{\$}, then it can be done stably in {\$}O(n{\^{}}{\{}\backslashomega + \backslasheta{\}}){\$} operations for any {\$}\backslasheta {\textgreater} 0{\$}. Here we extend this result to show that essentially all standard linear algebra operations, including LU decomposition, QR decomposition, linear equation solving, matrix inversion, solving least squares problems, (generalized) eigenvalue problems and the singular value decomposition can also be done stably (in a normwise sense) in {\$}O(n{\^{}}{\{}\backslashomega + \backslasheta{\}}){\$} operations.},
archivePrefix = {arXiv},
arxivId = {math/0612264},
author = {Demmel, James and Dumitriu, Ioana and Holtz, Olga},
doi = {10.1007/s00211-007-0114-x},
eprint = {0612264},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0612264v3.pdf:pdf},
issn = {0029599X},
journal = {Numerische Mathematik},
number = {1},
pages = {59--91},
primaryClass = {math},
title = {{Fast linear algebra is stable}},
volume = {108},
year = {2007}
}
@article{Anguita2012,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C06.pdf:pdf},
isbn = {9782874190490},
journal = {Esann},
number = {April},
pages = {25--27},
title = {{Structural Risk Minimization and Rademacher Complexity for Regression}},
year = {2012}
}
@article{Gonen2013,
abstract = {Instead of selecting a single kernel, multiple kernel learning (MKL) uses a weighted sum of kernels where the weight of each kernel is optimized during training. Such methods assign the same weight to a kernel over the whole input space, and we discuss localized multiple kernel learning (LMKL) that is composed of a kernel-based learning algorithm and a parametric gating model to assign local weights to kernel functions. These two components are trained in a coupled manner using a two-step alternating optimization algorithm. Empirical results on benchmark classification and regression data sets validate the applicability of our approach. We see that LMKL achieves higher accuracy compared with canonical MKL on classification problems with different feature representations. LMKL can also identify the relevant parts of images using the gating model as a saliency detector in image recognition problems. In regression tasks, LMKL improves the performance significantly or reduces the model complexity by storing significantly fewer support vectors. {\textcopyright} 2012 Elsevier Ltd.},
author = {G{\"{o}}nen, Mehmet and Alpaydin, Ethem},
doi = {10.1016/j.patcog.2012.09.002},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/gonen{\_}pr13{\_}paper.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Classification,Multiple kernel learning,Regression,Selective attention,Support vector machines,Support vector regression},
number = {3},
pages = {795--807},
title = {{Localized algorithms for multiple kernel learning}},
volume = {46},
year = {2013}
}
@book{Micchelli2013,
abstract = {We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. This problem is relevant in machine learning, statistics and signal processing. It is well known that a linear regression can benefit from knowledge that the underlying regression vector is sparse. The combinatorial problem of selecting the nonzero components of this vector can be "relaxed" by regularizing the squared error with a convex penalty function like the {\$}\backslashell{\_}1{\$} norm. However, in many applications, additional conditions on the structure of the regression vector and its sparsity pattern are available. Incorporating this information into the learning method may lead to a significant decrease of the estimation error. In this paper, we present a family of convex penalty functions, which encode prior knowledge on the structure of the vector formed by the absolute values of the regression coefficients. This family subsumes the {\$}\backslashell{\_}1{\$} norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish the basic properties of these penalty functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso method and other related methods.},
archivePrefix = {arXiv},
arxivId = {1010.0556},
author = {Micchelli, Charles A. and Morales, Jean M. and Pontil, Massimiliano},
booktitle = {Advances in Computational Mathematics},
doi = {10.1007/s10444-011-9245-9},
eprint = {1010.0556},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2Fs10444-011-9245-9.pdf:pdf},
isbn = {1044401192},
issn = {10197168},
keywords = {Convex optimization,Feature selection,LASSO,Linear regression,Regularization,Sparse estimation},
number = {3},
pages = {455--489},
title = {{Regularizers for structured sparsity}},
volume = {38},
year = {2013}
}
@article{Damoulas2008,
abstract = {MOTIVATION: The problems of protein fold recognition and remote homology detection have recently attracted a great deal of interest as they represent challenging multi-feature multi-class problems for which modern pattern recognition methods achieve only modest levels of performance. As with many pattern recognition problems, there are multiple feature spaces or groups of attributes available, such as global characteristics like the amino-acid composition (C), predicted secondary structure (S), hydrophobicity (H), van der Waals volume (V), polarity (P), polarizability (Z), as well as attributes derived from local sequence alignment such as the Smith-Waterman scores. This raises the need for a classification method that is able to assess the contribution of these potentially heterogeneous object descriptors while utilizing such information to improve predictive performance. To that end, we offer a single multi-class kernel machine that informatively combines the available feature groups and, as is demonstrated in this article, is able to provide the state-of-the-art in performance accuracy on the fold recognition problem. Furthermore, the proposed approach provides some insight by assessing the significance of recently introduced protein features and string kernels. The proposed method is well-founded within a Bayesian hierarchical framework and a variational Bayes approximation is derived which allows for efficient CPU processing times. RESULTS: The best performance which we report on the SCOP PDB-40D benchmark data-set is a 70{\%} accuracy by combining all the available feature groups from global protein characteristics but also including sequence-alignment features. We offer an 8{\%} improvement on the best reported performance that combines multi-class k-nn classifiers while at the same time reducing computational costs and assessing the predictive power of the various available features. Furthermore, we examine the performance of our methodology on the SCOP 1.53 benchmark data-set that simulates remote homology detection and examine the combination of various state-of-the-art string kernels that have recently been proposed.},
author = {Damoulas, Theodoros and Girolami, Mark a.},
doi = {10.1093/bioinformatics/btn112},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Damoulas, Girolami - 2008 - Probabilistic multi-class multi-kernel learning On protein fold recognition and remote homology detection.pdf:pdf},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {10},
pages = {1264--1270},
pmid = {18378524},
title = {{Probabilistic multi-class multi-kernel learning: On protein fold recognition and remote homology detection}},
volume = {24},
year = {2008}
}
@article{Wang2015a,
author = {Wang, Tinghua and Xie, Haihui and Zhong, Liyun and Hu, Shengzhou},
doi = {10.1166/jctn.2015.3998},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/wang2015.pdf:pdf},
issn = {15461955},
journal = {Journal of Computational and Theoretical Nanoscience},
keywords = {04 nov 2015 01,10,133,23,24,50,81 on,delivered by publishing technology,ip,kernel method,machine learning,mkl,multiple kernel learning,rice university,support vector machine,svm,text categorization,to,wed},
number = {9},
pages = {2121--2126},
title = {{A Multiple Kernel Learning Approach to Text Categorization}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=1546-1955{\&}volume=12{\&}issue=9{\&}spage=2121},
volume = {12},
year = {2015}
}
@article{Bengio2004,
abstract = {In this letter, we show a direct relation between spectral embedding methods and kernel principal components analysis and how both are special cases of a more general learning problem: learning the principal eigenfunctions of an operator defined from a kernel and the unknown data-generating density. Whereas spectral embedding methods provided only coordinates for the training points, the analysis justifies a simple extension to out-of-sample examples (the Nystr{\"{o}}m formula) for multidimensional scaling (MDS), spectral clustering, Laplacian eigenmaps, locally linear embedding (LLE), and Isomap. The analysis provides, for all such spectral embedding methods, the definition of a loss function, whose empirical average is minimized by the traditional algorithms. The asymptotic expected value of that loss defines a generalization performance and clarifies what these algorithms are trying to learn. Experiments with LLE, Isomap, spectral clustering, and MDS show that this out-of-sample embedding formula generalizes well, with a level of error comparable to the effect of small perturbations of the training set on the embedding.},
author = {Bengio, Yoshua and Delalleau, Olivier and {Le Roux}, Nicolas and Paiement, Jean-Fran{\c{c}}ois and Vincent, Pascal and Ouimet, Marie},
doi = {10.1162/0899766041732396},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2004 - Learning eigenfunctions links spectral embedding and kernel PCA.pdf:pdf},
isbn = {0899766041732396},
issn = {0899-7667},
journal = {Neural computation},
pages = {2197--2219},
pmid = {15333211},
title = {{Learning eigenfunctions links spectral embedding and kernel PCA.}},
volume = {16},
year = {2004}
}
@article{Ngiam2011,
abstract = {Unsupervised feature learning has been shown to be effective at learning representations that perform well on image, video and audio classiﬁcation. However, many existing feature learning algorithms are hard to use and require extensive hyperparameter tuning. In this work, we present sparse ﬁltering, a simple new algorithm which is efﬁcient and only has one hyperparameter, the number of features to learn. In contrast to most other feature learning methods, sparse ﬁltering does not explicitly attempt to construct a model of the data distribution. Instead, it optimizes a simple cost function – the sparsity of `2-normalized features – which can easily be implemented in a few lines of MATLAB code. Sparse ﬁltering scales gracefully to handle high-dimensional inputs, and can also be used to learn meaningful features in additional layers with greedy layer-wise stacking. We evaluate sparse ﬁltering on natural images, object classiﬁcation (STL-10), and phone classiﬁcation (TIMIT), and show that our method works well on a range of different modalities.},
author = {Ngiam, Jiquan and Koh, Pw and Chen, Zhenghao and Bhaskar, Sa and Ng, Ay},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/nips11-SparseFiltering.pdf:pdf},
isbn = {9781618395993},
journal = {Nips},
pages = {1--9},
title = {{Sparse Filtering.}},
url = {https://papers.nips.cc/paper/4334-sparse-filtering.pdf},
year = {2011}
}
@article{Bengio2005,
abstract = {We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms based on local kernels are sensitive to the curse of dimensionality. These include local manifold learning algorithms such as {\{}Isomap{\}} and {\{}LLE{\}}, support vector classifiers with Gaussian or other local kernels, and graph-based semisupervised learning algorithms using a local similarity function. These algorithms are shown to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. There$\backslash$nis a large class of data distributions for which non-local solutions could be expressed compactly and potentially be learned with few examples, but which will require a large number of local bases and therefore a large number of training examples when using a local learning algorithm.},
author = {Bengio, Yoshua and Bengio, Yoshua and Delalleau, Olivier and Delalleau, Olivier and Roux, Nicolas Le and Roux, Nicolas Le and Branch, Downtown and Branch, Downtown},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/tr1258.pdf:pdf},
number = {2},
pages = {1--17},
title = {{The Curse of Dimensionality for Local Kernel Machines}},
volume = {2},
year = {2005}
}
@article{Qinghui2016,
author = {Qinghui, Hu and Shiwei, Wei and Zhiyuan, Li and Xiaogang, Liu},
doi = {10.1016/j.neucom.2016.01.079},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S092523121600240X-main.pdf:pdf},
isbn = {8613077692},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Alternating optimization,Multiple kernel learning,Quasi-Newton method,multiple kernel learning},
pages = {1--9},
publisher = {Elsevier},
title = {{Quasi-newton method for LP multiple kernel learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S092523121600240X},
year = {2016}
}
@article{Cortes2010,
abstract = {This paper examines two-stage techniques for learning kernels based on a notion of alignment. It presents a number of novel theoretical, al- gorithmic, and empirical results for alignment- based techniques. Our results build on previous work by Cristianini et al. (2001), but we adopt a different definition of kernel alignment and significantly extend that work in several direc- tions: we give a novel and simple concentration bound for alignment between kernel matrices; show the existence of good predictors for ker- nels with high alignment, both for classification and for regression; give algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP; and re- port the results of extensive experimentswith this alignment-based method in classification and re- gression tasks, which showan improvement both over the uniformcombination of kernels and over other state-of-the-art learning kernel methods. 1.},
author = {Cortes, Corinna},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes - 2010 - Two-stage learning kernel algorithms.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML'10)},
pages = {239--246},
title = {{Two-stage learning kernel algorithms}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2010{\_}CortesMR10.pdf},
year = {2010}
}
@article{Hino2010,
abstract = {Kernel methods have been successfully used in many practical machine learning problems. Choosing a suitable kernel is left to the practitioner. A common way to an automatic selection of optimal kernels is to learn a linear combination of element kernels. In this paper, a novel framework of multiple kernel learning is proposed based on conditional entropy minimization criterion. For the proposed framework, three multiple kernel learning algorithms are derived. The algorithms are experimentally shown to be comparable to or outperform kernel Fisher discriminant analysis and other multiple kernel learning algorithms on benchmark data sets.},
author = {Hino, Hideitsu and Reyhani, Nima and Murata, Noboru},
doi = {10.1109/ICMLA.2010.40},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/05708837.pdf:pdf},
isbn = {9780769543000},
journal = {Proceedings - 9th International Conference on Machine Learning and Applications, ICMLA 2010},
keywords = {Discriminant analysis,Entropy,Kernel methods,Multiple Kernel Learning},
pages = {223--228},
title = {{Multiple kernel learning by conditional entropy minimization}},
year = {2010}
}
@article{Ying2015,
abstract = {Pairwise learning usually refers to a learning task which involves a loss function depending on pairs of examples, among which most notable ones include ranking, metric learning and AUC maximization. In this paper, we study an online algorithm for pairwise learning with a least-square loss function in an unconstrained setting of a reproducing kernel Hilbert space (RKHS), which we refer to as the Online Pairwise lEaRning Algorithm (OPERA). In contrast to existing works $\backslash$cite{\{}Kar,Wang{\}} which require that the iterates are restricted to a bounded domain or the loss function is strongly-convex, OPERA is associated with a non-strongly convex objective function and learns the target function in an unconstrained RKHS. Specifically, we establish a general theorem which guarantees the almost surely convergence for the last iterate of OPERA without any assumptions on the underlying distribution. Explicit convergence rates are derived under the condition of polynomially decaying step sizes. We also establish an interesting property for a family of widely-used kernels in the setting of pairwise learning and illustrate the above convergence results using such kernels. Our methodology mainly depends on the characterization of RKHSs using its associated integral operators and probability inequalities for random variables with values in a Hilbert space.},
archivePrefix = {arXiv},
arxivId = {1502.07229},
author = {Ying, Yiming and Zhou, Ding-Xuan},
eprint = {1502.07229},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1502.07229v1.pdf:pdf},
title = {{Online Pairwise Learning Algorithms with Kernels}},
url = {http://arxiv.org/abs/1502.07229},
year = {2015}
}
@article{B2015,
author = {B, Anastasia Pentina and Ben-david, Shai},
doi = {10.1007/978-3-319-24486-0},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/micio1.pdf:pdf},
isbn = {978-3-319-24485-3},
keywords = {kernel learning,lifelong learning,multi-task learning},
pages = {194--208},
title = {{Algorithmic Learning Theory}},
url = {http://link.springer.com/10.1007/978-3-319-24486-0},
volume = {9355},
year = {2015}
}
@article{Cortes1995,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/BF00994018},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2FBF00994018.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {9052598814225336358},
title = {{Support-vector networks}},
url = {http://link.springer.com/10.1007/BF00994018},
volume = {20},
year = {1995}
}
@article{Serre2005,
abstract = { We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.},
author = {Serre, Thomas and Wolf, Lior and Poggio, Tomaso},
doi = {10.1109/CVPR.2005.254},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/serre-PID73457-05.pdf:pdf},
isbn = {0769523722},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {994--1000},
pmid = {8476227},
title = {{Object recognition with features inspired by visual cortex}},
volume = {2},
year = {2005}
}
@article{Vishwanathan2010,
abstract = {Our objective is to train p-norm Multiple Kernel Learning (MKL) and,$\backslash$nmore generally,$\backslash$n$\backslash$nlinear MKL regularised by the Bregman divergence, using the Sequential$\backslash$n$\backslash$nMinimal Optimization (SMO) algorithm. The SMO algorithm is simple,$\backslash$neasy to$\backslash$n$\backslash$nimplement and adapt, and efficiently scales to large problems. As$\backslash$na result, it has$\backslash$n$\backslash$ngained widespread acceptance and SVMs are routinely trained using$\backslash$nSMO in diverse$\backslash$n$\backslash$nreal world applications. Training using SMO has been a long standing$\backslash$ngoal$\backslash$n$\backslash$nin MKL for the very same reasons. Unfortunately, the standard MKL$\backslash$ndual is not$\backslash$n$\backslash$ndifferentiable, and therefore can not be optimised using SMO style$\backslash$nco-ordinate ascent.$\backslash$n$\backslash$nIn this paper, we demonstrate that linear MKL regularised with the$\backslash$np-norm$\backslash$n$\backslash$nsquared, or with certain Bregman divergences, can indeed be trained$\backslash$nusing SMO.$\backslash$n$\backslash$nThe resulting algorithm retains both simplicity and efficiency and$\backslash$nis significantly$\backslash$n$\backslash$nfaster than state-of-the-art specialised p-norm MKL solvers. We show$\backslash$nthat we can$\backslash$n$\backslash$ntrain on a hundred thousand kernels in approximately seven minutes$\backslash$nand on fifty$\backslash$n$\backslash$nthousand points in less than half an hour on a single core.},
author = {Vishwanathan, Svn and Sun, Zhaonan},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vishwanathan, Sun - 2010 - Multiple Kernel Learning and the SMO Algorithm.pdf:pdf},
isbn = {9781617823800},
journal = {Advances in Neural Information Processing Systems},
pages = {9},
title = {{Multiple Kernel Learning and the SMO Algorithm.}},
url = {https://papers.nips.cc/paper/3985-multiple-kernel-learning-and-the-smo-algorithm.pdf},
year = {2010}
}
@article{Gonen2008,
author = {G{\"{o}}nen, Mehmet and Alpaydin, Ethem},
doi = {10.1145/1390156.1390201},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/Localized Multiple Kernel Learning.pdf:pdf},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
number = {x},
pages = {352--359},
title = {{Localized multiple kernel learning}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390201},
year = {2008}
}
@article{Bengio2004a,
abstract = {The annual Neural Information Processing (NIPS) conference is the flagship meeting on neural computation. It draws a diverse group of attendees—physicists, neuroscientists, mathematicians, statisticians, and computer scientists. The presentations are interdisciplinary, with contributions in algorithms, learning theory, cognitive science, neuroscience, brain imaging, vision, speech and signal processing, reinforcement learning and control, emerging technologies, and applications. Only thirty percent of the papers submitted are accepted for presentation at NIPS, so the quality is exceptionally high. This volume contains all the papers presented at the 2003 conference.},
author = {Bengio, Yoshua and Paiement, Jean-Francois and Vincent, Pascal and Delalllaux, Olivier and {Le Roux}, Nicholas and Ouimet, Marie},
doi = {10.1.1.9.3475},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio et al. - 2004 - Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps and Spectral Clustering.pdf:pdf},
isbn = {0262201526},
issn = {10495258},
journal = {Advances in neural information processing systems 16: proceedings of the 2003 conference},
pages = {1621},
title = {{Out-of-sample extensions for LLE, Isomap, MDS, Eigenmaps and Spectral Clustering}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=0F-9C7K8fQ8C{\&}pgis=1},
year = {2004}
}
@article{Bellet2013,
abstract = {The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.},
archivePrefix = {arXiv},
arxivId = {1306.6709},
author = {Bellet, Aur{\'{e}}lien and Habrard, Amaury and Sebban, Marc},
eprint = {1306.6709},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1306.6709v4.pdf:pdf},
keywords = {(),edit distance,mahalanobis distance,metric learning,similarity learning},
title = {{A Survey on Metric Learning for Feature Vectors and Structured Data}},
url = {http://arxiv.org/abs/1306.6709},
year = {2013}
}
@article{Likelihood2009,
author = {Likelihood, Empirical and Bands, Confidence and Author, Density Estimation and Hall, Peter and Source, Art B Owen and Statistics, Graphical and Statistics, Mathematical and Foundation, Interface and Url, America Stable},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/gentleman2000.pdf:pdf},
journal = {America},
keywords = {function closure,random number generators},
number = {3},
pages = {273-- 289},
title = {{Interface Foundation of America}},
volume = {2},
year = {2009}
}
@article{Lanckriet2004,
author = {Lanckriet, Gert R G and Cristianini, Nello and Bartlett, Peter and Ghaoui, Laurent El and Jordan, Michael I},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/LanckrietCBGJ03.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {27--72},
title = {{Learning the Kernel Matrix with Semidefinite Programming}},
volume = {5},
year = {2004}
}
@article{Pierce2001,
author = {Pierce, David and Cardie, Claire},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Cardie{\_}emnlp-2001.pdf:pdf},
journal = {In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing},
pages = {1--9},
title = {{Limitations of Co-Training for Natural Language Learning from Large Datasets}},
year = {2001}
}
@article{Khosla2012,
abstract = {The presence of bias in existing object recognition datasets is now well-known in the computer vision community. While it remains in question whether creating an unbiased dataset is possible given limited resources, in this work we propose a discriminative framework that directly exploits dataset bias during training. In particular, our model learns two sets of weights: (1) bias vectors associated with each individual dataset, and (2) visual world weights that are common to all datasets, which are learned by undoing the associated bias from each dataset. The visual world weights are expected to be our best possible approximation to the object model trained on an unbiased dataset, and thus tend to have good generalization ability. We demonstrate the effectiveness of our model by applying the learned weights to a novel, unseen dataset, and report superior results for both classification and detection tasks compared to a classical SVM that does not account for the presence of bias. Overall, we find that it is beneficial to explicitly account for bias when combining multiple datasets.},
author = {Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei a. and Torralba, Antonio},
doi = {10.1007/978-3-642-33718-5_12},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/eccv2012{\_}khosla.pdf:pdf},
isbn = {9783642337178},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {158--171},
title = {{Undoing the damage of dataset bias}},
volume = {7572 LNCS},
year = {2012}
}
@article{Lu2015,
abstract = {Online learning with multiple kernels has gained increasing interests in recent years and found many applications. For classification tasks, Online Multiple Kernel Classification (OMKC), which learns a kernel based classifier by seeking the optimal linear combination of a pool of single kernel classifiers in an online fashion, achieves superior accuracy and enjoys great flexibility compared with traditional single-kernel classifiers. Despite being studied extensively, existing OMKC algorithms suffer from high computational cost due to their unbounded numbers of support vectors. To overcome this drawback, we present a novel framework of Budget Online Multiple Kernel Learning (BOMKL) and propose a new Sparse Passive Aggressive learning to perform effective budget online learning. Specifically, we adopt a simple yet effective Bernoulli sampling to decide if an incoming instance should be added to the current set of support vectors. By limiting the number of support vectors, our method can significantly accelerate OMKC while maintaining satisfactory accuracy that is comparable to that of the existing OMKC algorithms. We theoretically prove that our new method achieves an optimal regret bound in expectation, and empirically found that the proposed algorithm outperforms various OMKC algorithms and can easily scale up to large-scale datasets.},
archivePrefix = {arXiv},
arxivId = {1511.04813},
author = {Lu, Jing and Hoi, Steven C. H. and Sahoo, Doyen and Zhao, Peilin},
eprint = {1511.04813},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1511.04813.pdf:pdf},
pages = {1--12},
title = {{Budget Online Multiple Kernel Learning}},
url = {http://arxiv.org/abs/1511.04813},
year = {2015}
}
@article{Thesis2006,
author = {Thesis, Master and Eigensatz, Michael},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/eigensatz{\_}2006{\_}IGG.pdf:pdf},
journal = {Technology},
title = {{Insights into the Geometry of the Gaussian Kernel and an Application in Geometric Modeling}},
year = {2006}
}
@article{Rudda,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.06112v1},
author = {Rudd, Ethan M and Scheirer, Walter J and Boult, Terrance E},
eprint = {arXiv:1506.06112v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1506.06112v1.pdf:pdf},
pages = {1--9},
title = {{The Extreme Value Machine}}
}
@article{Bousquet2006,
author = {Bousquet, Olivier},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bousquet - 2006 - Manifold Learning.pdf:pdf},
title = {{Manifold Learning}},
year = {2006}
}
@article{Marchigiano2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1410.8516v6},
author = {Marchigiano, Comitato Regionale},
eprint = {arXiv:1410.8516v6},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1410.8516v6.pdf:pdf},
number = {3},
pages = {1764--1772},
title = {{T Raining W Ith I Ndependent C Ranks a Lters}},
volume = {1},
year = {2009}
}
@article{Ying2012,
abstract = {The main theme of this paper is to develop a novel eigenvalue optimization framework for learning a Mahalanobis metric. Within this context, we introduce a novel metric learning approach called DML-eig which is shown to be equivalent to a well-known eigenvalue optimization problem called minimizing the maximal eigenvalue of a symmetric matrix (Overton, 1988; Lewis and Overton, 1996). Moreover, we formulate LMNN (Weinberger et al., 2005), one of the state-of-the-art metric learning methods, as a similar eigenvalue optimization problem. This novel framework not only provides new insights into metric learning but also opens new avenues to the design of efficient metric learning algorithms. Indeed, first-order algorithms are developed for DML-eig and LMNN which only need the computation of the largest eigenvector of a matrix per iteration. Their conver- gence characteristics are rigorously established. Various experiments on benchmark data sets show the competitive performance of our new approaches. In addition, we report an encouraging result on a difficult and challenging face verification data set called Labeled Faces in theWild (LFW).},
author = {Ying, Yiming},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ying12a.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {convex optimization,eigenvalue optimization,face verification,first order methods,matrix factorization,metric learning,semi definite programming},
pages = {1--26},
title = {{Distance Metric Learning with Eigenvalue Optimization}},
url = {http://jmlr.csail.mit.edu/papers/volume13/ying12a/ying12a.pdf},
volume = {13},
year = {2012}
}
@article{Kloft2010a,
archivePrefix = {arXiv},
arxivId = {arXiv:1003.0079v1},
author = {Kloft, M and Brefeld, Ulf and Sonnenburg, S and Zien, Alexander},
eprint = {arXiv:1003.0079v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/EECS-2010-21.pdf:pdf},
journal = {Arxiv preprint arXiv:1003.0079},
pages = {1--43},
title = {{Non-Sparse Regularization and Efficient Training with Multiple Kernels}},
url = {http://arxiv.org/pdf/1003.0079},
year = {2010}
}
@article{Stamos2015,
abstract = {다양한 데이터 셋에 존재하는 것들 중 잠재적인 sub-category들을 학습하는 방법 소개},
author = {Stamos, Dimitris and Murino, Vittorio and Pontil, Massimiliano},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Stamos{\_}Learning{\_}With{\_}Dataset{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr},
title = {{Learning with Dataset Bias in Latent Subcategory Models}},
year = {2015}
}
@article{Silver2012a,
abstract = {We present a new method for the detection of gene pathways associated with a multivariate quantitative trait, and use it to identify causal pathways associated with an imaging endophenotype characteristic of longitudinal structural change in the brains of patients with Alzheimer's disease (AD). Our method, known as pathways sparse reduced-rank regression (PsRRR), uses group lasso penalised regression to jointly model the effects of genome-wide single nucleotide polymorphisms (SNPs), grouped into functional pathways using prior knowledge of gene-gene interactions. Pathways are ranked in order of importance using a resampling strategy that exploits finite sample variability. Our application study uses whole genome scans and MR images from 99 probable AD patients and 164 healthy elderly controls in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. 66,182 SNPs are mapped to 185 gene pathways from the KEGG pathway database. Voxel-wise imaging signatures characteristic of AD are obtained by analysing 3D patterns of structural change at 6, 12 and 24. months relative to baseline. High-ranking, AD endophenotype-associated pathways in our study include those describing insulin signalling, vascular smooth muscle contraction and focal adhesion. All of these have been previously implicated in AD biology. In a secondary analysis, we investigate SNPs and genes that may be driving pathway selection. High ranking genes include a number previously linked in gene expression studies to ?-amyloid plaque formation in the AD brain (PIK3. R3,. PIK3. CG,. PRKCAand. PRKCB), and to AD related changes in hippocampal gene expression (ADCY2, ACTN1, ACACA, and GNAI1). Other high ranking previously validated AD endophenotype-related genes include CR1, TOMM40 and APOE. {\textcopyright} 2012 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {arXiv:1204.1937v1},
author = {Silver, Matt and Janousova, Eva and Hua, Xue and Thompson, Paul M. and Montana, Giovanni},
doi = {10.1016/j.neuroimage.2012.08.002},
eprint = {arXiv:1204.1937v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S1053811912007951-main.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$r1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Alzheimer's disease,Atrophy,Gene pathways,Imaging genetics,Sparse regression},
number = {3},
pages = {1681--1694},
pmid = {22982105},
publisher = {Elsevier Inc.},
title = {{Identification of gene pathways implicated in Alzheimer's disease using longitudinal imaging phenotypes with sparse regression}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2012.08.002},
volume = {63},
year = {2012}
}
@article{Lee,
author = {Lee, Ching-pei and Lin, Chih-jen},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/l2mcsvm.pdf:pdf},
keywords = {multi-class classification,squared hinge,support vector machines},
pages = {1--22},
title = {{A Study on L2-Loss ( Squared Hinge-Loss ) Multi-Class SVM}}
}
@article{Jawanpuria2012,
author = {Jawanpuria, Pratik},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jawanpuria - 2012 - On p -norm Path Following in Multiple Kernel Learning for Non-linear Feature Selection.pdf:pdf},
title = {{On p -norm Path Following in Multiple Kernel Learning for Non-linear Feature Selection}},
year = {2012}
}
@article{Anguita2013a,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and Reyes-ortiz, Jorge L},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C11.pdf:pdf},
isbn = {9782874190810},
number = {April},
pages = {24--26},
title = {{A Public Domain Dataset for Human Activity Recognition Using Smartphones}},
year = {2013}
}
@article{Kumar2015a,
abstract = {We propose a formulation and supervised training method for a kernel that learns semantic similarity between pairs of images. Because image class label already captures semantics, we suggest that if an image pair belongs to the same class, no matter what that class is, then it can be deemed semantically similar. Thus, our kernel takes a pair of images as inputs and is trained to estimate their probability of belonging to the same class. Our kernel is a Siamese convolutional neural network (CNN) with appropriate post-processing of the network output to produce gram matrices corresponding to a reproducing kernel Hilbert space that can be used in a support vector machine (SVM) for classification. We demonstrate that training a subset or even all the parameters of such a kernel on one set of classes generalizes to another, allowing estimation of similarity between pairs of images from those classes with little to no training effort. This suggests a new method for transfer learning from non-target classes to target classes such that only the SVM and perhaps a subset of parameters of the kernel's neural network need to be trained on target classes. We demonstrate near state-of-the-art recognition performance on standard image recognition benchmarks such as NIST and CIFAR-10 with orders of magnitude fewer samples from target classes than those used by traditional CNNs.},
archivePrefix = {arXiv},
arxivId = {1512.04086},
author = {Kumar, Neeraj and Sharma, Ranti Dev and Karmakar, Animesh and Sethi, Amit},
eprint = {1512.04086},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1512.04086.pdf:pdf},
title = {{Deep Learning-Based Image Kernel for Inductive Transfer}},
url = {http://arxiv.org/abs/1512.04086},
year = {2015}
}
@article{Anguita2012c,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
doi = {10.1007/s11063-012-9235-z},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Journal/J02 - NPL.pdf:pdf},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {Convex-concave programming,Maximal discrepancy,Model selection,Rademacher complexity,Support vector machine},
number = {3},
pages = {275--283},
title = {{In-sample model selection for trimmed hinge loss support vector machine}},
volume = {36},
year = {2012}
}
@article{Fasshauer2011,
abstract = {Positive definite kernels play an increasingly prominent role in many applications such as scattered data fitting, numerical solution of PDEs, computer experiments, machine learning, rapid prototyping and computer graphics. We discuss some of the historical and current devel- opments of the theory and applications of positive definite kernels – always with an eye toward the mathematics of G¨ ottingen in general and Robert Schaback in particular. A few comments concerning the future of the field are also provided.},
author = {Fasshauer, Gregory E},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/PDKernels.pdf:pdf},
journal = {Dolomite Research Notes on Approximation},
pages = {1--48},
title = {{Positive definite kernels: past, present and future}},
url = {http://www.math.iit.edu/{~}fass/PDKernels.pdf},
year = {2011}
}
@article{Sun,
author = {Sun, Bo and Zhang, Di and He, Jun and Yu, Lejun and Wu, Xuewen},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/burgess2015.pdf:pdf},
keywords = {coarse alignment method,face,face detection method,multi-feature fusion,offset parameters of a,simple multiple kernel learning},
pages = {1--10},
title = {{Multi- feature based robust face detection and coarse alignment method via Multiple Kernel Learning}},
volume = {9652},
year = {2015}
}
@article{Erhan2009,
abstract = {Whereas theoretical work suggests that deep architectures might be more efﬁcient at representing highly-varying functions, training deep architectures was unsuccessful until the recent advent of algorithms based on unsupervised pretraining. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difﬁcult learning problem. Answering these questions is important if learning in deep architectures is to be further improved. We attempt to shed some light on these questions through extensive simulations. The experiments conﬁrm and clarify the advantage of unsupervised pre-training. They demonstrate the robustness of the training procedure with respect to the random initialization, the positive effect of pre-training in terms of optimization and its role as a regularizer. We empirically show the inﬂuence of pre-training with respect to architecture depth, model capacity, and number of training examples.},
author = {Erhan, Dumitru and Manzagol, Pierre-Antoine and Bengio, Yoshua and Bengio, Samy and Vincent, Pascal},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/erhan09a.pdf:pdf},
issn = {15324435},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {153--160},
title = {{The difficulty of training deep architectures and the effect of unsupervised pre-training}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS09{\_}ErhanMBBV.pdf},
volume = {5},
year = {2009}
}
@article{Bartlett2005,
archivePrefix = {arXiv},
arxivId = {arXiv:math/0508275v1},
author = {Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
doi = {10.1214/009053605000000282},
eprint = {0508275v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0508275.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,article published by the,concentration inequalities,data-dependent complex-,error bounds,ity,rademacher averages,reprint of the original,this is an electronic},
number = {4},
pages = {1497--1537},
primaryClass = {arXiv:math},
title = {{Local Rademacher complexities}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1123250221/},
volume = {33},
year = {2005}
}
@article{SorenSonnenburg2006,
abstract = {While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun.},
author = {Sonnenburg, S{\"{o}}ren and R{\"{a}}tsch, Gunnar and Sch{\"{a}}fer, Christin and Sch{\"{o}}lkopf, Bernhard and {S{\"{o}}ren Sonnenburg} and {Gunnar R{\"{a}}tsch} and {Christin Sch{\"{a}}fer} and {Bernhard Sch{\"{o}}lkopf}},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/sonnenburg06a.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/7-1531-sonnenburg.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learning Research},
keywords = {large scale optimization,multiple kernel learning,string kernels,support vector ma-},
pages = {1531----1565},
title = {{Large Scale Multiple Kernel Learning}},
url = {http://jmlr.csail.mit.edu/papers/volume7/sonnenburg06a/sonnenburg06a.pdf http://dl.acm.org/citation.cfm?id=1248547.1248604},
volume = {7},
year = {2006}
}
@article{Nazarpour2014,
author = {Nazarpour, Abdollah and Adibi, Peyman},
doi = {10.1016/j.patcog.2014.12.001},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0031320314004890-main.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Handwritten Digit Recognition,Multiple Kernel Learning,Objection Recognition,Pattern Recognition,Supervised Dimensionality Reduction},
number = {5},
pages = {1854--1862},
publisher = {Elsevier},
title = {{Author ' s Accepted Manuscript Two-Stage Multiple Kernel Learning for Supervised Dimensionality Reduction}},
url = {http://dx.doi.org/10.1016/j.patcog.2014.12.001},
volume = {48},
year = {2014}
}
@article{Young2013,
abstract = {Accurately identifying the patients that have mild cognitive impairment (MCI) who will go on to develop Alzheimer's disease (AD) will become essential as new treatments will require identification of AD patients at earlier stages in the disease process. Most previous work in this area has centred around the same automated techniques used to diagnose AD patients from healthy controls, by coupling high dimensional brain image data or other relevant biomarker data to modern machine learning techniques. Such studies can now distinguish between AD patients and controls as accurately as an experienced clinician. Models trained on patients with AD and control subjects can also distinguish between MCI patients that will convert to AD within a given timeframe (MCI-c) and those that remain stable (MCI-s), although differences between these groups are smaller and thus, the corresponding accuracy is lower. The most common type of classifier used in these studies is the support vector machine, which gives categorical class decisions. In this paper, we introduce Gaussian process (GP) classification to the problem. This fully Bayesian method produces naturally probabilistic predictions, which we show correlate well with the actual chances of converting to AD within 3 years in a population of 96 MCI-s and 47 MCI-c subjects. Furthermore, we show that GPs can integrate multimodal data (in this study volumetric MRI, FDG-PET, cerebrospinal fluid, and APOE genotype with the classification process through the use of a mixed kernel). The GP approach aids combination of different data sources by learning parameters automatically from training data via type-II maximum likelihood, which we compare to a more conventional method based on cross validation and an SVM classifier. When the resulting probabilities from the GP are dichotomised to produce a binary classification, the results for predicting MCI conversion based on the combination of all three types of data show a balanced accuracy of 74{\%}. This is a substantially higher accuracy than could be obtained using any individual modality or using a multikernel SVM, and is competitive with the highest accuracy yet achieved for predicting conversion within three years on the widely used ADNI dataset. ?? 2013 The Authors. Published by Elsevier Inc. All rights reserved.},
author = {Young, Jonathan and Modat, Marc and Cardoso, Manuel J. and Mendelson, Alex and Cash, Dave and Ourselin, Sebastien},
doi = {10.1016/j.nicl.2013.05.004},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S2213158213000600-main.pdf:pdf},
isbn = {2213-1582 (Electronic)$\backslash$n2213-1582 (Linking)},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Alzheimer's disease,Gaussian process,Mild cognitive impairment,Multimodality,Probabilistic classification,Risk scores,Support vector machine},
number = {1},
pages = {735--745},
pmid = {24179825},
publisher = {The Authors},
title = {{Accurate multimodal probabilistic prediction of conversion to Alzheimer's disease in patients with mild cognitive impairment}},
url = {http://dx.doi.org/10.1016/j.nicl.2013.05.004},
volume = {2},
year = {2013}
}
@article{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, Juergen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1404.7828v4.pdf:pdf},
pages = {1--88},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://arxiv.org/abs/1404.7828 http://dx.doi.org/10.1016/j.neunet.2014.09.003},
year = {2014}
}
@article{Oneto2014a,
author = {Oneto, Luca and Pilarz, Bernardo and Ghio, Alessandro and Anguita, Davide},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C26.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C26{\_}P.pdf:pdf},
number = {ii},
title = {{Model Selection for Big Data : Algorithmic Stability and Bag of Little Bootstraps on GPUs}},
year = {2014}
}
@article{Luo2015,
author = {Luo, Wei and Yang, Jian and Xu, Wei and Li, Jun and Zhang, Jian},
doi = {10.1016/j.neucom.2015.04.075},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0925231215005561-main.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Feature coding,Feature combination,Image classification,Multiple kernel learning},
pages = {1--9},
publisher = {Elsevier},
title = {{Higher-level Feature Combination via Multiple Kernel Learning for Image Classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215005561},
year = {2015}
}
@article{Vlasic2005,
abstract = {Face Transfer is a method for mapping videorecorded performances of one individual to facial animations of another. It extracts visemes (speech-related mouth articulations), expressions, and three-dimensional (3D) pose from monocular video or film footage. These parameters are then used to generate and drive a detailed 3D textured face mesh for a target identity, which can be seamlessly rendered back into target footage. The underlying face model automatically adjusts for how the target performs facial expressions and visemes. The performance data can be easily edited to change the visemes, expressions, pose, or even the identity of the target--the attributes are separably controllable. This supports a wide variety of video rewrite and puppetry applications.Face Transfer is based on a multilinear model of 3D face meshes that separably parameterizes the space of geometric variations due to different attributes (e.g., identity, expression, and viseme). Separability means that each of these attributes can be independently varied. A multilinear model can be estimated from a Cartesian product of examples (identities {\&}times; expressions {\&}times; visemes) with techniques from statistical analysis, but only after careful preprocessing of the geometric data set to secure one-to-one correspondence, to minimize cross-coupling artifacts, and to fill in any missing examples. Face Transfer offers new solutions to these problems and links the estimated model with a face-tracking algorithm to extract pose, expression, and viseme parameters.},
author = {Vlasic, Daniel and Brand, Matthew and Pfister, Hanspeter and Popovi{\'{c}}, Jovan},
doi = {10.1145/1073204.1073209},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/TR2005-048.pdf:pdf},
isbn = {1595933646},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {32 vassar street,cambridge,computer vision,facial animation,ma,mit csail,the stata center,tracking},
number = {3},
pages = {426},
title = {{Face transfer with multilinear models}},
volume = {24},
year = {2005}
}
@article{NAKAMOTO2011,
author = {Li, Hang},
doi = {10.1587/transinf.E94.D.1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/l2r.pdf:pdf},
issn = {0916-8532},
journal = {IEICE Transactions on Information and Systems},
keywords = {information retrieval,language processing,learning to rank,natural,svm},
number = {1},
pages = {1--2},
title = {{A Short Introduction to Learning to Rank}},
url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/transinf/E94.D.1?from=CrossRef},
volume = {E94-D},
year = {2011}
}
@article{Bagnell2015,
author = {Bagnell, J.A and Farahmand, A.-m. and Electric, Mitsubishi},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bagnell, Farahmand - 2015 - Learning Positive Functions in a Hilbert Space.pdf:pdf},
title = {{Learning Positive Functions in a Hilbert Space}},
year = {2015}
}
@article{RobSchapirePrincetonUniversity2013,
author = {{Rob Schapire(Princeton University)}},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rob Schapire(Princeton University) - 2013 - Lecture 9 Definition of Rademacher Complexity.pdf:pdf},
journal = {COS 511: Theoretical Machine Learning},
number = {5},
pages = {1--5},
title = {{Lecture 9: Definition of Rademacher Complexity}},
url = {http://www.cs.princeton.edu/courses/archive/spring13/cos511/scribe{\_}notes/0305.pdf},
year = {2013}
}
@article{Cortes2013a,
author = {Cortes, Corinna},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/41863.pdf:pdf},
journal = {Proceedings of the {\ldots}},
title = {{Multi-class classification with maximum margin multiple kernel}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013{\_}cortes13},
volume = {28},
year = {2013}
}
@article{Author,
archivePrefix = {arXiv},
arxivId = {1511.05706},
author = {Author, Anonymous and Address, Affiliation},
eprint = {1511.05706},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1511.05706.pdf:pdf},
pages = {1--9},
title = {{Efficient Output Kernel Learning for Multiple Tasks}}
}
@article{Anguita2014,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
doi = {10.1016/j.patrec.2013.04.027},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Journal/J05 - PRL.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {rademacher complexity,structural risk minimization,support vector machine},
pages = {210--219},
publisher = {Elsevier B.V.},
title = {{Unlabeled patterns to tighten Rademacher complexity error bounds for kernel classifiers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865513001852},
volume = {37},
year = {2014}
}
@article{J.C.Gower1971,
abstract = {A general coefficient measuring the similarity between two sampling units is defined. The matrix of similarities between all pairs of sample units is shown to be positive semi- definite (except possibly when there are missing values). This is important for the multi- dimensional Euclidean representation of the sample and also establishes some inequalities amongst the similarities relating three individuals. The definition is extended to cope with a hierarchy of characters.},
author = {{J. C. Gower}},
doi = {10.2307/2528823},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Jaccard{\_}kernel{\_}proof.pdf:pdf},
isbn = {0006341X},
issn = {0006341X},
journal = {Biometrics},
number = {4},
pages = {857--871},
title = {{A General Coefficient of Similarity and Some of Its Properties}},
volume = {27},
year = {1971}
}
@article{Romera-Paredes2013,
abstract = {Many real world datasets occur or can be arranged into multi-modal structures. With such datasets, the tasks to be learnt can be referenced by multiple indices. Current mul- titask learning frameworks are not designed to account for the preservation of this infor- mation. We propose the use of multilinear algebra as a natural way to model such a set of related tasks. We present two learn- ing methods; one is an adapted convex relax- ation method used in the context of tensor completion. The second method is based on the Tucker decomposition and on alternating minimization. Experiments on synthetic and real data indicate that the multilinear ap- proaches provide a significant improvement over other multitask learningmethods. Over- all our second approach yields the best per- formance in all datasets.},
author = {Romera-Paredes, Bernardion and Aung, Min Hane and Bianchi-Berthouze, Nadia and Pontil, Massimiliano},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/romera-paredes13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
title = {{Multilinear multitask learning}},
url = {http://jmlr.org/proceedings/papers/v28/romera-paredes13.html},
volume = {28},
year = {2013}
}
@article{Jenatton2010,
abstract = {We propose to combine two approaches formod- eling data admitting sparse representations: on the one hand, dictionary learning has proven ef- fective for various signal processing tasks. On the other hand, recent work on structured spar- sity provides a natural framework for modeling dependencies between dictionary elements. We thus consider a tree-structured sparse regulariza- tion to learn dictionaries embedded in a hierar- chy. The involved proximal operator is com- putable exactly via a primal-dual method, allow- ing the use of accelerated gradient techniques. Experiments showthat for natural image patches, learned dictionary elements organize themselves in such a hierarchical structure, leading to an im- proved performance for restoration tasks. When applied to text documents, our method learns hi- erarchies of topics, thus providing a competitive alternative to probabilistic topic models.},
archivePrefix = {arXiv},
arxivId = {0909.0844},
author = {Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and Bach, Francis and Fr, Inria},
doi = {10.1.1.173.1120},
eprint = {0909.0844},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/icml2010{\_}JenattonMOB10.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
pages = {487--494},
title = {{Proximal Methods for Sparse Hierarchical Dictionary Learning}},
url = {http://www.icml2010.org/papers/416.pdf},
year = {2010}
}
@article{Watanabe,
author = {Watanabe, Takanori and Kessler, Daniel and Scott, Clayton and Sripada, Chandra},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Watanabe et al. - Unknown - Multisite Disease Classification with Functional Connectomes via Multitask Structured Sparse SVM.pdf:pdf},
keywords = {alternating direction method,chine,multitask learning,resting-state fmri,structured sparsity,support vector ma-},
pages = {1--9},
title = {{Multisite Disease Classification with Functional Connectomes via Multitask Structured Sparse SVM}}
}
@article{Yu2010,
abstract = {BACKGROUND: This paper introduces the notion of optimizing different norms in the dual problem of support vector machines with multiple kernels. The selection of norms yields different extensions of multiple kernel learning (MKL) such as L(infinity), L1, and L2 MKL. In particular, L2 MKL is a novel method that leads to non-sparse optimal kernel coefficients, which is different from the sparse kernel coefficients optimized by the existing L(infinity) MKL method. In real biomedical applications, L2 MKL may have more advantages over sparse integration method for thoroughly combining complementary information in heterogeneous data sources. RESULTS: We provide a theoretical analysis of the relationship between the L2 optimization of kernels in the dual problem with the L2 coefficient regularization in the primal problem. Understanding the dual L2 problem grants a unified view on MKL and enables us to extend the L2 method to a wide range of machine learning problems. We implement L2 MKL for ranking and classification problems and compare its performance with the sparse L(infinity) and the averaging L1 MKL methods. The experiments are carried out on six real biomedical data sets and two large scale UCI data sets. L2 MKL yields better performance on most of the benchmark data sets. In particular, we propose a novel L2 MKL least squares support vector machine (LSSVM) algorithm, which is shown to be an efficient and promising classifier for large scale data sets processing. CONCLUSIONS: This paper extends the statistical framework of genomic data fusion based on MKL. Allowing non-sparse weights on the data sources is an attractive option in settings where we believe most data sources to be relevant to the problem at hand and want to avoid a "winner-takes-all" effect seen in L(infinity) MKL, which can be detrimental to the performance in prospective studies. The notion of optimizing L2 kernels can be straightforwardly extended to ranking, classification, regression, and clustering algorithms. To tackle the computational burden of MKL, this paper proposes several novel LSSVM based MKL algorithms. Systematic comparison on real data sets shows that LSSVM MKL has comparable performance as the conventional SVM MKL algorithms. Moreover, large scale numerical experiments indicate that when cast as semi-infinite programming, LSSVM MKL can be solved more efficiently than SVM MKL. AVAILABILITY: The MATLAB code of algorithms implemented in this paper is downloadable from http://homes.esat.kuleuven.be/{\~{}}sistawww/bioi/syu/l2lssvm.html.},
author = {Yu, Shi and Falck, Tillmann and Daemen, Anneleen and Tranchevent, Leon-Charles and Suykens, Johan Ak and {De Moor}, Bart and Moreau, Yves},
doi = {10.1186/1471-2105-11-309},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2010 - L2-norm multiple kernel learning and its application to biomedical data fusion.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
pages = {309},
pmid = {20529363},
title = {{L2-norm multiple kernel learning and its application to biomedical data fusion.}},
volume = {11},
year = {2010}
}
@article{Vahdat,
author = {Vahdat, Mehrnoosh},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C28.pdf:pdf},
number = {Ml},
title = {{Learning{\_}Analytics{\_}and{\_}Educational{\_}Data{\_}Mining}},
url = {www.edfutures.net/index.php?title=Learning{\_}Analytics{\#}Learning{\_}Analytics{\_}and{\_}Educational{\_}Data{\_}Mining{\_}.E2.80.93{\_}What.E2.80.99s{\_}the{\_}Difference.3F}
}
@article{Liang2007,
author = {Liang, Guohua and Zhu, Xingquan and Zhang, Chengqi},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2913-16564-1-PB.pdf:pdf},
isbn = {9781577355090},
keywords = {Student Abstracts and Posters},
pages = {1802--1803},
title = {{An Empirical Study of Bagging Predictors for Different Learning Algorithms}},
year = {2007}
}
@article{Rifkin2004,
abstract = {We consider the problem of multiclass classification. Our main thesis is that a simple " one-vs-all " scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines. This thesis is interesting in that it disagrees with a large body of recent published work on multiclass classification. We support our position by means of a critical review of the existing literature, a substantial collection of carefully controlled experimental work, and theoretical arguments.},
author = {Rifkin, Ryan and Klautau, Aldebaro and Org, Klautau@ieee},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/rifkin04a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Multiclass Classification,Regularization},
pages = {101--141},
title = {{In Defense of One-Vs-All Classification}},
volume = {5},
year = {2004}
}
@article{Bach2008b,
abstract = {We present a convex formulation of dictionary learning for sparse signal decomposition. Convexity is obtained by replacing the usual explicit upper bound on the dictionary size by a convex rank-reducing term similar to the trace norm. In particular, our formulation introduces an explicit trade-off between size and sparsity of the decomposition of rectangular matrices. Using a large set of synthetic examples, we compare the estimation abilities of the convex and non-convex approaches, showing that while the convex formulation has a single local minimum, this may lead in some cases to performance which is inferior to the local minima of the non-convex formulation.},
archivePrefix = {arXiv},
arxivId = {0812.1869},
author = {Bach, Francis and Mairal, Julien and Ponce, Jean},
eprint = {0812.1869},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0812.1869.pdf:pdf},
pages = {1--12},
title = {{Convex Sparse Matrix Factorizations}},
url = {http://arxiv.org/abs/0812.1869},
year = {2008}
}
@article{Kim2002,
author = {Kim, Kwang In and Jung, Keechul and Kim, Hang Joon},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Jung, Kim - 2002 - Face verification using kernel principle component analysis.pdf:pdf},
journal = {IEE Signal Processing Letters},
number = {2},
pages = {40--42},
title = {{Face verification using kernel principle component analysis}},
volume = {9},
year = {2002}
}
@article{Koltchinskii2006,
archivePrefix = {arXiv},
arxivId = {arXiv:0708.0083v1},
author = {Koltchinskii, Vladimir},
doi = {10.1214/009053606000001019},
eprint = {arXiv:0708.0083v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0708.0083.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,classification,concentration inequalities,empirical risk minimization,inequalities,model selection,oracle,rademacher complexities},
number = {6},
pages = {2593--2656},
title = {{Local Rademacher complexities and oracle inequalities in risk minimization}},
url = {http://projecteuclid.org/euclid.aos/1179935055},
volume = {34},
year = {2006}
}
@article{Kong2006,
author = {Kong, Hong and Davis, California},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kong, Davis - 2006 - Spectral Kernel Learning for Semi-Supervised Classi cation.pdf:pdf},
journal = {International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {1150--1155},
title = {{Spectral Kernel Learning for Semi-Supervised Classi cation}},
year = {2006}
}
@article{Pavlidis2001,
author = {Pavlidis, Paul and Weston, Jason and Jinsong, Cai and Grundy, William Noble},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/exp-phylo.pdf:pdf},
journal = {Proceedings of the Fifth International Conference on Computational Molecular Biology},
number = {212},
pages = {242--248},
title = {{Gene functional classification from heterogeneous data}},
year = {2001}
}
@article{Aiollia,
abstract = {The goal of Multiple Kernel Learning (MKL) is to combine kernels derived from multiple sources in a data-driven way with the aim to enhance the accuracy of a kernel based machine. In this paper, we propose a time and space efficient MKL algorithm that can easily cope with hundreds of thousands of kernels and more. We compared our algorithm with other baselines plus three state-of-the-art MKL methods showing that our approach is often superior.},
author = {Aiolli, Fabio and Donini, Michele},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Sito mdonini/authenticity/paper/donini{\_}ESANN14.pdf:pdf},
isbn = {9782874190957},
journal = {ESANN 2014 proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
number = {April},
pages = {23--25},
title = {{Easy multiple kernel learning}},
year = {2014}
}
@article{Xie2015,
author = {Xie, Chengjun and Zhang, Jie and Li, Rui and Li, Jinyan and Hong, Peilin and Xia, Junfeng and Chen, Peng},
doi = {10.1016/j.compag.2015.10.015},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0168169915003282-main.pdf:pdf},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {insect classification,multiple-task sparse representation of},
pages = {123--132},
publisher = {Elsevier B.V.},
title = {{Automatic classification for field crop insects via multiple-task sparse representation and multiple-kernel learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0168169915003282},
volume = {119},
year = {2015}
}
@article{Anguita2013g,
abstract = {In this paper we propose a novel energy efficient approach for the recog- nition of human activities using smartphones as wearable sensing devices, targeting assisted living applications such as remote patient activity monitoring for the disabled and the elderly. The method exploits fixed-point arithmetic to propose a modified multiclass Support Vector Machine (SVM) learning algorithm, allowing to better pre- serve the smartphone battery lifetime with respect to the conventional floating-point based formulation while maintaining comparable system accuracy levels. Experiments show comparative results between this approach and the traditional SVM in terms of recognition performance and battery consumption, highlighting the advantages of the proposed method.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and Reyes-Ortiz, Jorge L.},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Journal/J03 - JUCS.pdf:pdf},
isbn = {3642407277},
issn = {0948695X},
journal = {Journal of Universal Computer Science},
keywords = {Activity recognition,Assisted healthcare,Energy efficiency,Fixed-point arithmetic,Remote monitoring,SVM,Smartphones},
number = {9},
pages = {1295--1314},
title = {{Energy efficient smartphone-based activity recognition using fixed-point arithmetic}},
volume = {19},
year = {2013}
}
@article{Jenssen2010,
abstract = {We introduce kernel entropy component analysis (kernel ECA) as a new method for data transformation and dimensionality reduction. Kernel ECA reveals structure relating to the Renyi entropy of the input space data set, estimated via a kernel matrix using Parzen windowing. This is achieved by projections onto a subset of entropy preserving kernel principal component analysis (kernel PCA) axes. This subset needs not in general to correspond to the top eigenvalues of the kernel matrix, in contrast to dimensionality reduction using kernel PCA. We show that kernel ECA may produce strikingly different trans- formed data sets compared to kernel PCA, with a distinct angle- based structure. A new spectral clustering algorithm utilizing this structure is developed with positive results. Furthermore, kernel ECA is shown to be an useful alternative for pattern denoising.},
author = {Jenssen, Robert},
doi = {10.1109/TPAMI.2009.100},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/04912217.pdf:pdf},
isbn = {9781424478743},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Clustering,Kernel PCA,Parzen windowing,Pattern denoising,Renyi entropy,Spectral data transformation},
number = {5},
pages = {847--860},
pmid = {20299709},
title = {{Kernel entropy component analysis}},
volume = {32},
year = {2010}
}
@article{Xu2008,
abstract = {Multiple Kernel Learning (MKL) can be formulated as a convex-concave minmax optimization problem, whose saddle point corresponds to the optimal solution to MKL. Most MKL methods employ the L1-norm simplex constraints on the combination weights of kernels, which therefore involves optimization of a non-smooth function of the kernel weights. These methods usually divide the optimization into two cycles: one cycle deals with the optimization on the kernel combination weights, and the other cycle updates the parameters of SVM. Despite the success of their efficiency, they tend to discard informative complementary kernels. To improve accuracy, we introduce smoothness to the optimization procedure. Furthermore, we transform the optimization into a single smooth convex optimization problem and employ the Nesterov's method to efficiently solve the optimization problem. Experiments on benchmark data sets demonstrate that the proposed algorithm clearly improves current MKL methods in a number scenarios.},
author = {Xu, Zenglin and Lyu, Michael R},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1756-8169-1-PB.pdf:pdf},
isbn = {9781577354642},
journal = {Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI-10)},
keywords = {Technical Papers -- Machine Learning},
pages = {637--642},
title = {{Smooth Optimization for Effective Multiple Kernel Learning}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/viewPDFInterstitial/1756/},
year = {2008}
}
@article{Rakotomamonjy2008,
abstract = {Multiple kernel learning (MKL) aims at simultaneously learning a kernel and the associated predictor in supervised learning settings. For the support vector machine, an efficient and general multiple kernel learning algorithm, based on semi-infinite linear programming, has been recently proposed. This approach has opened new perspectives since it makes MKL tractable for large-scale problems, by iteratively using existing support vector machine code. However, it turns out that this iterative algorithm needs numerous iterations for converging towards a reasonable solution. In this paper, we address the MKL problem through a weighted 2-norm regularization formulation with an additional constraint on the weights that encourages sparse kernel combinations. Apart from learning the combination, we solve a standard SVM optimization problem, where the kernel is defined as a linear combination of multiple kernels. We propose an algorithm, named SimpleMKL, for solving this MKL problem and provide a new insight on MKL algorithms based on mixed-norm regularization by showing that the two approaches are equivalent. We show how SimpleMKL can be applied beyond binary classification, for problems like regression, clustering (one-class classification) or multiclass classification. Experimental results show that the proposed algorithm converges rapidly and that its efficiency compares favorably to other MKL algorithms. Finally, we illustrate the usefulness of MKL for some regressors based on wavelet kernels and on some model selection problems related to multiclass classification problems.},
author = {Rakotomamonjy, Alain and Bach, Francis and Canu, Stephane and Grandvalet, Yves},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/rakotomamonjy08a.pdf:pdf},
isbn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {classification,kernel-combination,kernel-method,machine-learning,mkl,multiple-kernel-learning,simpmkl,support-vector-machines},
pages = {2491--2521},
title = {{SimpleMKL}},
volume = {9},
year = {2008}
}
@article{Bach2012,
archivePrefix = {arXiv},
arxivId = {1108.0775v2},
author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
doi = {10.1561/2200000015},
eprint = {1108.0775v2},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1108.0775.pdf:pdf},
isbn = {9780262016469},
issn = {1935-8237},
journal = {Found. Trends Mach. Learn.},
number = {1},
pages = {1--106},
title = {{Optimization with Sparsity-Inducing Penalties}},
url = {http://dx.doi.org/10.1561/2200000015},
volume = {4},
year = {2012}
}
@article{Azadi2014,
author = {Azadi, Samaneh and Sra, S},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/azadi14.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st International Conference on {\ldots}},
keywords = {alternating direction method of,ards an optimal stochastic,multipliers},
title = {{Towards an optimal stochastic alternating direction method of multipliers}},
url = {http://jmlr.org/proceedings/papers/v32/azadi14.html},
volume = {32},
year = {2014}
}
@article{Chapelle2010a,
abstract = {RankSVM (Herbrich et al. in Advances in large margin classifiers. MIT Press, Cambridge, MA, 2000; Joachims in Proceedings of the ACM conference on knowledge discovery and data mining (KDD), 2002) is a pairwise method for designing ranking models. SVMLight is the only publicly available software for RankSVM. It is slow and, due to incomplete training with it, previous evaluations show RankSVM to have inferior ranking performance. We propose new methods based on primal Newton method to speed up RankSVM training and show that they are 5 orders of magnitude faster than SVMLight. Evaluation on the Letor benchmark datasets after complete training using such methods shows that the performance of RankSVM is excellent.},
author = {Chapelle, O. and Keerthi, S. S.},
doi = {10.1007/s10791-009-9109-9},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2Fs10791-009-9109-9.pdf:pdf},
isbn = {1386-4564},
issn = {13864564},
journal = {Information Retrieval},
keywords = {AUC optimization,Ranking,Support vector machines},
number = {3},
pages = {201--215},
title = {{Efficient algorithms for ranking with SVMs}},
volume = {13},
year = {2010}
}
@article{Schapire1999,
author = {Schapire, Robert E and Schapire, Robert E},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Improved Boosting Algorithms.pdf:pdf},
journal = {Computer},
keywords = {boosting algorithms,decision trees,multiclass classi cation,output coding},
number = {1997},
pages = {297--336},
title = {{Improved Boosting Algorithms Using Con dence-rated Predictions}},
volume = {336},
year = {1999}
}
@article{Afkanpour2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1205.0288v2},
author = {Afkanpour, Arash and Gy{\"{o}}rgy, Andr{\'{a}}s and Szepesvari, Csaba and Bowling, Michael},
eprint = {arXiv:1205.0288v2},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/afkanpour13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {374--382},
title = {{A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning}},
url = {http://jmlr.org/proceedings/papers/v28/afkanpour13.html},
year = {2013}
}
@article{Romera-Paredes2013a,
abstract = {We study the problem of learning a tensor from a set of linear measurements. A prominent methodology for this problem is based on a generalization of trace norm regularization, which has been used extensively for learning low rank matrices, to the tensor setting. In this paper, we highlight some limitations of this approach and propose an alternative convex relaxation on the Euclidean ball. We then describe a technique to solve the associated regularization problem, which builds upon the alternating direction method of multipliers. Experiments on one synthetic dataset and two real datasets indicate that the proposed method improves significantly over tensor trace norm regularization in terms of estimation error, while remaining computationally tractable.},
archivePrefix = {arXiv},
arxivId = {arXiv:1307.4653v1},
author = {Romera-Paredes, Bernardino and Pontil, Massimiliano},
eprint = {arXiv:1307.4653v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1307.4653.pdf:pdf},
journal = {arXiv preprint arXiv:1307.4653},
pages = {1--17},
title = {{A New Convex Relaxation for Tensor Completion}},
url = {http://arxiv.org/abs/1307.4653},
year = {2013}
}
@article{Orabona2012,
abstract = {In recent years there has been a lot of interest in designing principled classification algorithms over multiple cues, based on the intuitive notion that using more features should lead to better performance. In the domain of kernel methods, a principled way to use multiple features is the Multi Kernel Learning (MKL) approach. Here we present a MKL optimization algorithm based on stochastic gradient descent that has a guaranteed convergence rate. We directly solve theMKL problem in the primal formulation. By having a p-norm formulation of MKL, we introduce a parameter that controls the level of sparsity of the solution, while leading to an easier optimization problem. We prove theoretically and exper- imentally that 1) our algorithm has a faster convergence rate as the number of kernels grows; 2) the training complexity is linear in the number of training examples; 3) very fewiterations are sufficient to reach good solutions. Experiments on standard benchmark databases support our claims.},
author = {Orabona, Francesco and Jie, Luo and Caputo, Barbara},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/orabona12a.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning {\ldots}},
keywords = {convergence bounds,descent,large scale,learning kernels,multiple kernel learning,online optimization,stochastic subgradient},
pages = {227--253},
title = {{Multi Kernel Learning with Online-Batch Optimization}},
url = {http://francesco.orabona.com/papers/12jmlr.pdf},
volume = {13},
year = {2012}
}
@article{Sutskever2013,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6639346},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/momentum.pdf:pdf},
isbn = {978-1-4799-0356-6},
issn = {15206149},
journal = {Jmlr W{\&}Cp},
number = {2010},
pages = {1139--1147},
title = {{On the importance of initialization and momentum in deep learning}},
volume = {28},
year = {2013}
}
@article{Yang2012,
abstract = {In this paper, a group-sensitive multiple kernel learning (GS-MKL) method is proposed for object recognition to accommodate the intraclass diversity and the interclass correlation. By introducing the "group" between the object category and individual images as an intermediate representation, GS-MKL attempts to learn group-sensitive multikernel combinations together with the associated classifier. For each object category, the image corpus from the same category is partitioned into groups. Images with similar appearance are partitioned into the same group, which corresponds to the subcategory of the object category. Accordingly, intraclass diversity can be represented by the set of groups from the same category but with diverse appearances; interclass correlation can be represented by the correlation between groups from different categories. GS-MKL provides a tractable solution to adapt multikernel combination to local data distribution and to seek a tradeoff between capturing the diversity and keeping the invariance for each object category. Different from the simple hybrid grouping strategy that solves sample grouping and GS-MKL training independently, two sample grouping strategies are proposed to integrate sample grouping and GS-MKL training. The first one is a looping hybrid grouping method, where a global kernel clustering method and GS-MKL interact with each other by sharing group-sensitive multikernel combination. The second one is a dynamic divisive grouping method, where a hierarchical kernel-based grouping process interacts with GS-MKL. Experimental results show that performance of GS-MKL does not significantly vary with different grouping strategies, but the looping hybrid grouping method produces slightly better results. On four challenging data sets, our proposed method has achieved encouraging performance comparable to the state-of-the-art and outperformed several existing MKL methods.},
author = {Yang, Jingjing and Tian, Yonghong and Duan, Ling-Yu and Huang, Tiejun and Gao, Wen},
doi = {10.1109/TIP.2012.2183139},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2012 - Group-sensitive multiple kernel learning for object recognition.pdf:pdf},
isbn = {9781424444199},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {Algorithms,Artificial Intelligence,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
number = {Iccv},
pages = {2838--52},
pmid = {22249707},
title = {{Group-sensitive multiple kernel learning for object recognition.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22481821},
volume = {21},
year = {2012}
}
@article{Igel2010,
author = {Igel, Christian},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Igel - 2010 - Rademacher Complexity.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/rademacher.pdf:pdf},
pages = {1--33},
title = {{Rademacher Complexity}},
year = {2010}
}
@article{Ning2010,
abstract = {This paper focuses on exploring personalized multi-task learning approaches for collaborative filtering towards the goal of improving the prediction performance of rating prediction systems. These methods first specifically identify a set of users that are closely related to the user under con-sideration (i.e., active user), and then learn multiple rating prediction models simultaneously, one for the active user and one for each of the related users. Such learning for multiple models (tasks) in parallel is implemented by representing all learning instances (users and items) using a coupled user-item representation, and within error-insensitive Support Vector Regression (frame-work applying multi-task kernel tricks. A comprehensive set of experiments shows that multi-task learning approaches lead to significant performance improvement over conventional alternatives.},
author = {Ning, Xia and Karypis, George},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ning10a.pdf:pdf},
journal = {Journal of Machine Learning Research-Proceedings Track},
keywords = {collaborative filtering,multi-task learning},
number = {1998},
pages = {269--284},
title = {{Multi-task Learning for Recommender Systems}},
year = {2010}
}
@article{Gonen2011,
abstract = {In recent years, several methods have been proposed to combine multiple kernels instead of using a single one. These different kernels may correspond to using different notions of similarity or may be using information coming from multiple sources (different representations or different feature subsets). In trying to organize and highlight the similarities and differences between them, we give a taxonomy of and review several multiple kernel learning algorithms. We perform experiments on real data sets for better illustration and comparison of existing algorithms. We see that though there may not be large differences in terms of accuracy, there is difference between them in complexity as given by the number of stored support vectors, the sparsity of the solution as given by the number of used kernels, and training time complexity. We see that overall, using multiple kernels instead of a single one is useful and believe that combining kernels in a nonlinear or data-dependent way seems more promising than linear combination in fusing information provided by simple linear kernels, whereas linear methods are more reasonable when combining complex Gaussian kernels.},
author = {Gonen, Mehmet and Alpaydin, Ethem},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/gonen{\_}jmlr11{\_}paper.pdf:pdf},
isbn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {kernel machines,multiple kernel learning,support vector machines},
pages = {2211--2268},
title = {{Multiple Kernel Learning Algorithms}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000293757900004},
volume = {12},
year = {2011}
}
@article{Oneto2012,
author = {Oneto, Luca and Anguita, Davide and Ghio, Alessandro and Ridella, Sandro},
doi = {10.1007/978-3-642-33266-1_61},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C08.pdf:pdf},
isbn = {9783642332654},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Gene Expression Datasets,Rademacher Complexity,Structural Risk Minimization,Support Vector Machine},
number = {PART 2},
pages = {491--498},
title = {{Rademacher complexity and structural risk minimization: An application to human gene expression datasets}},
volume = {7553 LNCS},
year = {2012}
}
@article{Gonen2012a,
abstract = {Multiple kernel learning algorithms are proposed to combine kernels in order to obtain a better similarity measure or to integrate feature representations coming from different data sources. Most of the previous research on such methods is focused on the computational efficiency issue. However, it is still not feasible to combine many kernels using existing Bayesian approaches due to their high time complexity. We propose a fully conjugate Bayesian formulation and derive a deterministic variational approximation, which allows us to combine hundreds or thousands of kernels very efficiently. We briefly explain how the proposed method can be extended for multiclass learning and semi-supervised learning. Experiments with large numbers of kernels on benchmark data sets show that our inference method is quite fast, requiring less than a minute. On one bioinformatics and three image recognition data sets, our method outperforms previously reported results with better generalization performance.},
archivePrefix = {arXiv},
arxivId = {1206.6465},
author = {Gonen, Mehmet and G{\"{o}}nen, Mehmet},
eprint = {1206.6465},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/5.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/gonen{\_}icml12{\_}paper.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning},
number = {1},
title = {{Bayesian Efficient Multiple Kernel Learning}},
url = {http://users.ics.aalto.fi/gonen/icml12.php http://arxiv.org/abs/1206.6465},
year = {2012}
}
@article{Aiolli2015,
author = {Aiolli, Fabio and Donini, Michele},
doi = {10.1016/j.neucom.2014.11.078},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Sito mdonini/authenticity/paper/donini{\_}neurocomputing15.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Feature learning,Feature selection,Kernel methods,Multiple kernel learning,multiple kernel learning},
pages = {1--10},
publisher = {Elsevier},
title = {{EasyMKL: a scalable multiple kernel learning algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215003653},
year = {2015}
}
@article{Strazar2016,
abstract = {Efficient and accurate low-rank approximations to multiple data sources are essential in the era of big data. The scaling of kernel-based learning algorithms to large datasets is limited by the square complexity associated with computation and storage of the kernel matrix, which is assumed to be available in recent most multiple kernel learning algorithms. We propose a method to learn simultaneous low-rank approximations of a set of base kernels in regression tasks. We present the Mklaren algorithm, which approximates multiple kernel matrices with least angle regression in the low-dimensional feature space. The idea is based on entirely geometrical concepts and does not assume access to full kernel matrices. The algorithm achieves linear complexity in the number of data points as well as kernels, while it accounts for the correlations between kernel matrices. When explicit feature space representation is available for kernels, we use the relation between primal and dual regression weights to gain model interpretation. Our approach outperforms contemporary kernel matrix approximation approaches when learning with multiple kernels on standard regression datasets, as well as improves selection of relevant kernels in comparison to multiple kernel learning methods.},
archivePrefix = {arXiv},
arxivId = {1601.04366},
author = {Stra{\v{z}}ar, Martin and Curk, Toma{\v{z}}},
eprint = {1601.04366},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1601.04366.pdf:pdf},
title = {{Learning the kernel matrix via predictive low-rank approximations}},
url = {http://arxiv.org/abs/1601.04366},
year = {2016}
}
@article{Kim2002a,
abstract = {Even the support vector machine (SVM) has been proposed to provide a good generalization performance, the classification result of the practically implemented SVM is often far from the theoretically expected level because their implementations are based on the approximated algorithms due to the high complexity of time and space. To improve the limited classification performance of the real SVM, we propose to use the SVM ensembles with bagging (bootstrap aggregating). Each individual SVM is trained independently using the randomly chosen training samples via a bootstrap technique. Then, they are aggregated into to make a collective decision in several ways such as the majority voting, the LSE(least squares estimation)-based weighting, and the double-layer hierarchical combining. Various simulation results for the IRIS data classification and the hand-written digit recognitionshow that the proposed SVM ensembles with bagging outperforms a single SVM in terms of classification accuracy greatly.},
author = {Kim, Hc and Pang, S and Je, Hm and Kim, D and Bang, Sy},
doi = {10.1007/3-540-45665-1_31},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/3cd64ba608789b035b4e15376b1bde28179a (1).pdf:pdf},
isbn = {9783540440161},
journal = {Lecture Notes in Computer Science},
pages = {397--408},
title = {{Support Vector Machine Ensemble with Bagging}},
url = {http://dx.doi.org/10.1007/3-540-45665-1{\_}31},
year = {2002}
}
@article{Zien2007,
abstract = {In many applications it is desirable to learn from several kernels. "Multiple kernel learning" (MKL) allows the practitioner to optimize over linear combinations of kernels. By enforcing sparse coefficients, it also generalizes feature selection to kernel selection. We propose MKL for joint feature maps. This provides a convenient and principled way for MKL with multiclass problems. In addition, we can exploit the joint feature map to learn kernels on output spaces. We show the equivalence of several different primal formulations including different regularizers. We present several optimization methods, and compare a convex quadratically constrained quadratic program (QCQP) and two semi-infinite linear programs (SILPs) on toy data, showing that the SILPs are faster than the QCQP. We then demonstrate the utility of our method by applying the SILP to three real world datasets.},
author = {Zien, Alexander and Ong, Cheng Soon},
doi = {10.1145/1273496.1273646},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zien, Ong - 2007 - Multiclass multiple kernel learning.pdf:pdf},
isbn = {978-1-59593-793-3},
journal = {Proceedings of the 24th international conference on Machine learning},
number = {2},
pages = {1191--1198},
title = {{Multiclass multiple kernel learning}},
url = {http://dx.doi.org/10.1145/1273496.1273646$\backslash$nhttp://portal.acm.org/citation.cfm?id=1273646},
volume = {1},
year = {2007}
}
@article{Jain2012,
author = {Jain, P and Kulis, B and Davis, Jv and Dhillon, Is},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/metric{\_}kernel{\_}learning{\_}jmlr12.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning},
pages = {519--547},
title = {{Metric and kernel learning using a linear transformation}},
url = {http://dl.acm.org/citation.cfm?id=2188402},
volume = {13},
year = {2012}
}
@book{Siekmann1887,
author = {Siekmann, J and Hartmanis, J and Leeuwen, J Van},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-642-29066-4{_}11},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/bok{\%}3A978-3-642-15880-3.pdf:pdf},
isbn = {354042315X},
title = {{Lecture Notes in Artificial Intelligence}},
year = {1887}
}
@article{Hern2015,
author = {Hern, Daniel},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/hernandez-lobatoa15.pdf:pdf},
journal = {Icml-2015},
title = {{A Probabilistic Model for Dirty Multi-task Feature Selection}},
volume = {37},
year = {2015}
}
@book{Shawe-Taylor2004,
author = {Shawe-Taylor, John and Cristianini, Nello},
publisher = {Cambridge University Press},
title = {{Kernel Methods for Pattern Analysis}},
year = {2004}
}
@article{Friedman1998,
author = {Friedman, J and Hastie, T and Tibshirani, R},
doi = {10.1214/aos/1016218223},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/10.1.1.51.9525.pdf:pdf},
issn = {0090-5364, 2168-8966},
title = {{Additive Logistic Regression: a Statistical View of Boosting}},
year = {1998}
}
@article{Xu2013,
abstract = {Multiple kernel learning (MKL) has been proposed for kernel methods by learning the optimal kernel from a set of predefined base kernels. However, the traditional L1MKL method often achieves worse results than the simplest method using the average of base kernels (i.e., average kernel) in some practical applications. In order to improve the effectiveness of MKL, this paper presents a novel soft margin perspective for MKL. Specifically, we introduce an additional slack variable called kernel slack variable to each quadratic constraint of MKL, which corresponds to one support vector machine model using a single base kernel. We first show that L1MKL can be deemed as hard margin MKL, and then we propose a novel soft margin framework for MKL. Three commonly used loss functions, including the hinge loss, the square hinge loss, and the square loss, can be readily incorporated into this framework, leading to the new soft margin MKL objective functions. Many existing MKL methods can be shown as special cases under our soft margin framework. For example, the hinge loss soft margin MKL leads to a new box constraint for kernel combination coefficients. Using different hyper-parameter values for this formulation, we can inherently bridge the method using average kernel, L1MKL, and the hinge loss soft margin MKL. The square hinge loss soft margin MKL unifies the family of elastic net constraint/regularizer based approaches; and the square loss soft margin MKL incorporates L2MKL naturally. Moreover, we also develop efficient algorithms for solving both the hinge loss and square hinge loss soft margin MKL. Comprehensive experimental studies for various MKL algorithms on several benchmark data sets and two real world applications, including video action recognition and event recognition demonstrate that our proposed algorithms can efficiently achieve an effective yet sparse solution for MKL.},
author = {Xu, Xinxing and Tsang, Ivor W. and Xu, Dong},
doi = {10.1109/TNNLS.2012.2237183},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/smmkl.pdf:pdf},
issn = {2162237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Multiple kernel learning,Support vector machines.},
number = {5},
pages = {749--761},
pmid = {24808425},
title = {{Soft margin multiple kernel learning}},
volume = {24},
year = {2013}
}
@article{Anguita2013d,
author = {Anguita, Davide and Ghio, Alessandro and Lawal, Isah Abdullahi and Oneto, Luca},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C15.pdf:pdf},
keywords = {model selection,online learning,support vector machines},
pages = {77--78},
title = {{A Heuristic Approach to Model Selection for Online Support Vector Machines}},
year = {2013}
}
@article{Oneto2014b,
author = {Oneto, Luca and Ghio, Alessandro and Anguita, Davide and Ridella, Sandro},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Workshop/W5 - NIPS 2013.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Workshop/W5 - P{\_}NIPS{\_}2013.pdf:pdf},
pages = {1--4},
title = {{Learning With Few Bits on Small – Scale Devices}},
year = {2014}
}
@article{H??llermeier2008,
abstract = {Preference learning is an emerging topic that appears in different guises in the recent literature. This work focuses on a particular learning scenario called label ranking, where the problem is to learn a mapping from instances to rankings over a finite number of labels. Our approach for learning such a mapping, called ranking by pairwise comparison (RPC), first induces a binary preference relation from suitable training data using a natural extension of pairwise classification. A ranking is then derived from the preference relation thus obtained by means of a ranking procedure, whereby different ranking methods can be used for minimizing different loss functions. In particular, we show that a simple (weighted) voting strategy minimizes risk with respect to the well-known Spearman rank correlation. We compare RPC to existing label ranking methods, which are based on scoring individual labels instead of comparing pairs of labels. Both empirically and theoretically, it is shown that RPC is superior in terms of computational efficiency, and at least competitive in terms of accuracy. ?? 2008 Elsevier B.V. All rights reserved.},
author = {Hullermeier, Eyke and Furnkranz, Johannes and Cheng, Weiwei and Brinker, Klaus},
doi = {10.1016/j.artint.2008.08.002},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/LabelRankingByLearningPairwisePreferences.pdf:pdf},
isbn = {0004-3702},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Constraint classification,Pairwise classification,Preference learning,Ranking},
number = {16-17},
pages = {1897--1916},
title = {{Label ranking by learning pairwise preferences}},
volume = {172},
year = {2008}
}
@article{Lee2015,
author = {Lee, Chan-Su and Elgammal, Ahmed and Torki, Marwan},
doi = {10.1016/j.patcog.2015.08.024},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S003132031500312X-main.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Correspondence,Dimensionality reduction,Joint manifold representation,Manifold learning},
publisher = {Elsevier},
title = {{Learning representations from multiple manifolds}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S003132031500312X},
year = {2015}
}
@article{Anguita2013,
author = {Anguita, D and Ghio, a and Oneto, L and Ridella, S},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C12.pdf:pdf},
isbn = {9782874190810},
journal = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
title = {{A Learning Machine with a Bit-Based Hypothesis Space}},
year = {2013}
}
@article{Anguita,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C02.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/TechReport/TR2 - SmartLab{\_}Report.pdf:pdf},
number = {Md},
title = {{Maximal Discrepancy Vs . Rademacher Complexity for Error Estimation}}
}
@article{Mairal2014a,
abstract = {An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.3332v1},
author = {Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
eprint = {arXiv:1406.3332v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1406.3332v2.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--10},
title = {{Convolutional Kernel Networks}},
url = {http://arxiv.org/abs/1406.3332},
year = {2014}
}
@article{Cotter2011,
abstract = {We investigate training and using Gaussian kernel SVMs by approximating the kernel with an explicit finite- dimensional polynomial feature representation based on the Taylor expansion of the exponential. Although not as efficient as the recently-proposed random Fourier features [Rahimi and Recht, 2007] in terms of the number of features, we show how this polynomial representation can provide a better approximation in terms of the computational cost involved. This makes our "Taylor features" especially attractive for use on very large data sets, in conjunction with online or stochastic training.},
archivePrefix = {arXiv},
arxivId = {1109.4603},
author = {Cotter, Andrew and Keshet, Joseph and Srebro, Nathan},
eprint = {1109.4603},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1109.4603v1.pdf:pdf},
journal = {arXiv preprint arXiv:1109.4603},
pages = {11},
title = {{Explicit Approximations of the Gaussian Kernel}},
url = {http://arxiv.org/abs/1109.4603},
year = {2011}
}
@article{Rudin2007,
abstract = {We introduce a useful tool for analyzing boosting algorithms called the ``smooth margin function,'' a differentiable approximation of the usual margin for boosting algorithms. We present two boosting algorithms based on this smooth margin, ``coordinate ascent boosting'' and ``approximate coordinate ascent boosting,'' which are similar to Freund and Schapire's AdaBoost algorithm and Breiman's arc-gv algorithm. We give convergence rates to the maximum margin solution for both of our algorithms and for arc-gv. We then study AdaBoost's convergence properties using the smooth margin function. We precisely bound the margin attained by AdaBoost when the edges of the weak classifiers fall within a specified range. This shows that a previous bound proved by R$\backslash$"{\{}a{\}}tsch and Warmuth is exactly tight. Furthermore, we use the smooth margin to capture explicit properties of AdaBoost in cases where cyclic behavior occurs.},
archivePrefix = {arXiv},
arxivId = {0803.4092},
author = {Rudin, Cynthia and Schapire, Robert E. and Daubechies, Ingrid},
doi = {10.1214/009053607000000785},
eprint = {0803.4092},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0803.4092.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {AdaBoost,Arc-gv,Boosting,Convergence rates,Coordinate descent,Large margin classification},
number = {6},
pages = {2723--2768},
title = {{Analysis of boosting algorithms using the smooth margin function}},
volume = {35},
year = {2007}
}
@article{Hoffmann2007,
abstract = {Kernel principal component analysis (kernel PCA) is a non-linear extension of PCA. This study introduces and investigates the use of kernel PCA for novelty detection. Training data are mapped into an infinite-dimensional feature space. In this space, kernel PCA extracts the principal components of the data distribution. The squared distance to the corresponding principal subspace is the measure for novelty. This new method demonstrated a competitive performance on two-dimensional synthetic distributions and on two real-world data sets: handwritten digits and breast-cancer cytology. ?? 2006 Pattern Recognition Society.},
author = {Hoffmann, Heiko},
doi = {10.1016/j.patcog.2006.07.009},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffmann - 2007 - Kernel PCA for novelty detection.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Breast cancer,Handwritten digit,Kernel method,Novelty detection,PCA},
pages = {863--874},
title = {{Kernel PCA for novelty detection}},
volume = {40},
year = {2007}
}
@article{Biggio2012,
abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted train- ing data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms as- sume that their training data comes from a natural or well-behaved distribution. How- ever, this assumption does not generally hold in security-sensitive settings. As we demon- strate, an intelligent adversary can, to some extent, predict the change of the SVM's deci- sion function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal so- lution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradi- ent ascent procedure reliably identifies good local maxima of the non-convex validation er- ror surface, which significantly increases the classifier's test error.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.6389v2},
author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
eprint = {arXiv:1206.6389v2},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/880.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
pages = {1807--1814},
title = {{Poisoning Attacks against Support Vector Machines}},
year = {2012}
}
@article{Comon2007,
author = {Comon, Pierre and Lim, Lek-heng},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/iciam.pdf:pdf},
journal = {Analysis},
number = {Iciam},
pages = {1--30},
title = {{Multilinear algebra in machine learning and signal processing}},
year = {2007}
}
@article{Chapelle2000,
author = {Chapelle, O and Chapelle, O and Vapnik, V and Vapnik, V},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C04.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 12},
title = {{Model Selection for Support Vector Machines}},
url = {http://www.ens-lyon.fr/{\{}$\backslash${~}{\}}ochapell/ms{\_}nips99.ps},
year = {2000}
}
@article{Cho2009,
abstract = {We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets},
author = {Cho, Youngmin and Saul, Lawrence K},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/nips09{\_}kernel.pdf:pdf},
journal = {Nips},
pages = {342--350},
title = {{Kernel Methods for Deep Learning}},
volume = {9},
year = {2009}
}
@article{Orsenigo2004,
abstract = {An algorithm is proposed for generating decision trees in which multivariate splitting rules are based on the new concept of discrete support vector machines. By this term a discrete version of SVMs is denoted in which the error is properly expressed as the count of misclassified instances, in place of a proxy of the misclassification distance considered by traditional SVMs. The resulting mixed integer programming problem formulated at each node of the decision tree is then efficiently solved by a tabu search heuristic. Computational tests performed on both well-known benchmark and large marketing datasets indicate that the proposed algorithm consistently outperforms other classification approaches in terms of accuracy, and is therefore capable of good generalization on validation sets. ?? 2003 Published by Elsevier B.V.},
author = {Orsenigo, Carlotta and Vercellis, Carlo},
doi = {10.1016/j.csda.2003.11.005},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0167947303002779-main.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Classification,Decision trees,Support vector machines,Tabu search},
number = {2 SPEC. ISS.},
pages = {311--322},
title = {{Discrete support vector decision trees via tabu search}},
volume = {47},
year = {2004}
}
@article{Cayton2005,
abstract = {Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to find a low-dimensional representation of the data. In this paper, we discuss the motivation, background, and algorithms proposed for manifold learning. Isomap, Locally Linear Embedding, Laplacian Eigenmaps, Semidefinite Embedding, and a host of variants of these algorithms are examined.},
author = {Cayton, Lawrence},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cayton - 2005 - Algorithms for manifold learning.pdf:pdf},
isbn = {CS2008-0923},
journal = {Univ of California at San Diego Tech Rep},
number = {CS2008-0923},
pages = {973--80},
title = {{Algorithms for manifold learning}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.117.511{\&}rep=rep1{\&}type=pdf},
volume = {44},
year = {2005}
}
@article{Micchelli2005,
author = {Micchelli, C and Pontil, M and WU, Q and Zhou, Ding-Xuan},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/eb-lern-kernel.pdf:pdf},
title = {{Error bounds for learning the kernel}},
url = {http://eprints.pascal-network.org/archive/00001014/},
volume = {68},
year = {2005}
}
@article{Ahmed2014,
author = {Ahmed, Amr and Das, Abhimanyu and Smola, Alexander J},
doi = {10.1145/2556195.2556264},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/wsdm2014-multitask.pdf:pdf},
isbn = {9781450323512},
journal = {Proceedings of the 7th ACM international conference on Web search and data mining},
pages = {153--162},
title = {{Scalable hierarchical multitask learning algorithms for conversion optimization in display advertising}},
year = {2014}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/lecun-98.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Cortes2012,
abstract = {This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similaritymeasure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning amaximumalignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression. Keywords: kernel methods, learning kernels, feature selection 1.},
archivePrefix = {arXiv},
arxivId = {1203.0550},
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
eprint = {1203.0550},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes, Mohri, Rostamizadeh - 2012 - Algorithms for learning kernels based on centered alignment.pdf:pdf;:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes, Mohri, Rostamizadeh - 2012 - Algorithms for learning kernels based on centered alignment(2).pdf:pdf},
issn = {15324435},
journal = {The Journal of Machine Learning {\ldots}},
keywords = {feature selection,kernel methods,learning kernels},
pages = {795--828},
title = {{Algorithms for learning kernels based on centered alignment}},
url = {http://dl.acm.org/citation.cfm?id=2188413},
volume = {13},
year = {2012}
}
@article{Bach2007,
abstract = {We consider the least-square regression problem with regularization by a block 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1-norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic model consistency of the group Lasso. We derive necessary and sufficient conditions for the consistency of group Lasso under practical assumptions, such as model misspecification. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this infinite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisfied.},
archivePrefix = {arXiv},
arxivId = {0707.3390},
author = {Bach, Francis},
doi = {10.1007/s10182-011-0155-4},
eprint = {0707.3390},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/bach08b.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
pages = {1179--1225},
pmid = {11974822},
title = {{Consistency of the group Lasso and multiple kernel learning}},
url = {http://eprints.pascal-network.org/archive/00003190/},
volume = {9},
year = {2007}
}
@article{Baxter2000,
author = {Baxter, Jonathan},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/live-731-1898-jair.pdf:pdf},
journal = {Air},
pages = {149--198},
title = {{A Model of Inductive Bias Learning}},
volume = {12},
year = {2000}
}
@article{Si2014,
author = {Si, Si and Hsieh, Cho-Jui and Dhillon, Inderjit},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Si, Hsieh, Dhillon - 2014 - Memory Efficient Kernel Approximation.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st International Conference on Machine Learning},
pages = {701--709},
title = {{Memory Efficient Kernel Approximation}},
url = {http://jmlr.org/proceedings/papers/v32/si14.html},
volume = {32},
year = {2014}
}
@article{Anguita2012a,
abstract = {Activity-Based Computing [1] aims to capture the state of the user and its environment by exploiting heterogeneous sensors in order to provide adaptation to exogenous computing resources. When these sensors are attached to the subject's body, they permit continu- ous monitoring of numerous physiological signals. This has appealing use in healthcare applications, e.g. the exploitation of Ambient Intelligence (AmI) in daily activity monitoring for elderly people. In this paper, we present a system for human physical Activity Recognition (AR) using smartphone inertial sensors. As these mobile phones are limited in terms of energy and computing power, we propose a novel hardware-friendly approach for multiclass classification. This method adapts the standard Support Vector Machine (SVM) and exploits fixed-point arithmetic for computational cost reduction. A comparison with the traditional SVM shows a significant improvement in terms of computational costs while maintaining similar accuracy, which can contribute to develop more sus- tainable systems for AmI.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and Reyes-Ortiz, Jorge L.},
doi = {10.1007/978-3-642-35395-6_30},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C10.pdf:pdf},
isbn = {9783642353949},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Activity Recognition,Hardware-Friendly,SVM,Smartphones},
pages = {216--223},
title = {{Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine}},
volume = {7657 LNCS},
year = {2012}
}
@article{Moreno2005,
author = {Peluffo-Ordonez, Diego Hernan and Castro-Ospina, Andres Eduardo and Alvarado-Perez, Juan Carlos and Revelo-Fuelagan, Edgardo Javier},
doi = {10.1007/11578079},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/mkl{\_}spec{\_}dim{\_}red.pdf:pdf},
isbn = {978-3-540-29850-2},
keywords = {dimensionality reduction,multiple kernel learning,pca},
pages = {1--12},
title = {{Multiple Kernel Learning for Spectral Dimensionality Reduction}},
url = {http://dl.acm.org/citation.cfm?id=2099369.2099371},
volume = {3773},
year = {2005}
}
@article{Kloft2012,
abstract = {We derive an upper bound on the local Rademacher complexity of p-norm multiple kernel learning, which yields a tighter excess risk bound than global approaches. Previous local approaches analyzed the case p = 1 only while our
analysis covers all cases, assuming the different feature mappings corresponding to the different kernels to be uncorrelated. We also show a lower bound that shows that the bound is tight, and derive consequences regarding excess loss, namely fast convergence rates.},
archivePrefix = {arXiv},
arxivId = {1103.0790},
author = {Kloft, Marius and Blanchard, Gilles},
eprint = {1103.0790},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kloft, Blanchard - 2012 - The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning.pdf:pdf},
keywords = {Learning/Statistics {\&} Optimisation},
pages = {1--31},
title = {{The Local Rademacher Complexity of Lp-Norm Multiple Kernel Learning}},
url = {http://eprints.pascal-network.org/archive/00009401/},
year = {2012}
}
@article{Zhang2003,
author = {Zhang, Tong},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Kernel Learning/10.1.1.69.4762.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 15},
pages = {454--461},
title = {{Effective Dimension and Generalization of Kernel Learning}},
url = {http://papers.nips.cc/paper/2196-effective-dimension-and-generalization-of-kernel-learning.pdf$\backslash$nfiles/4016/Zhang - 2003 - Effective Dimension and Generalization of Kernel L.pdf$\backslash$nfiles/4017/2196-effective-dimension-and-generalization-of-kernel-learning.htm},
year = {2003}
}
@article{Donoho2000,
abstract = {The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the better- known sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of par- ticular phenomena (e.g. instance ↔ human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height, ...). In traditional statistical methodology, we assumed many observations and a few, well- chosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables – voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or even movies, so that a single observation has dimensions in the thousands or billions, while there are only tens or hundreds of instances available for study. Classical methods are simply not designed to cope with this kind of explosive growth of dimensionality of the observation vector. We can say with complete confidence that in the coming cen- tury, high-dimensional data analysis will be a very significant activity, and completely new methods of high-dimensional data analysis will be developed; we just don't know what they are yet. Mathematicians are ideally prepared for appreciating the abstract issues involved in finding patterns in such high-dimensional data. Two of the most influential prin- ciples in the coming century will be principles originally discovered and cultivated by mathematicians: the blessings of dimensionality and the curse of dimensionality. The curse of dimensionality is a phrase used by several subfields in the mathematical sciences; I use it here to refer to the apparent intractability of systematically searching through a high-dimensional space, the apparent intractability of accurately approxi- mating a general high-dimensional function, the apparent intractability of integrating a high-dimensional function. The blessings of dimensionality are less widely noted, but they include the concen- tration of measure phenomenon (so-called in the geometry of Banach spaces), which means that certain random fluctuations are very well controlled in high dimensions and the success of asymptotic methods, used widely in mathematical statistics and statistical physics, which suggest that statements about very high-dimensional settings may be made where moderate dimensions would be too complicated. There is a large body of interesting work going on in the mathematical sciences, both to attack the curse of dimensionality in specific ways, and to extend the benefits of dimensionality. I will mention work in high-dimensional approximation theory, in probability theory, and in mathematical statistics. I expect to see in the coming decades many further mathematical elaborations to our inventory of Blessings and Curses, and I expect such contributions to have a broad impact on society's ability to extract meaning from the massive datasets it has decided to compile. In my talk, I will also draw on my personal research experiences which suggest to me (1) there are substantial chances that by interpreting ongoing development in high-dimensional data analysis, mathematicians can become aware of new problems in harmonic analysis; and (2) that many of the problems of data analysis even in fairly low dimensions are unsolved and are similar to problems in mathematics which have only recently been attacked, and for which only the merest beginnings have been made. Both fields can progress together.},
author = {Donoho, Dl},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/16486-Donoho.pdf:pdf},
journal = {AMS Math Challenges Lecture},
keywords = {Data Mining. Multivariate Data Analysis. Principal},
pages = {1--33},
title = {{High-dimensional data analysis: The curses and blessings of dimensionality}},
url = {http://mlo.cs.man.ac.uk/resources/Curses.pdf},
year = {2000}
}
@article{Ying2009,
abstract = {In this paper we develop a novel probabilistic generalization bound for learning the kernel problem. First, we show that the generalization analysis of the kernel learning algorithms reduces to investigation of the suprema of the Rademacher chaos process of order two over candidate kernels, which we refer to as Rademacher chaos complexity. Next, we show how to estimate the empirical Rademacher chaos complexity by well-established metric entropy integrals and pseudo-dimension of the set of candidate kernels. Our new methodology mainly depends on the principal theory of U-processes. Finally, we establish satisfactory excess generalization bounds and misclassification error rates for learning Gaussian kernels and general radial basis kernels.},
author = {Ying, Yiming and Campbell, Colin},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/colt-2009-final.pdf:pdf},
journal = {Proceedings 22nd Annual Conference on Learning Theory (COLT)},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
number = {1},
pages = {247--254},
title = {{Generalization Bounds for Learning the Kernel}},
url = {http://eprints.pascal-network.org/archive/00004263/},
year = {2009}
}
@article{Bhadra2016,
abstract = {In this paper, we introduce the first method that (1) can complete kernel matrices with completely missing rows and columns as opposed to individual missing kernel values, (2) does not require any of the kernels to be complete a priori, and (3) can tackle non-linear kernels. These aspects are necessary in practical applications such as integrating legacy data sets, learning under sensor failures and learning when measurements are costly for some of the views. The proposed approach predicts missing rows by modelling both within-view and between-view relationships among kernel values. We show, both on simulated data and real world data, that the proposed method outperforms existing techniques in the restricted settings where they are available, and extends applicability to new settings.},
archivePrefix = {arXiv},
arxivId = {1602.02518},
author = {Bhadra, Sahely and Kaski, Samuel and Rousu, Juho},
eprint = {1602.02518},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1602.02518.pdf:pdf},
title = {{Multi-view Kernel Completion}},
url = {http://arxiv.org/abs/1602.02518},
year = {2016}
}
@article{Loosli,
author = {Loosli, Ga{\"{e}}lle and Universit{\'{e}}, Clermont and Pascal, Universit{\'{e}} Blaise},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/10.1.1.394.4922.pdf:pdf},
pages = {1--36},
title = {{SVM in Kre ˘ ın spaces}}
}
@article{Cristianini2002,
abstract = {Editor: Kernel based methods are increasingly being used for data modeling because of their conceptual simplicity and outstanding performance on many tasks. However, the kernel function is often chosen using trial-and-error heuristics. In this paper we address the problem of measuring the degree of agreement between a kernel and a learning task. A quantitative measure of agreement is important from both a theoretical and practical point of view. We propose a quantity to capture this notion, which we call Alignment. We study its theoretical properties, and derive a series of simple algorithms for adapting a kernel to the labels and vice versa. This produces a series of novel methods for clustering and transduction, kernel combination and kernel selection. The algorithms are tested on two publicly available datasets and are shown to exhibit good performance.},
author = {Cristianini, Nello and Kandola, J and Elisseeff, Andre and Shawe-Taylor, John},
doi = {10.1.1.23.6757},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1946-on-kernel-target-alignment.pdf:pdf},
isbn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 14},
keywords = {alignment,eigenvalues,eigenvectors,kernels,transduction},
pages = {367----373},
title = {{On kernel-target alignment}},
year = {2002}
}
@article{Science2005,
author = {Buhlmann, Peter and Hothorn, Torsten},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/buehlmann.pdf:pdf},
keywords = {and phrases,ent boosting,generalized additive models,generalized linear models,gradi-,software,survival analysis,variable selection},
number = {1997},
pages = {1--19},
title = {{BOOSTING ALGORITHMS: REGULARIZATION, PREDICTION AND MODEL FITTING}},
year = {2005}
}
@book{Principe2010,
abstract = {This book presents the first cohesive treatment of Information Theoretic Learning (ITL) algorithms to adapt linear or nonlinear learning machines both in supervised or unsupervised paradigms. ITL is a framework where the conventional concepts of second order statistics (covariance, L2 distances, correlation functions) are substituted by scalars and functions with information theoretic underpinnings, respectively entropy, mutual information and correntropy. ITL quantifies the stochastic structure of the data beyond second order statistics for improved performance without using full-blown Bayesian approaches that require a much larger computational cost. This is possible because of a non-parametric estimator of Renyis quadratic entropy that is only a function of pairwise differences between samples. The book compares the performance of ITL algorithms with the second order counterparts in many engineering and machine learning applications. Students, practitioners and researchers interested in statistical signal processing, computational intelligence, and machine learning will find in this book the theory to understand the basics, the algorithms to implement applications, and exciting but still unexplored leads that will provide fertile ground for future research.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Principe, Jose C},
booktitle = {Xtemp01},
doi = {10.1007/978-1-4419-1570-2},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/J.Principe{\_}entropy{\_}learning.pdf:pdf},
isbn = {9781441915696},
issn = {1098-6596},
number = {XIV},
pages = {462},
pmid = {25246403},
title = {{Information Theoretic Learning: Renyi's Entropy and Kernel Perspectives}},
url = {http://www.amazon.com/Information-Theoretic-Learning-Perspectives-Statistics/dp/1441915699},
year = {2010}
}
@inproceedings{Kumar2015,
author = {Kumar, Vipin and Minz, Sonajharia},
booktitle = {Proceedings of the Third International Symposium on Women in Computing and Informatics.},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/p31-kumar.pdf:pdf},
pages = {31--37},
title = {{Multi-view Ensemble Learning: A Supervised Feature Set Partitioning for High Dimensional Data Classification}},
year = {2015}
}
@article{Huynh2005,
abstract = {Human activity is one of the most important ingredients of context information. In wearable computing scenarios, activities such as walking, standing and sitting can be inferred from data provided by body-worn acceleration sensors. In such settings, most approaches use a single set of features, regardless of which activity to be recognized. In this paper we show that recognition rates can be improved by careful selection of individual features for each activity. We present a systematic analysis of features computed from a real-world data set and show how the choice of feature and the window length over which the feature is computed affects the recognition rates for different activities. Finally, we give a recommendation of suitable features and window lengths for a set of common activities},
author = {Huynh, T{\^{a}}m and Schiele, Bernt},
doi = {10.1145/1107548.1107591},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/huynh2005eusai.pdf:pdf},
isbn = {1595933042},
journal = {Proceedings of the 2005 joint conference on Smart objects and ambient intelligence: innovative context-aware services: usages and technologies},
number = {october},
pages = {159--163},
title = {{Analyzing Features for Activity Recognition}},
url = {http://dl.acm.org/citation.cfm?id=1107591},
year = {2005}
}
@article{Micchelli2007,
author = {Micchelli, Charles A. and Pontil, Massimiliano},
doi = {10.1007/s10994-006-0679-0},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2Fs10994-006-0679-0.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Banach space regularization,Convex optimization,Kernel methods,Learning the kernels,Sparsity},
number = {2-3},
pages = {297--319},
title = {{Feature space perspectives for learning the kernel}},
volume = {66},
year = {2007}
}
@article{Shawe-Taylor1996,
abstract = {The paper introduces some generalizations of Vapnik's method of structural risk minimization (SRM). As well as making explicit some of the details on SRM, it provides a result that allows one to trade off errors on the training sample against improved generalization performance. It then considers the more general case when the hierarchy of classes is chosen in response to the data. A result is presented on the generalization performance of classifiers with a "large margin." This theoretically explains the impressive generalization performance of the maximal margin hyperplane algorithm of Vapnik and co-workers (which is the basis for their support vector machines). The paper concludes with a more general result in terms of "luckiness" functions, which provides a quite general way for exploiting serendipitous simplicity in observed data to obtain better prediction accuracy from small training sets. Four examples are given of such functions, including the Vapnik-Chervonenkis (VC) dimension measured on the sample.},
author = {Shawe-Taylor, J and Bartlett, P L and Williamson, Robert C and Anthony, M},
doi = {10.1109/18.705570},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/00705570.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
number = {5},
pages = {1926--1940},
title = {{Structural Risk Minimization over Data-Dependent Hierarchies}},
volume = {44},
year = {1996}
}
@article{Hussain2011a,
abstract = {The paper $\backslash$cite{\{}hs-11{\}} presented a bound on the generalisation error of classifiers learned through multiple kernel learning. The bound has (an improved) $\backslash$emph{\{}additive{\}} dependence on the number of kernels (with the same logarithmic dependence on this number). However, parts of the proof were incorrectly presented in that paper. This note remedies this weakness by restating the problem and giving a detailed proof of the Rademacher complexity bound from $\backslash$cite{\{}hs-11{\}}.},
archivePrefix = {arXiv},
arxivId = {1106.6258},
author = {Hussain, Zakria and Shawe-Taylor, John},
eprint = {1106.6258},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1106.6258v2.pdf:pdf},
issn = {15324435},
journal = {arXiv preprint arXiv:1106.6258},
number = {2004},
pages = {1--11},
title = {{A Note on Improved Loss Bounds for Multiple Kernel Learning}},
url = {http://arxiv.org/abs/1106.6258},
volume = {15},
year = {2011}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
archivePrefix = {arXiv},
arxivId = {026218253X},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
eprint = {026218253X},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/RW.pdf:pdf},
isbn = {026218253X},
issn = {0129-0657},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
volume = {14},
year = {2004}
}
@article{Burch2001,
author = {Burch, Carl},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/MVML{\_}final.pdf:pdf},
isbn = {8621543451},
keywords = {active learning,analysis,canonical correlation,clustering,co-regularization,co-training,dimensionality reduction,ensemble learning,learning,multi-view learning,semi-supervised,statistical learning theory,supervised learning,transfer},
title = {{A survey of machine learning}},
year = {2001}
}
@article{Oneto2014,
author = {Oneto, Luca and Ghio, Alessandro and Sam, Albert and Anguita, Davide and Parra, Xavier},
doi = {10.1007/978-3-319-11179-7-23},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C22.pdf:pdf},
isbn = {9783319111780},
issn = {16113349},
keywords = {human activity recognition,postural tran-,sitions,smartphones,support vector machines,temporal filtering},
pages = {177--184},
title = {{Human Activity Recognition on Smartphones with Awareness of Basic Activities and Postural Transitions}},
year = {2014}
}
@article{Eom2009,
abstract = {Prediction of respiratory motion has the potential to substantially improve cancer radiation therapy. A nonlinear finite element (FE) model of respiratory motion during full breathing cycle has been developed based on patient specific pressure-volume relationship and 4D Computed Tomography (CT) data. For geometric modeling of lungs and ribcage we have constructed intermediate CAD surface which avoids multiple geometric smoothing procedures. For physiologically relevant respiratory motion modeling we have used pressure-volume (PV) relationship to apply pressure loading on the surface of the model. A hyperelastic soft tissue model, developed from experimental observations, has been used. Additionally, pleural sliding has been considered which results in accurate deformations in the superior-inferior (SI) direction. The finite element model has been validated using 51 landmarks from the CT data. The average differences in position is seen to be 0.07 cm (SD = 0.20 cm), 0.07 cm (0.15 cm), and 0.22 cm (0.18 cm) in the left-right, anterior-posterior, and superior-inferior directions, respectively. {\textcopyright} 2009 Springer-Verlag.},
author = {Hinrichs, Chris and Singh, Vikas and Xu3, Guofan and Johnson3, Sterling},
doi = {10.1007/978-3-642-04271-3},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/nihms-194125.pdf:pdf},
isbn = {978-3-642-04270-6},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {348--355},
title = {{MKL for robust Multi-modality AD Classification}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77952257660{\&}partnerID=tZOtx3y1},
volume = {5762},
year = {2009}
}
@article{Gong2012a,
abstract = {Multi-task learning (MTL) aims to improve the performance of multiple related tasks by exploiting the intrinsic relationships among them. Recently, multi-task feature learning algorithms have received increasing attention and they have been successfully applied to many applications involving high-dimensional data. However, they assume that all tasks share a common set of features, which is too restrictive and may not hold in real-world applications, since outlier tasks often exist. In this paper, we propose a Robust MultiTask Feature Learning algorithm (rMTFL) which simultaneously captures a common set of features among relevant tasks and identifies outlier tasks. Specifically, we decompose the weight (model) matrix for all tasks into two components. We impose the well-known group Lasso penalty on row groups of the first component for capturing the shared features among relevant tasks. To simultaneously identify the outlier tasks, we impose the same group Lasso penalty but on column groups of the second component. We propose to employ the accelerated gradient descent to efficiently solve the optimization problem in rMTFL, and show that the proposed algorithm is scalable to large-size problems. In addition, we provide a detailed theoretical analysis on the proposed rMTFL formulation. Specifically, we present a theoretical bound to measure how well our proposed rMTFL approximates the true evaluation, and provide bounds to measure the error between the estimated weights of rMTFL and the underlying true weights. Moreover, by assuming that the underlying true weights are above the noise level, we present a sound theoretical result to show how to obtain the underlying true shared features and outlier tasks (sparsity patterns). Empirical studies on both synthetic and real-world data demonstrate that our proposed rMTFL is capable of simultaneously capturing shared features among tasks and identifying outlier tasks.},
author = {Gong, Pinghua and Ye, Jieping and Zhang, Changshui},
doi = {10.1145/2339530.2339672},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2012{\_}KDD{\_}RobustMTL.pdf:pdf},
isbn = {9781450314626},
issn = {2154-817X},
journal = {KDD : proceedings / International Conference on Knowledge Discovery {\&} Data Mining. International Conference on Knowledge Discovery {\&} Data Mining},
keywords = {feature selection,multi-task learning,outlier tasks detection},
pages = {895--903},
pmid = {24078896},
title = {{Robust Multi-Task Feature Learning.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3783219{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2012},
year = {2012}
}
@article{Weinberger2009a,
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
author = {Weinberger, Kilian Q and Saul, Lawrence K},
doi = {10.1126/science.277.5323.215},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/jmlr10{\_}weinberger09a.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Metric learning,convex optimization,ing,mahalanobis distance,metric learn-,multi-class classification,semi-definite programming,support vector machines},
pages = {207--244},
pmid = {17490632},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
volume = {10},
year = {2009}
}
@article{Bootkrajang2012,
author = {Bootkrajang, Jakramate and Kab{\'{a}}n, Ata},
doi = {10.1007/978-3-642-33460-3_15},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1125527.pdf:pdf},
isbn = {9783642334597},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {143--158},
title = {{Label-noise robust logistic regression and its applications}},
volume = {7523 LNAI},
year = {2012}
}
@article{Jose2013,
author = {Jose, Cijo and Goyal, Prasoon and Aggrwal, Parv and Varma, Manik},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Local Deep Kernel Learning for Eﬃcient Non-linear SVM Prediction.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {486--494},
title = {{Local Deep Kernel Learning for Efficient Non-linear SVM Prediction}},
url = {http://jmlr.org/proceedings/papers/v28/jose13.html},
volume = {28},
year = {2013}
}
@article{Huang2008,
abstract = {In order to overcome the overfitting problem caused by noises and outliers in support vector regression (SVR) ,a weighted coefficient model based on support vector data description (SVDD) is presented in this paper. The weighted coefficient value to each input sample is confirmed according to its distance to the center of the smallest enclosing hypersphere in the feature space. The proposed model is applied to weighted support vector regression (WSVR) for 1-dimensional data set simulation. Simulation results indicate that the proposed method actually reduces the error of regression and yields higher accuracy than support vector regression (SVR) does.},
author = {Huang, Weimin Huang Weimin and Shen, Leping Shen Leping},
doi = {10.1109/CCCM.2008.25},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/04609509.pdf:pdf},
isbn = {978-0-7695-3290-5},
journal = {2008 ISECS International Colloquium on Computing, Communication, Control, and Management},
keywords = {Algorithm,Data Description,Support Vector Regression},
pages = {250--254},
title = {{Weighted Support Vector Regression Algorithm Based on Data Description}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4609509},
volume = {1},
year = {2008}
}
@article{Anguita2010,
abstract = {A common belief is that Machine Learning Theory (MLT) is not very useful, in pratice, for performing effective SVM model selection. This fact is supported by experience, because well-known hold-out methods like cross-validation, leave-one-out, and the bootstrap usually achieve better results than the ones derived from MLT. We show in this paper that, in a small sample setting, i.e. when the dimensionality of the data is larger than the number of samples, a careful application of the MLT can outperform other methods in selecting the optimal hyperparameters of a SVM.},
author = {Anguita, D. and Ghio, a. and Greco, N. and Oneto, L. and Ridella, S.},
doi = {10.1109/IJCNN.2010.5596450},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C01.pdf:pdf},
isbn = {978-1-4244-6916-1},
issn = {1098-7576},
journal = {Neural Networks (IJCNN), The 2010 International Joint Conference on},
title = {{Model selection for support vector machines: Advantages and disadvantages of the Machine Learning Theory}},
year = {2010}
}
@article{Dinuzzo2011,
abstract = {A client{\&}x2013;server architecture to simultaneously solve multiple learning tasks from distributed datasets is described. In such architecture, each client corresponds to an individual learning task and the associated dataset of examples. The goal of the architecture is to perform information fusion from multiple datasets while preserving privacy of individual data. The role of the server is to collect data in real time from the clients and codify the information in a common database. Such information can be used by all the clients to solve their individual learning task, so that each client can exploit the information content of all the datasets without actually having access to private data of others. The proposed algorithmic framework, based on regularization and kernel methods, uses a suitable class of {\&}x201C;mixed effect{\&}x201D; kernels. The methodology is illustrated through a simulated recommendation system, as well as an experiment involving pharmacological data coming from a multicentric clinical trial.},
archivePrefix = {arXiv},
arxivId = {arXiv:0812.4235v2},
author = {Dinuzzo, Francesco and Pillonetto, Gianluigi and {De Nicolao}, Giuseppe},
doi = {10.1109/TNN.2010.2095882},
eprint = {arXiv:0812.4235v2},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0812.4235v2.pdf:pdf},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {collaborative filtering,conjoint analysis,inductive transfer,kernel methods,learning learn,multitask learning,population methods,recommender systems,regularization theory},
number = {2},
pages = {290--303},
pmid = {21156391},
title = {{Client-server multi-task learning from distributed datasets}},
volume = {22},
year = {2011}
}
@article{Bach2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.06800v2},
author = {Bach, Francis},
eprint = {arXiv:1502.06800v2},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/kernelexp{\_}jmlr{\_}revision.pdf:pdf},
pages = {1--38},
title = {{and Random Feature Expansions}},
year = {2015}
}
@article{Williams2002,
author = {Williams, Christopher K.I.},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Williams - 2002 - on a Connection Betwen Kernel Pca and Metric Multidimensional Scaling.pdf:pdf},
keywords = {eigenproblem,kernel pca,mds,metric multidimensional scaling},
pages = {11--19},
title = {{on a Connection Betwen Kernel Pca and Metric Multidimensional Scaling}},
year = {2002}
}
@article{Gomez-Chova2012,
abstract = {This letter proposes the kernel entropy component analysis for clustering remote sensing data. The method generates nonlinear features that reveal structure related to the R{\'{e}}nyi entropy of the input space data set. Unlike other kernel feature-extraction methods, the top eigenvalues and eigenvectors of the kernel matrix are not necessarily chosen. Data are interestingly mapped with a distinct angular structure, which is exploited to derive a new angle-based spectral clustering algorithm based on the mapped data. An out-of-sample extension of the method is also presented to deal with test data. We focus on cloud screening from Medium Resolution Imaging Spectrometer images. Several images are considered to account for the high variability of the problem. Good results obtained show the suitability of the proposal.},
author = {Gomez-Chova, L and Jenssen, R and Camps-Valls, G},
doi = {10.1109/LGRS.2011.2167212},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/06041013.pdf:pdf},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
number = {2},
pages = {312--316},
title = {{Kernel Entropy Component Analysis for Remote Sensing Image Clustering}},
url = {files/877/GRSL-Gomez-Chova{\_}et{\_}al-2012-Kernel{\_}Entropy{\_}Component{\_}Analysis{\_}for{\_}Remote{\_}Sensing{\_}Image{\_}Clustering.pdf},
volume = {9},
year = {2012}
}
@article{Maurer,
abstract = {We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent tasks learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.06279v1},
author = {Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
eprint = {arXiv:1505.06279v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1505.06279v1.pdf:pdf},
journal = {arXiv:1505.06279 [cs, stat]},
keywords = {learning theory,learning-to-learn,multitask learning,representation learning,statistical,transfer learning},
pages = {1--28},
title = {{The Benefit of Multitask Representation Learning}},
url = {http://arxiv.org/abs/1505.06279$\backslash$nhttp://www.arxiv.org/pdf/1505.06279.pdf},
year = {2015}
}
@article{Learning1999,
author = {Learning, Machine and Learning, Machine},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/LargeMarginsUsingPerceptron.pdf:pdf},
issn = {0885-6125},
number = {3},
title = {{Large Margin Classi cation Using the Perceptron Algorithm 1.}},
volume = {37},
year = {1999}
}
@article{Har-Peled2002,
author = {Har-Peled, S and Har-Peled, S and Roth, D and Roth, D and Zimak, D and Zimak, D},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2295-constraint-classification-for-multiclass-classification-and-ranking.pdf:pdf},
isbn = {0262025507},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {809--816},
title = {{Constraint classification for multiclass classification and ranking}},
year = {2002}
}
@article{Birlutiu2013,
abstract = {This paper presents a framework for optimizing the preference learning pro- cess. In many real-world applications in which preference learning is involved the avail- able training data is scarce and obtaining labeled training data is expensive. Luckily in many of the preference learning situations data is available from multiple sub jects. We use the multi-task formalism to enhance the individual training data by making use of the preference information learned from other sub jects. Furthermore, since obtain- ing labels is expensive, we optimally choose which data to ask a sub ject for labelling to obtain maximum of information about her/his preferences. This paradigm --called active learning-- has hardly been studied in a multi-task formalism. We propose an alternative for the standard criteria in active learning which actively chooses queries by making use of the available preference data from other sub jects. The advantage of this alternative is the reduced computation costs and reduced time sub jects are in- volved. We validate empirically our approach on two real-world data sets involving the preferences of people: an audiological data set and an art image data set. Keywords learning preferences, active learning, experimental design, multi-task learning, hierarchical modeling},
author = {Birlutiu, Adriana and Groot, Perry and Heskes, Tom},
doi = {10.1007/s10994-012-5297-4},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Birlutiu, Groot, Heskes - 2013 - Efficiently learning the preferences of people.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {Active learning,Experimental design,Hierarchical modeling,Learning preferences,Multi-task learning},
number = {1},
pages = {1--28},
title = {{Efficiently learning the preferences of people}},
volume = {90},
year = {2013}
}
@article{Kumar2012,
author = {Kumar, Abhishek and Niculescu-Mizil, Alexandru and Kavukcuoglu, Koray and Iii, Hal Daume and Daum, Hal},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/645.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
pages = {1295--1302},
title = {{A Binary Classification Framework for Two-Stage Multiple Kernel Learning}},
year = {2012}
}
@article{Squarcina2015a,
author = {Squarcina, Letizia and Castellani, Umberto and Bellani, Marcella and Perlini, Cinzia and Lasalvia, Antonio and Dusi, Nicola and Bonetto, Chiara and Cristofalo, Doriana and Tosato, Sarah and Rambaldelli, Gianluca and Alessandrini, Franco and Zoccatelli, Giada and Pozzi-Mucelli, Roberto and Lamonaca, Dario and Ceccato, Enrico and Pileggi, Francesca and Mazzi, Fausto and Santonastaso, Paolo and Ruggeri, Mirella and Brambilla, Paolo and Group, The G.E.T.U.P.},
doi = {10.1016/j.neuroimage.2015.12.007},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S1053811915011209-main.pdf:pdf},
issn = {10538119},
journal = {NeuroImage},
keywords = {Affective psychosis,Cortical thickness,Frontal,MRI,Schizophrenia,Temporal cortex},
publisher = {Elsevier B.V.},
title = {{Classification of first-episode psychosis in a large cohort of patients using support vector machine and multiple kernel learning techniques}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811915011209},
year = {2015}
}
@article{Nascimento2016,
author = {Nascimento, Andr{\'{e}} C. A. and Prud{\^{e}}ncio, Ricardo B. C. and Costa, Ivan G.},
doi = {10.1186/s12859-016-0890-3},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1186{\%}2Fs12859-016-0890-3.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {1186,17,2016,46,bmc bioinformatics,cimento et al,doi 10,s12859-016-0890-3},
number = {1},
pages = {46},
publisher = {BMC Bioinformatics},
title = {{A multiple kernel learning algorithm for drug-target interaction prediction}},
url = {http://www.biomedcentral.com/1471-2105/17/46},
volume = {17},
year = {2016}
}
@article{Gisbrecht2015a,
author = {Gisbrecht, Andrej and Hammer, Barbara},
doi = {10.1002/widm.1147},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/gisbrecht2015.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {2},
pages = {51--73},
title = {{Data visualization by nonlinear dimensionality reduction}},
volume = {5},
year = {2015}
}
@article{LeCun2015,
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/nature14539.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Wyman2013,
abstract = {The Alzheimer's Disease Neuroimaging Initiative (ADNI) three-dimensional T1-weighted magnetic resonance imaging (MRI) acquisitions provide a rich data set for developing and testing analysis techniques for extracting structural endpoints. To promote greater rigor in analysis and meaningful comparison of different algorithms, the ADNI MRI Core has created standardized analysis sets of data comprising scans that met minimum quality control requirements. We encourage researchers to test and report their techniques against these data. Standard analysis sets of volumetric scans from ADNI-1 have been created, comprising screening visits, 1-year completers (subjects who all have screening, 6- and 12-month scans), 2-year annual completers (screening, 1-year and 2-year scans), 2-year completers (screening, 6-months, 1-year, 18-months [mild cognitive impaired (MCI) only], and 2-year scans), and complete visits (screening, 6-month, 1-year, 18-month [MCI only], 2-year, and 3-year [normal and MCI only] scans). As the ADNI-GO/ADNI-2 data become available, updated standard analysis sets will be posted regularly. ?? 2013 The Alzheimer's Association. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Wyman, Bradley T. and Harvey, Danielle J. and Crawford, Karen and Bernstein, Matt A. and Carmichael, Owen and Cole, Patricia E. and Crane, Paul K. and Decarli, Charles and Fox, Nick C. and Gunter, Jeffrey L. and Hill, Derek and Killiany, Ronald J. and Pachai, Chahin and Schwarz, Adam J. and Schuff, Norbert and Senjem, Matthew L. and Suhy, Joyce and Thompson, Paul M. and Weiner, Michael and Jack, Clifford R.},
doi = {10.1016/j.jalz.2012.06.004},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S1552526012023795-main.pdf:pdf},
isbn = {8604418032},
issn = {15525260},
journal = {Alzheimer's and Dementia},
keywords = {ADNI,Alzheimer's disease,Analysis standards,Biomarkers,Magnetic resonance imaging},
number = {3},
pages = {332--337},
pmid = {23110865},
publisher = {Elsevier},
title = {{Standardization of analysis sets for reporting results from ADNI MRI data}},
url = {http://dx.doi.org/10.1016/j.jalz.2012.06.004},
volume = {9},
year = {2013}
}
@article{Hsu2002,
abstract = {Support vector machines (SVMs) were originally designed for binary classification. How to effectively extend it for multiclass classification is still an ongoing research issue. Several methods have been proposed where typically we construct a multiclass classifier by combining several binary classifiers. Some authors also proposed methods that consider all classes at once. As it is computationally more expensive to solve multiclass problems, comparisons of these methods using large-scale problems have not been seriously conducted. Especially for methods solving multiclass SVM in one step, a much larger optimization problem is required so up to now experiments are limited to small data sets. In this paper we give decomposition implementations for two such "all-together" methods. We then compare their performance with three methods based on binary classifications: "one-against-all," "one-against-one," and directed acyclic graph SVM (DAGSVM). Our experiments indicate that the "one-against-one" and DAG methods are more suitable for practical use than the other methods. Results also show that for large problems methods by considering all data at once in general need fewer support vectors.},
author = {Hsu, Cw and Lin, Cj},
doi = {10.1109/TNN.2002.1021904},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/00991427.pdf:pdf},
isbn = {3-540-32026-1},
issn = {1045-9227},
journal = {Neural Networks, IEEE Transactions on},
number = {2},
pages = {415--425},
pmid = {18244499},
title = {{A comparison of methods for multiclass support vector machines}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=991427},
volume = {13},
year = {2002}
}
@article{Cortes2009a,
author = {Cortes, Corinna},
doi = {10.1145/1553374.1553538},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes - 2009 - Invited talk Can learning kernels help performance.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--1},
title = {{Invited talk: Can learning kernels help performance?}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553538},
year = {2009}
}
@article{Liu2007a,
author = {Liu, Yong and Liao, Shizhong},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/9377-45126-1-PB.pdf:pdf},
keywords = {Novel Machine Learning Algorithms Track},
pages = {2814--2820},
title = {{Eigenvalues Ratio for Kernel Selection of Kernel Methods}},
year = {2007}
}
@article{Cao2005,
abstract = {This paper presents a weighted support vector machine (WSVM) to improve the outlier sensitivity problem of standard support vector machine (SVM) for two-class data classification. The basic idea is to assign different weights to different data points such that the WSVM training algorithm learns the decision surface according to the relative importance of data points in the training data set. The weights used in WSVM are generated by kernel-based possibilistic c-means (KPCM) algorithm, whose partition generates relative high values for important data points but low values for outliers. Experimental results indicate that the proposed method reduces the affect of outliers and yields higher classification rate than standard SVM does when outliers exist in the training data set.},
author = {Cao, a.},
doi = {10.1109/IJCNN.2005.1555965},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/01555965.pdf:pdf},
isbn = {0-7803-9048-2},
journal = {Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.},
pages = {859--864},
title = {{Weighted support vector machine for data classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1555965},
volume = {2},
year = {2005}
}
@article{Anguita2013e,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Reyes-Ortiz, Jorge Luis and Ridella, Sandro},
doi = {10.1007/978-3-642-40728-4_55},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C16.pdf:pdf},
isbn = {9783642407277},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Human Activity Recognition,L1-L2 Regularization,Sequential Minimal Optimization algorithm,Support Vector Machine},
number = {i},
pages = {434--441},
title = {{A novel procedure for training L1-L2 support vector machine classifiers}},
volume = {8131 LNCS},
year = {2013}
}
@article{Sweeney2013,
abstract = {Magnetic resonance imaging (MRI) can be used to detect lesions in the brains of multiple sclerosis (MS) patients and is essential for diagnosing the disease and monitoring its progression. In practice, lesion load is often quantified by either manual or semi-automated segmentation of MRI, which is time-consuming, costly, and associated with large inter- and intra-observer variability. We propose OASIS is Automated Statistical Inference for Segmentation (OASIS), an automated statistical method for segmenting MS lesions in MRI studies. We use logistic regression models incorporating multiple MRI modalities to estimate voxel-level probabilities of lesion presence. Intensity-normalized T1-weighted, T2-weighted, fluid-attenuated inversion recovery and proton density volumes from 131 MRI studies (98 MS subjects, 33 healthy subjects) with manual lesion segmentations were used to train and validate our model. Within this set, OASIS detected lesions with a partial area under the receiver operating characteristic curve for clinically relevant false positive rates of 1{\%} and below of 0.59{\%} (95{\%} CI; [0.50{\%}, 0.67{\%}]) at the voxel level. An experienced MS neuroradiologist compared these segmentations to those produced by LesionTOADS, an image segmentation software that provides segmentation of both lesions and normal brain structures. For lesions, OASIS out-performed LesionTOADS in 74{\%} (95{\%} CI: [65{\%}, 82{\%}]) of cases for the 98 MS subjects. To further validate the method, we applied OASIS to 169 MRI studies acquired at a separate center. The neuroradiologist again compared the OASIS segmentations to those from LesionTOADS. For lesions, OASIS ranked higher than LesionTOADS in 77{\%} (95{\%} CI: [71{\%}, 83{\%}]) of cases. For a randomly selected subset of 50 of these studies, one additional radiologist and one neurologist also scored the images. Within this set, the neuroradiologist ranked OASIS higher than LesionTOADS in 76{\%} (95{\%} CI: [64{\%}, 88{\%}]) of cases, the neurologist 66{\%} (95{\%} CI: [52{\%}, 78{\%}]) and the radiologist 52{\%} (95{\%} CI: [38{\%}, 66{\%}]). OASIS obtains the estimated probability for each voxel to be part of a lesion by weighting each imaging modality with coefficient weights. These coefficients are explicit, obtained using standard model fitting techniques, and can be reused in other imaging studies. This fully automated method allows sensitive and specific detection of lesion presence and may be rapidly applied to large collections of images. {\textcopyright} 2013 The Authors.},
author = {Sweeney, Elizabeth M. and Shinohara, Russell T. and Shiee, Navid and Mateen, Farrah J. and Chudgar, Avni a. and Cuzzocreo, Jennifer L. and Calabresi, Peter a. and Pham, Dzung L. and Reich, Daniel S. and Crainiceanu, Ciprian M.},
doi = {10.1016/j.nicl.2013.03.002},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/OASIS is Automated Statistical Inference for Segmentation, with applications to multiple sclerosis lesion segmentation in MRI[star].pdf:pdf},
issn = {22131582},
journal = {NeuroImage: Clinical},
keywords = {Brain,Lesion segmentation,Logistic regression,MRI,Multiple sclerosis,Statistical modeling},
number = {1},
pages = {402--413},
pmid = {24179794},
title = {{OASIS is Automated Statistical Inference for Segmentation, with applications to multiple sclerosis lesion segmentation in MRI}},
volume = {2},
year = {2013}
}
@article{Ehrenfeucht1989,
abstract = {We prove a lower bound of $\Omega$((1/$\epsilon$)$\backslash$ln (1/$\delta$)+VC$\backslash$dim$\backslash$n(C)/$\epsilon$) on the number of random examples required for distribution-free$\backslash$nlearning of a concept class C, where VCdim(C) is the Vapnik-Chervonenkis$\backslash$ndimension and $\epsilon$ and $\delta$ are the accuracy and confidence$\backslash$nparameters. This improves the previous best lower bound of $\Omega$((1/$\epsilon$)$\backslash$ln (1/$\delta$)+VC$\backslash$dim (C)) and comes close to the known$\backslash$ngeneral upper bound of 0((1/$\epsilon$)$\backslash$ln (1/$\delta$)+(VC$\backslash$dim (C)/$\epsilon$)$\backslash$ln$\backslash$n(1/$\epsilon$)) for consistent algorithms. We show that for many interesting$\backslash$nconcept classes, including kCNF and kDNF, our bound is actually tight$\backslash$nto within a constant factor.},
author = {Ehrenfeucht, Andrzej and Haussler, David and Kearns, Michael and Valiant, Leslie},
doi = {10.1016/0890-5401(89)90002-3},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/lower.pdf:pdf},
isbn = {0-55869-019-5},
issn = {08905401},
journal = {Information and Computation},
number = {3},
pages = {247--261},
title = {{A general lower bound on the number of examples needed for learning}},
volume = {82},
year = {1989}
}
@article{Bengio2006,
abstract = {We present a series of theoretical arguments supporting the claim that a large class of modern learning algorithms that rely solely on the smoothness prior  with similarity between examples expressed with a local kernel  are sensitive to the curse of dimensionality, or more precisely to the variability of the target. Our discussion covers supervised, semisupervised and unsupervised learning algorithms. These algorithms are found to be local in the sense that crucial properties of the learned function at x depend mostly on the neighbors of x in the training set. This makes them sensitive to the curse of dimensionality, well studied for classical non-parametric statistical learning. We show in the case of the Gaussian kernel that when the function to be learned has many variations, these algorithms require a number of training examples proportional to the number of variations, which could be large even though there may exist short descriptions of the target function, i.e. their Kolmogorov complexity may be low. This suggests that there exist non-local learning algorithms that at least have the potential to learn about such structured but apparently complex functions (because locally they have many variations), while not using very specific prior domain knowledge.},
author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/NIPS2005{\_}424.pdf:pdf},
isbn = {9780262232531},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 18},
pages = {107--114},
title = {{The Curse of Highly Variable Functions for Local Kernel Machines}},
year = {2006}
}
@article{Loizou2010,
abstract = {In this study we investigate the use of multiscale Amplitude Modulation-Frequency Modulation (AM-FM) methods for analyzing brain white matter lesions that are associated with multiple sclerosis MRI lesions imaged at 0 and 6-12 months. We use the instantaneous amplitude (IA) and the instantaneous frequency (IF) to assess disease progression. The IA and the IF were calculated in transverse sections of T2- weighted magnetic resonance (MR) images acquired from 38 symptomatic untreated subjects between the first and the second examination scan. The findings suggest that the high-, medium-, and low- frequency scale instantaneous amplitude and frequency can be used to differentiate between normal tissue and lesions at 0 and 6-12 months. Moreover, support vector machine (SVM) models gave satisfactory results for differentiating lesions at 0 months using the medium scale IA and IF components for expanded disability status scale (EDSS) {\textless}=2 and EDSS {\textgreater}2. Further work is needed with more subjects in validating the proposed AM-FM analysis. {\textcopyright} 2010 International Federation for Medical and Biological Engineering.},
author = {Loizou, C. P. and Murray, V. and Pattichis, M. S. and Pantziaris, M. and Seimenis, I. and Pattichis, C. S.},
doi = {10.1007/978-3-642-13039-7},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/tripoliti2016.pdf:pdf},
isbn = {978-3-642-13038-0},
issn = {16800737},
journal = {IFMBE Proceedings},
keywords = {AM-FM analysis,Multiple sclerosis,brain white matter,disease progression},
pages = {446--449},
title = {{XII Mediterranean Conference on Medical and Biological Engineering and Computing 2010}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-77957582021{\&}partnerID=tZOtx3y1},
volume = {29},
year = {2010}
}
@article{Wilson2013,
abstract = {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that it is possible to reconstruct several popular standard covariances within our framework.},
archivePrefix = {arXiv},
arxivId = {1302.4245},
author = {Wilson, a and Adams, R},
eprint = {1302.4245},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/wilson13.pdf:pdf},
journal = {Proceedings of the 30th {\ldots}},
number = {3},
pages = {1067--1075},
title = {{Gaussian process kernels for pattern discovery and extrapolation}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013{\_}wilson13},
volume = {28},
year = {2013}
}
@article{Benzi2004a,
abstract = {Gianfranco Cimmino (1908-1989) authored several papers in the field of numerical analysis, and par- ticularly in the area of matrix computations. His most important contribution in this field is the iterative method for solving linear algebraic systems that bears his name, published in 1938. This paper reviews Cimmino's main contributions to numerical mathematics, together with subsequent developments inspired by his work. Some back- ground information on Italian mathematics and on Mauro Picone's Istituto Nazionale per le Applicazioni del Calcolo, where Cimmino's early numerical work took place, is provided. The lasting importance of Cimmino's work in various application areas is demonstrated by an analysis of citation patterns in the broad technical and scientific literature.},
author = {Benzi, Michele},
doi = {http://www.dm.unibo.it/convegni/cimmino.html},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/cimmino.pdf:pdf},
journal = {Atti del Seminario di Analisi Matematica, Dipartimento di Matematica dell'Universita` di Bologna. Volume Speciale: Ciclo di Conferenze in Memoria di Gianfranco Cimmino},
keywords = {Cimmino's method,history of numerical linear algebra},
number = {March-April},
pages = {87--109},
title = {{Gianfranco Cimmino's contributions to numerical mathematics}},
url = {http://www.mathcs.emory.edu/{~}benzi/Web{\_}papers/cimmino.pdf},
year = {2004}
}
@article{Scholkopf2012,
abstract = {A new method for performing a nonlinear form of Principal Component Analysis is proposed. By the use of integral operator kernelfunctions, one can efficiently compute principal components in highdimensional feature spaces, related to input space by some nonlinearmap; for instance the space of all possible d pixel products in images.We give the derivation of the method and present experimental resultson polynomial feature extraction for pattern recognition.},
author = {Scholkopf, B and Smola, a J and Muller, K R},
doi = {10.1162/089976698300017467},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scholkopf, Smola, Muller - 2012 - Kernel Principal Component Analysis.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Computer Vision And Mathematical Methods In Medical And Biomedical Image Analysis},
pages = {583--588},
pmid = {21939901},
title = {{Kernel Principal Component Analysis}},
url = {http://www.tribesandclimatechange.org/docs/tribes{\_}450.pdf$\backslash$npapers2://publication/uuid/D79004C6-EBB3-42CE-9F15-120270D985BE},
volume = {1327},
year = {2012}
}
@article{Evgeniou2004,
abstract = {Past empirical work has shown
that learning multiple related tasks from data simultaneously can
be advantageous in terms of predictive performance relative to
learning these tasks independently. In this paper we present an
approach to multi--task learning based on the minimization of
regularization functionals similar to existing ones, such as the
one for Support Vector Machines (SVMs), that have been
successfully used in the past for single--task learning. Our
approach allows to model the relation between tasks in terms of a
novel kernel function that uses a task--coupling parameter. We
implement an instance of the proposed approach similar to SVMs
and test it empirically using simulated as well as real data. The
experimental results show that the proposed method performs better
than existing multi--task learning methods and largely outperforms
single--task learning using SVMs.},
author = {Evgeniou, Theodoros and Pontil, Massimiliano},
doi = {10.1145/1014052.1014067},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0912f50fd2564057d5000000.pdf:pdf},
isbn = {1581138881},
keywords = {Learning/Statistics {\&} Optimisation,Multimodal Integration,Theory {\&} Algorithms},
number = {July 2015},
title = {{Regularized Multi-task Learning}},
url = {http://eprints.pascal-network.org/archive/00000825/},
year = {2004}
}
@article{Tagliazucchi2014,
abstract = {The mining of huge databases of resting-state brain activity recordings represents state of the art in the assessment of endogenous neuronal activity-and may be a promising tool in the search for functional biomarkers. However, the resting state is an uncontrolled condition and its heterogeneity is neither sufficiently understood nor accounted for. We test the hypothesis that subjects exhibit unstable wakefulness, i.e., drift into sleep during typical resting-state experiments. Analyzing 1,147 resting-state functional magnetic resonance data sets, we revealed a reliable loss of wakefulness in a third of subjects within 3min and demonstrated the dynamic nature of the resting state, with fundamental changes in the associated functional neuroanatomy. Implications include the necessity of wakefulness monitoring and modeling, taking measures to maintain a state of wakefulness, acknowledging the possibility of sleep and exploring its consequences, and especially the critical assessment of possible false-positive or false-negative results. {\textcopyright} 2014 Elsevier Inc.},
author = {Tagliazucchi, Enzo and Laufs, Helmut},
doi = {10.1016/j.neuron.2014.03.020},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0896627314002505-main.pdf:pdf},
isbn = {1097-4199 (Electronic)$\backslash$r0896-6273 (Linking)},
issn = {10974199},
journal = {Neuron},
number = {3},
pages = {695--708},
pmid = {24811386},
publisher = {Elsevier Inc.},
title = {{Decoding Wakefulness Levels from Typical fMRI Resting-State Data Reveals Reliable Drifts between Wakefulness and Sleep}},
url = {http://dx.doi.org/10.1016/j.neuron.2014.03.020},
volume = {82},
year = {2014}
}
@article{Kloft2010,
abstract = {Recent research on multiple kernel learning has lead to a number of approaches for combining kernels in regularized risk minimization. The proposed approaches include different formulations of objectives and varying regularization strategies. In this paper we present a unifying general optimization criterion for multiple kernel learning and show how existing formulations are subsumed as special cases. We also derive the criterion's dual representation, which is suitable for general smooth optimization algorithms. Finally, we evaluate multiple kernel learning in this framework analytically using a Rademacher complexity bound on the generalization error and empirically in a set of experiments.},
archivePrefix = {arXiv},
arxivId = {1005.0437},
author = {Kloft, Marius and R{\"{u}}ckert, Ulrich and Bartlett, Pl},
eprint = {1005.0437},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1005.0437.pdf:pdf},
journal = {{\ldots} Learning and Knowledge Discovery in {\ldots}},
number = {Ii},
pages = {1--16},
title = {{A unifying view of multiple kernel learning}},
url = {http://arxiv.org/abs/1005.0437$\backslash$nhttp://link.springer.com/chapter/10.1007/978-3-642-15883-4{\_}5},
year = {2010}
}
@article{Sydorov2014,
author = {Sydorov, Vladyslav and Sakurada, Mayu and Lampert, Christoph H},
doi = {10.1109/CVPR.2014.182},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/sydorov-cvpr2014.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr},
pages = {1402--1409},
title = {{Deep Fisher Kernels – End to End Learning of the Fisher Kernel GMM Parameters}},
year = {2014}
}
@article{Lever2012,
abstract = {We propose a method to efficiently construct
data dependent kernels which can make use of
large quantities of (unlabeled) data. Our construction
makes an approximation in the standard
construction of semi-supervised kernels in Sindhwani
et al. (2005). In typical cases these kernels
can be computed in nearly-linear time (in
the amount of data), improving on the cubic time
of the standard construction, enabling large scale
semi-supervised learning in a variety of contexts.
The methods are validated on semi-supervised
and unsupervised problems on data sets containing
upto 64,000 sample points.},
archivePrefix = {arXiv},
arxivId = {arXiv:1110.4416v1},
author = {Lever, Guy and Diethe, Tom and Shawe-Taylor, John},
eprint = {arXiv:1110.4416v1},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lever, Diethe, Shawe-Taylor - 2012 - Data dependent kernels in nearly-linear time.pdf:pdf},
keywords = {Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
title = {{Data dependent kernels in nearly-linear time}},
url = {http://eprints.pascal-network.org/archive/00009238/},
volume = {XX},
year = {2012}
}
@article{Dyrba2015,
abstract = {Alzheimer's disease (AD) patients exhibit alterations in the functional connectivity between spatially segregated brain regions which may be related to both local gray matter (GM) atrophy as well as a decline in the fiber integrity of the underlying white matter tracts. Machine learning algorithms are able to automatically detect the patterns of the disease in image data, and therefore, constitute a suitable basis for automated image diagnostic systems. The question of which magnetic resonance imaging (MRI) modalities are most useful in a clinical context is as yet unresolved. We examined multimodal MRI data acquired from 28 subjects with clinically probable AD and 25 healthy controls. Specifically, we used fiber tract integrity as measured by diffusion tensor imaging (DTI), GM volume derived from structural MRI, and the graph-theoretical measures 'local clustering coefficient' and 'shortest path length' derived from resting-state functional MRI (rs-fMRI) to evaluate the utility of the three imaging methods in automated multimodal image diagnostics, to assess their individual performance, and the level of concordance between them. We ran the support vector machine (SVM) algorithm and validated the results using leave-one-out cross-validation. For the single imaging modalities, we obtained an area under the curve (AUC) of 80{\%} for rs-fMRI, 87{\%} for DTI, and 86{\%} for GM volume. When it came to the multimodal SVM, we obtained an AUC of 82{\%} using all three modalities, and 89{\%} using only DTI measures and GM volume. Combined multimodal imaging data did not significantly improve classification accuracy compared to the best single measures alone. Hum Brain Mapp 36:2118-2131, 2015. {\textcopyright} 2015 Wiley Periodicals, Inc.},
author = {Dyrba, Martin and Grothe, Michel and Kirste, Thomas and Teipel, Stefan J.},
doi = {10.1002/hbm.22759},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/dyrba2015.pdf:pdf},
issn = {10970193},
journal = {Human Brain Mapping},
keywords = {Alzheimer's disease,Diffusion tensor imaging,Magnetic resonance imaging,Multiple kernel support vector machine,Resting-state functional magnetic resonance imagin},
number = {6},
pages = {2118--2131},
pmid = {25664619},
title = {{Multimodal analysis of functional and structural disconnection in Alzheimer's disease using multiple kernel SVM}},
volume = {36},
year = {2015}
}
@article{Weinberger2006,
author = {Weinberger, K.Q. and Weinberger, K.Q. and Saul, L.K. and Saul, L.K.},
doi = {10.1007/s11263-005-4939-z},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinberger et al. - 2006 - Unsupervised Learning of Image Manifolds by Semidefinite Programming.pdf-:pdf-},
isbn = {0769521584},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {data anal-,dimensionality reduction,image manifolds,kernel methods,manifold learning,semide nite embedding,semide nite programming,ysis},
number = {1},
pages = {77--90},
title = {{Unsupervised Learning of Image Manifolds by Semidefinite Programming}},
url = {http://www.springerlink.com/index/10.1007/s11263-005-4939-z},
volume = {70},
year = {2006}
}
@article{Pascanu2013,
abstract = {There are two widely known issues with prop- erly training Recurrent Neural Networks, the vanishing and the exploding gradient prob- lems detailed in Bengio et al. (1994). In this paper we attempt to improve the under- standing of the underlying issues by explor- ing these problems from an analytical, a geo- metric and a dynamical systems perspective. Our analysis is used to justify a simple yet ef- fective solution. We propose a gradient norm clipping strategy to deal with exploding gra- dients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/pascanu13.pdf:pdf},
issn = {1045-9227},
journal = {International Conference on Machine Learning},
number = {2},
pages = {1310--1318},
pmid = {18267787},
title = {{On the difficulty of training recurrent neural networks}},
url = {http://jmlr.org/proceedings/papers/v28/pascanu13.pdf},
year = {2013}
}
@article{Cotter2012,
abstract = {We present a novel approach for training kernel Support Vector Machines, establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach, and show that our method works well in practice compared to existing alternatives.},
archivePrefix = {arXiv},
arxivId = {1204.0566},
author = {Cotter, Andrew and Shalev-Shwartz, Shai and Srebro, Nathan},
eprint = {1204.0566},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/493.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
pages = {943--950},
title = {{The Kernelized Stochastic Batch Perceptron}},
volume = {1},
year = {2012}
}
@article{Huang2006,
author = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
doi = {10.1016/j.neucom.2005.12.126},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0925231206000385-main.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {back-propagation algorithm,extreme learning machine,feedforward neural networks,random,real-time learning,support vector machine},
number = {1-3},
pages = {489--501},
title = {{Extreme learning machine: Theory and applications}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231206000385},
volume = {70},
year = {2006}
}
@article{Baldassarre2012,
abstract = {Structured sparsity methods have been recently proposed that allow to incorporate additional spatial and temporal information for estimating models for decoding mental states from fMRI data. These methods carry the promise of being more interpretable...},
author = {Baldassarre, Luca and Mour{\~{a}}o-Miranda, Janaina and Pontil, Massimiliano},
doi = {10.1109/PRNI.2012.31},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldassarre, Mour{\~{a}}o-Miranda, Pontil - 2012 - Structured sparsity models for brain decoding from fMRI data.pdf:pdf},
isbn = {9780769547657},
journal = {Proceedings - 2012 2nd International Workshop on Pattern Recognition in NeuroImaging, PRNI 2012},
keywords = {brain decoding,fMRI,stability,structured sparsity},
pages = {5--8},
title = {{Structured sparsity models for brain decoding from fMRI data}},
year = {2012}
}
@article{Bach2004,
abstract = {While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.},
author = {Bach, Francis R. and Lanckriet, Gert R. G. and Jordan, Michael I.},
doi = {10.1145/1015330.1015424},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach, Lanckriet, Jordan - 2004 - Multiple kernel learning, conic duality, and the SMO algorithm.pdf:pdf},
isbn = {1581138285},
issn = {1581138285},
journal = {Twentyfirst international conference on Machine learning ICML 04},
pages = {6},
title = {{Multiple kernel learning, conic duality, and the SMO algorithm}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015424},
volume = {69},
year = {2004}
}
@article{Do2009,
abstract = {A serious drawback of kernel methods, and Support Vector Machines (SVM) in particular, is the difficulty in choosing a suitable kernel function for a given dataset. One of the approaches proposed to address this problem is Multiple Kernel Learning (MKL) in which several kernels are combined adaptively for a given dataset. Many of the existing MKL methods use the SVM objective function and try to find a linear combination of basic kernels such that the separating margin between the classes is maximized. However, these methods ignore the fact that the theoretical error bound depends not only on the margin, but also on the radius of the smallest sphere that contains all the training instances. We present a novel MKL algorithm that optimizes the error bound taking account of both the margin and the radius. The empirical results show that the proposed method compares favorably with other state-of-the-art MKL methods.},
author = {Do, Huyen and Kalousis, Alexandros and Woznica, Adam and Hilario, Melanie},
doi = {10.1007/978-3-642-04180-8},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/unige{\_}6892{\_}attachment01.pdf:pdf},
isbn = {978-3-642-04179-2},
journal = {Machine Learning and Knowledge Discovery in Databases-Lecture Notes in Computer Science vol. 5781},
pages = {330--343},
title = {{Margin and Radius Based Multiple Kernel Learning}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-04180-8},
volume = {5781},
year = {2009}
}
@article{Qiu2009,
abstract = {The cell defense mechanism of RNA interference has applications in gene function analysis and promising potentials in human disease therapy. To effectively silence a target gene, it is desirable to select appropriate initiator siRNA molecules having satisfactory silencing capabilities. Computational prediction for silencing efficacy of siRNAs can assist this screening process before using them in biological experiments. String kernel functions, which operate directly on the string objects representing siRNAs and target mRNAs, have been applied to support vector regression for the prediction and improved accuracy over numerical kernels in multidimensional vector spaces constructed from descriptors of siRNA design rules. To fully utilize information provided by string and numerical data, we propose to unify the two in a kernel feature space by devising a multiple kernel regression framework where a linear combination of the kernels is used. We formulate the multiple kernel learning into a quadratically constrained quadratic programming (QCQP) problem, which although yields global optimal solution, is computationally demanding and requires a commercial solver package. We further propose three heuristics based on the principle of kernel-target alignment and predictive accuracy. Empirical results demonstrate that multiple kernel regression can improve accuracy, decrease model complexity by reducing the number of support vectors, and speed up computational performance dramatically. In addition, multiple kernel regression evaluates the importance of constituent kernels, which for the siRNA efficacy prediction problem, compares the relative significance of the design rules. Finally, we give insights into the multiple kernel regression mechanism and point out possible extensions.},
author = {Qiu, Shibin and Lane, Terran},
doi = {10.1109/TCBB.2008.139},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/04731239.pdf:pdf},
issn = {15455963},
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
keywords = {Multiple kernel heuristics,Multiple kernel learning,QCQP optimization,RNA interference,SiRNA efficacy,Support vector regression},
number = {2},
pages = {190--199},
pmid = {19407344},
title = {{A framework for multiple kernel support vector regression and its applications to siRNA efficacy prediction}},
volume = {6},
year = {2009}
}
@article{Koppen2000,
abstract = {In this text, some question related to higher dimensional geometrical spaces will be discussed. The goal is to give the reader a feeling for geometric distortions related to the use of such spaces (e.g. as search spaces).},
author = {K{\"{o}}ppen, M},
doi = {10.1200/JCO.2010.30.1986},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/koppen-curse.pdf:pdf},
issn = {1527-7755},
journal = {5th Online World Conference on Soft Computing in Industrial Applications (WSC5)},
pages = {4--8},
pmid = {21041706},
title = {{The curse of dimensionality}},
url = {http://yaroslavvb.com/papers/koppen-curse.pdf},
volume = {1},
year = {2000}
}
@article{Bolon-Canedo2013,
abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and S{\'{a}}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo},
doi = {10.1007/s10115-012-0487-8},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/survey{\_}FS{\_}veronica.pdf:pdf},
isbn = {0219-1377},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Embedded methods,Feature selection,Filters,Synthetic datasets,Wrappers},
number = {3},
pages = {483--519},
title = {{A review of feature selection methods on synthetic data}},
volume = {34},
year = {2013}
}
@article{Gershman2012,
abstract = {Variational methods are widely used for ap- proximate posterior inference. However, their use is typically limited to families of distributions that enjoy particular conjugacy properties. To circumvent this limitation, we propose a family of variational approx- imations inspired by nonparametric kernel density estimation. The locations of these kernels and their bandwidth are treated as variational parameters and optimized to im- prove an approximate lower bound on the marginal likelihood of the data. Unlike most other variational approximations, using mul- tiple kernels allows the approximation to cap- ture multiple modes of the posterior. We demonstrate the efficacy of the nonparamet- ric approximation with a hierarchical logistic regression model and a nonlinear matrix fac- torization model. We obtain predictive per- formance as good as or better than more spe- cialized variational methods and MCMC ap- proximations. The method is easy to apply to graphical models for which standard vari- ational methods are difficult to derive.},
archivePrefix = {arXiv},
arxivId = {1206.4665},
author = {Gershman, Samuel J and Hoffman, Matthew D and Blei, David M},
eprint = {1206.4665},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/360.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {International Conference on Machine Learning},
pages = {1--8},
title = {{Nonparametric Variational Inference}},
url = {papers2://publication/uuid/438C2319-9497-40AF-A55C-B120999773AA},
year = {2012}
}
@article{Scholkopf1998,
abstract = {A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear map—for instance, the space of all possible five-pixel products in 16 × 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.},
author = {Sch{\"{o}}lkopf, Bernhard and Smola, Alexander and M{\"{u}}ller, Klaus-Robert},
doi = {10.1162/089976698300017467},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/10.1162@089976698300017467.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {5},
pages = {1299--1319},
pmid = {21939901},
title = {{Nonlinear Component Analysis as a Kernel Eigenvalue Problem}},
volume = {10},
year = {1998}
}
@article{Bengio2007,
abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
doi = {citeulike-article-id:4640046},
eprint = {0500581},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/3048-greedy-layer-wise-training-of-deep-networks.pdf:pdf},
isbn = {0262195682},
issn = {01628828},
journal = {Advances in neural information processing systems},
number = {1},
pages = {153},
pmid = {19018704},
primaryClass = {submit},
title = {{Greedy Layer-Wise Training of Deep Networks}},
volume = {19},
year = {2007}
}
@misc{Yuan2015,
author = {Yuan, Xiao Tong and Wang, Zhenzhen and Deng, Jiankang and Liu, Qingshan},
booktitle = {IEEE Transactions on Neural Networks and Learning Systems},
doi = {10.1109/TNNLS.2015.2476659},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ieee{\_}efficient{\_}chi2{\_}kernel{\_}lin.pdf:pdf},
isbn = {8113029001},
issn = {21622388},
pages = {1--6},
title = {{Efficient ?? Kernel Linearization via Random Feature Maps}},
year = {2015}
}
@article{Schleif2015,
author = {Schleif, Frank-michael and Gisbrecht, Andrej and Tino, Peter},
doi = {10.1007/978-3-319-24261-3},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/micio2.pdf:pdf},
isbn = {978-3-319-24260-6},
pages = {160--170},
title = {{Similarity-Based Pattern Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-24261-3},
volume = {9370},
year = {2015}
}
@article{Mwangi2014,
abstract = {Machine learning techniques are increasingly being used in making relevant predictions and inferences on individual subjects neuroimaging scan data. Previous studies have mostly focused on categorical discrimination of patients and matched healthy controls and more recently, on prediction of individual continuous variables such as clinical scores or age. However, these studies are greatly hampered by the large number of predictor variables (voxels) and low observations (subjects) also known as the curse-of-dimensionality or small-n-large-p problem. As a result, feature reduction techniques such as feature subset selection and dimensionality reduction are used to remove redundant predictor variables and experimental noise, a process which mitigates the curse-of-dimensionality and small-n-large-p effects. Feature reduction is an essential step before training a machine learning model to avoid overfitting and therefore improving model prediction accuracy and generalization ability. In this review, we discuss feature reduction techniques used with machine learning in neuroimaging studies.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Mwangi, Benson and Tian, Tian Siva and Soares, Jair C.},
doi = {10.1007/s12021-013-9204-3},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2Fs12021-013-9204-3.pdf:pdf},
isbn = {2122633255},
issn = {15392791},
journal = {Neuroinformatics},
keywords = {Dimensionality reduction,Feature reduction,Feature selection,Machine learning,Multivariate,Neuroimaging,Predictive Modeling},
number = {2},
pages = {229--244},
pmid = {24013948},
title = {{A review of feature reduction techniques in Neuroimaging}},
volume = {12},
year = {2014}
}
@article{Bucak2014,
abstract = {Multiple kernel learning (MKL) is a principled approach for selecting and combining kernels for a given recognition task. A number of studies have shown that MKL is a useful tool for object recognition, where each image is represented by multiple sets of features and MKL is applied to combine different feature sets. We review the state-of-the-art for MKL, including different formulations and algorithms for solving the related optimization problems, with the focus on their applications to object recognition. One dilemma faced by practitioners interested in using MKL for object recognition is that different studies often provide conflicting results about the effectiveness and efficiency of MKL. To resolve this, we conduct extensive experiments on standard datasets to evaluate various approaches to MKL for object recognition. We argue that the seemingly contradictory conclusions offered by studies are due to different experimental setups. The conclusions of our study are: (i) given a sufficient number of training examples and feature/kernel types, MKL is more effective for object recognition than simple kernel combination (e.g., choosing the best performing kernel or average of kernels); and (ii) among the various approaches proposed for MKL, the sequential minimal optimization, semi-infinite programming, and level method based ones are computationally most efficient.},
author = {Bucak, Serhat and Jin, Rong and Jain, Anil},
doi = {10.1109/TPAMI.2013.212},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bucak, Jin, Jain - 2014 - Multiple Kernel Learning for Visual Object Recognition A Review.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Feature evaluation and selection,Histograms,Image color analysis,Introductory and Survey,Kernel,Machine learning,Multiple kernel learning,Object recognition,Optimization,Training,Visualization,convex optimization,support vector machine,visual object recognition},
number = {7},
pages = {1354--1369},
title = {{Multiple Kernel Learning for Visual Object Recognition: A Review}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6654166},
volume = {36},
year = {2014}
}
@article{Micchelli2005a,
abstract = {We study the problem of finding an optimal kernel from a prescribed convex set of kernels K for learning a real-valued function by regularization. We establish for a wide variety of regularization functionals that this leads to a convex optimization problem and, for square loss regularization, we characterize the solution of this problem. We show that, although K may be an uncountable set, the optimal kernel is always obtained as a convex combination of at most m+2 basic kernels, where m is the number of data examples. In particular, our results apply to learning the optimal radial kernel or the optimal dot product kernel.},
author = {Micchelli, Charles A. and Pontil, Massimiliano},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/tb-institute.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {1099--1125},
title = {{Learning the kernel function via regularization}},
url = {http://www.ams.org/mathscinet-getitem?mr=MR2249850$\backslash$npapers2://publication/uuid/02165668-428F-4E42-B41D-EFB808712DAE},
volume = {6},
year = {2005}
}
@article{Denchev2015,
abstract = {Quantum annealing (QA) has been proposed as a quantum enhanced optimization heuristic exploiting tunneling. Here, we demonstrate how finite range tunneling can provide considerable computational advantage. For a crafted problem designed to have tall and narrow energy barriers separating local minima, the D-Wave 2X quantum annealer achieves significant runtime advantages relative to Simulated Annealing (SA). For instances with 945 variables this results in a time-to-99$\backslash${\%}-success-probability that is {\$}\backslashsim 10{\^{}}8{\$} times faster than SA running on a single processor core. We also compared physical QA with Quantum Monte Carlo (QMC), an algorithm that emulates quantum tunneling on classical processors. We observe a substantial constant overhead against physical QA: D-Wave 2X runs up to {\$}\backslashsim 10{\^{}}8{\$} times faster than an optimized implementation of QMC on a single core. To investigate whether finite range tunneling will also confer an advantage for problems of practical interest, we conduct numerical studies on binary optimization problems that cannot yet be represented on quantum hardware. For random instances of the number partitioning problem, we find numerically that QMC, as well as other algorithms designed to simulate QA, scale better than SA and better than the best known classical algorithms for this problem. We discuss the implications of these findings for the design of next generation quantum annealers.},
archivePrefix = {arXiv},
arxivId = {1512.02206},
author = {Denchev, Vasil S. and Boixo, Sergio and Isakov, Sergei V. and Ding, Nan and Babbush, Ryan and Smelyanskiy, Vadim and Martinis, John and Neven, Hartmut},
eprint = {1512.02206},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1512.02206v3.pdf:pdf},
number = {3},
pages = {1--17},
title = {{What is the Computational Value of Finite Range Tunneling?}},
url = {http://arxiv.org/abs/1512.02206},
year = {2015}
}
@article{Oneto2013,
abstract = {The problem of assessing the performance of a classifier, in the finite-sample setting, has been addressed by Vapnik in his seminal work by using data-independent measures of complexity. Recently, several authors have addressed the same problem by proposing data-dependent measures, which tighten previous results by taking in account the actual data distribution. In this framework, we derive some data-dependent bounds on the generalization ability of a classifier by exploiting the Rademacher Complexity and recent concentration results: in addition of being appealing for practical purposes, as they exploit empirical quantities only, these bounds improve previously known results. {\textcopyright} 2013 Elsevier Ltd.},
author = {Oneto, Luca and Ghio, Alessandro and Anguita, Davide and Ridella, Sandro},
doi = {10.1016/j.neunet.2013.03.017},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Journal/J04 - NN.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Concentration of measure,Data-dependent bounds,Error estimation,Rademacher complexity},
pages = {107--111},
publisher = {Elsevier Ltd},
title = {{An improved analysis of the Rademacher data-dependent bound using its self bounding property}},
url = {http://dx.doi.org/10.1016/j.neunet.2013.03.017},
volume = {44},
year = {2013}
}
@article{Zhang2008a,
abstract = {The Support Vector Machine (SVM) is a popular classification paradigm in machine learning and has achieved great success in real applications. However, the standard SVM can not select variables automatically and therefore its solution typically utilizes all the input variables without discrimination. This makes it difficult to identify important predictor variables, which is often one of the primary goals in data analysis. In this paper, we propose two novel types of regularization in the context of the multicategory SVM (MSVM) for simultaneous classification and variable selection. The MSVM generally requires estimation of multiple discriminating functions and applies the argmax rule for prediction. For each individual variable, we propose to characterize its importance by the supnorm of its coefficient vector associated with different functions, and then minimize the MSVM hinge loss function subject to a penalty on the sum of supnorms. To further improve the supnorm penalty, we propose the adaptive regularization, which allows different weights imposed on different variables according to their relative importance. Both types of regularization automate variable selection in the process of building classifiers, and lead to sparse multi-classifiers with enhanced interpretability and improved accuracy, especially for high dimensional low sample size data. One big advantage of the supnorm penalty is its easy implementation via standard linear programming. Several simulated examples and one real gene data analysis demonstrate the outstanding performance of the adaptive supnorm penalty in various data settings.},
archivePrefix = {arXiv},
arxivId = {0803.3676},
author = {Zhang, Hao Helen and Liu, Yufeng and Wu, Yichao and Zhu, Ji},
doi = {10.1214/08-EJS122},
eprint = {0803.3676},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Zhang-EJS08.pdf:pdf},
issn = {1935-7524},
keywords = {and phrases,classification,l 1 -norm penalty,multicategory},
pages = {149--167},
pmid = {3692296878244318979},
title = {{Variable selection for the multicategory SVM via adaptive sup-norm regularization}},
url = {http://arxiv.org/abs/0803.3676},
volume = {2},
year = {2008}
}
@article{Zhu2015,
abstract = {Abstract Kernelized modification of Ho–Kashyap algorithm with squared approximation of the misclassification errors (KMHKS) is an effective algorithm for nonlinearly separable classification problems. While KMHKS only adopts one kernel function. So a multi-kernel classification machine with reduced complexity named Nystr{\"{o}}m approximation matrix with Multiple KMHKSs (NMKMHKS) has been developed. But NMKMHKS has to initialize many parameters and has not an ability to deal with noise well. To this end, we propose an improved multi-kernel classification machine with Nystr{\"{o}}m approximation technique (INMKMHKS). INMKMHKS is based on a new way of generating kernel functions and a new Nystr{\"{o}}m approximation technique. The contributions of INMKMHKS are that (1) avoiding the problem of setting too many parameters; (2) keeping comparable space and computational complexities after comparing with NMKMHKS; (3) having a tighter generalization risk bound in terms of Rademacher complexity analysis; (4) having a better recognition than NMKMHKS on average; (5) possessing an ability to deal with noise and practical images.},
author = {Zhu, Changming and Gao, Daqi},
doi = {http://dx.doi.org/10.1016/j.patcog.2014.10.029},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0925231215015787-main.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Generalization risk analysis,Multiple kernel learning,Nystr{\"{o}}m approximation,Pattern classification},
number = {4},
pages = {1490--1509},
publisher = {Elsevier},
title = {{Improved multi-kernel classification machine with Nystr{\"{o}}m approximation technique}},
url = {http://www.sciencedirect.com/science/article/pii/S0031320314004464},
volume = {48},
year = {2015}
}
@article{Meinshausen2014,
abstract = {Large-scale data are often characterised by some degree of inhomogeneity as data are either recorded in different time regimes or taken from multiple sources. We look at regression models and the effect of randomly changing coefficients, where the change is either smoothly in time or some other dimension or even without any such structure. Fitting varying-coefficient models or mixture models can be appropriate solutions but are computationally very demanding and often try to return more information than necessary. If we just ask for a model estimator that shows good predictive properties for all regimes of the data, then we are aiming for a simple linear model that is reliable for all possible subsets of the data. We propose a maximin effects estimator and look at its prediction accuracy from a theoretical point of view in a mixture model with known or unknown group structure. Under certain circumstances the estimator can be computed orders of magnitudes faster than standard penalised regression estimators, making computations on large-scale data feasible. Empirical examples complement the novel methodology and theory.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.0596v1},
author = {Meinshausen, Nicolai and B{\"{u}}hlmann, Peter},
doi = {10.1214/15-AOS1325},
eprint = {arXiv:1406.0596v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1406.0596.pdf:pdf},
issn = {00905364},
journal = {arXiv preprint arXiv:1406.0596},
pages = {1--26},
title = {{Maximin effects in inhomogeneous large-scale data}},
url = {http://arxiv.org/abs/1406.0596},
year = {2014}
}
@article{Weinberger2006b,
author = {Weinberger, K Q and Blitzer, J and Saul, L K},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/NIPS2005{\_}265.pdf:pdf},
journal = {Proc. NIPS},
title = {{Distance metric learning for large margin nearest neighbor classification}},
year = {2006}
}
@article{Meier2009,
abstract = {We propose a new sparsity-smoothness penalty for high-dimensional generalized additive models. The combination of sparsity and smoothness is crucial for mathematical theory as well as performance for finite-sample data. We present a computationally efficient algorithm, with provable numerical convergence properties, for optimizing the penalized likelihood. Furthermore, we provide oracle results which yield asymptotic optimality of our estimator for high dimensional but sparse additive models. Finally, an adaptive version of our sparsity-smoothness penalized approach yields large additional performance gains.},
archivePrefix = {arXiv},
arxivId = {0806.4115},
author = {Meier, Lukas and {Van De Geer}, Sara and B{\"{u}}hlmann, Peter},
doi = {10.1214/09-AOS692},
eprint = {0806.4115},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/0806.4115.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Group lasso,Model selection,Nonparametric regression,Oracle inequality,Penalized likelihood,Sparsity},
number = {6 B},
pages = {3779--3821},
title = {{High-dimensional additive modeling}},
volume = {37},
year = {2009}
}
@article{Eckstein1992,
abstract = {This paper shows, by means of an operator called asplitting operator, that the DouglasRachford splitting method for finding a zero of the sum of two monotone operators is a special case of the proximal point algorithm. Therefore, applications of DouglasRachford splitting, such as the alternating direction method of multipliers for convex programming decomposition, are also special cases of the proximal point algorithm. This observation allows the unification and generalization of a variety of convex programming algorithms. By introducing a modified version of the proximal point algorithm, we derive a new,generalized alternating direction method of multipliers for convex programming. Advances of this sort illustrate the power and generality gained by adopting monotone operator theory as a conceptual framework.},
author = {Eckstein, Jonathan and Bertsekas, Dimitri P.},
doi = {10.1007/BF01581204},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/P-1919-20783099.pdf:pdf},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Monotone operators,decomposition,proximal point algorithm},
number = {1-3},
pages = {293--318},
title = {{On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators}},
volume = {55},
year = {1992}
}
@article{Ben-David2003,
abstract = {The approach of learning of multiple "related" tasks simultaneously has proven quite successful in practice; however, theoretical justification for this success has remained elusive. The starting point for previous work on multiple task learning has been that the tasks to be learned jointly are somehow "algorithmically related", in the sense that the results of applying a specific learning algorithm to these tasks are assumed similar. We offer an alternative approach, defining relatedness of tasks on the basis of similarity betwen the example generating distributions that underline these tasks. We provid a formal framework for this notion of task relatedness, which captures a sub-domain of the wide scope of issues in which one may apply a multiple task learning approach. Our notion of task similarity is relevant to a variety of real life multitask learning scenarios and allows the formal derivation of generalization bounds that are strictly stronger than the previously known bounds for both the learning-to-learn and the multitask learning scenarios. We give precise conditions under which our bounds guarnatee generalization on the basis of smaller sample sizes and the standard single-task approach.},
author = {Ben-David, S and Schuller, Reba},
doi = {10.1007/978-3-540-45167-9_41},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ben-david+schuller-2003.pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
journal = {Learning Theory and Kernel Machines},
pages = {567--580},
title = {{Exploiting task relatedness for multiple task learning}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-45167-9{\_}41},
year = {2003}
}
@article{Vapnik1998,
author = {Vapnik, Vladimir},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Statistical-Learning-Theory.pdf:pdf},
isbn = {0-471-03003-1},
journal = {Adaptive and learning Systems for Signal Processing, Communications and Control},
pages = {1--740},
title = {{Statistical Learning Theory}},
url = {http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471030031.html},
year = {1998}
}
@article{Chapelle2010,
abstract = {In this paper we propose a novel algorithm for multi-task learning with boosted decision trees. We learn several different learning tasks with a joint model, explicitly addressing the specifics of each learning task with task-specific parameters and the commonalities between them through shared parameters. This enables implicit data sharing and regularization. We evaluate our learning method on web-search ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. Our experiments validate that learning various tasks jointly can lead to significant improvements in performance with surprising reliability.},
author = {Chapelle, Olivier and Shivaswamy, Pannagadatta and Vadrevu, Srinivas and Weinberger, Kilian and Zhang, Ya and Tseng, Belle},
doi = {10.1145/1835804.1835953},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/multiboost.pdf:pdf},
isbn = {9781450300551},
journal = {Proceedings of the 16th {\{}ACM{\}} {\{}SIGKDD{\}} international conference on Knowledge discovery and data mining},
keywords = {algorithms,boosting,decision trees,information search and retrieval,learning,multitask,ranking,web search},
pages = {1189--1198},
title = {{Multi-task learning for boosting with application to web search ranking}},
year = {2010}
}
@article{Anguita2012d,
abstract = {Activity-Based Computing [1] aims to capture the state of the user and its environment by exploiting heterogeneous sensors in order to provide adaptation to exogenous computing resources. When these sensors are attached to the subject¿s body, they permit continuous monitoring of numerous physiological signals. This has appealing use in healthcare applications, e.g. the exploitation of Ambient Intelligence (AmI) in daily activity monitoring for elderly people. In this paper, we present a system for human physical Activity Recognition (AR) using smartphone inertial sensors. As these mobile phones are limited in terms of energy and computing power, we propose a novel hardware-friendly approach for multiclass classification. This method adapts the standard Support Vector Machine (SVM) and exploits fixed-point arithmetic. In addition to the clear computational advantages of fixed-point arithmetic, it is easy to show the regularization effect of the number of bits and then the connections with the Statistical Learning Theory. A comparison with the traditional SVM shows a significant improvement in terms of computational costs while maintaining similar accuracy, which can contribute to develop more sustainable systems for AmI.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and {Llanas Parra}, Francesc Xavier and {Reyes Ortiz}, Jorge Luis},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Workshop/W1 - NIPS 2012.pdf:pdf},
journal = {Neural Information Processing Systems Conference},
pages = {1--9},
title = {{Human activity recognition on smartphones for mobile context awareness}},
url = {http://hdl.handle.net/2117/22167},
year = {2012}
}
@article{Park2008,
abstract = {The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (i.e., double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse-Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods, including bridge regression and a robust variant.},
author = {Park, Trevor and Casella, George},
doi = {10.1198/016214508000000337},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Lasso.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {empirical bayes,gibbs sampler,hierarchical model,inverse gaussian,linear regression,penalized regression,scale},
number = {482},
pages = {681--686},
pmid = {21156729},
title = {{The Bayesian Lasso}},
volume = {103},
year = {2008}
}
@article{Aiollib,
author = {Aiolli, Fabio and Ciman, Matteo and Donini, Michele and Ombretta, Gaggi},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Sito mdonini/authenticity/paper/donini{\_}persuasive14.pdf:pdf},
journal = {Persuasive 2014 Posters},
keywords = {icl},
pages = {11--13},
title = {{Serious Game to Persuade People to Use Stairs}},
year = {2014}
}
@article{Kloft2011a,
author = {Kloft, Marius and Blanchard, Gilles},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/nips2011.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24},
pages = {2438--2446},
title = {{The Local {\{}R{\}}ademacher complexity of $\backslash$ell{\_}p-norm multiple kernel learning}},
year = {2011}
}
@article{Hall2015,
abstract = {We evaluated the performance of the Disease State Index (DSI) method when predicting progression to Alzheimer's disease (AD) in patients with subjective cognitive impairment (SCI), amnestic or non-amnestic mild cognitive impairment (aMCI, naMCI). The DSI model measures patients' similarity to diagnosed cases based on available data, such as cognitive tests, the APOE genotype, CSF biomarkers and MRI. We applied the DSI model to data from the DESCRIPA cohort, where non-demented patients (N=775) with different subtypes of cognitive impairment were followed for 1 to 5 years. Classification accuracies for the subgroups were calculated with the DSI using leave-one-out crossvalidation. The DSI's classification accuracy in predicting progression to AD was 0.75 (AUC=0.83) in the total population, 0.70 (AUC=0.77) for aMCI and 0.71 (AUC=0.76) for naMCI. For a subset of approximately half of the patients with high or low DSI values, accuracy reached 0.86 (all), 0.78 (aMCI), and 0.85 (naMCI). For patients with MRI or CSF biomarker data available, theywere 0.78 (all), 0.76 (aMCI) and 0.76 (naMCI), while for clear cases the accuracies rose to 0.90 (all), 0.83 (aMCI) and 0.91 (naMCI). The results show that the DSI model can distinguish between clear and ambiguous cases, assess the severity of the disease and also provide information on the effectiveness of different biomarkers. While a specific test or biomarker may confound analysis for an individual patient, combining several different types of tests and biomarkers could be able to reveal the trajectory of the disease and improve the prediction of AD progression.},
author = {Hall, Anette and Mattila, Jussi and Koikkalainen, Juha and L{\"{o}}tjonen, Jyrki and Wolz, Robin and Scheltens, Philip and Frisoni, Giovanni and Tsolaki, Magdalini and Nobili, Flavio and Freund-levi, Yvonne and Minthon, Lennart and Fr{\"{o}}lich, Lutz and Hampel, Harald and Visser, Pieter Jelle and Soininen, Hilkka},
doi = {10.2174/1567205012666141218123829},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/CURR ALZ RES{\_}Predicting Progression AD with Dis State Index{\_}2015.pdf:pdf},
isbn = {1567-2050},
issn = {1875-5828},
journal = {Current Alzheimer Research},
keywords = {alzheimer,cerebrospinal fluid,computer-assisted diagnosis,csf,dementia,descripa,magnetic,mci,mild cognitive impairment,mri,resonance imaging,s disease},
number = {1},
pages = {69--79},
pmid = {25523428},
title = {{Predicting Progression from Cognitive Impairment to Alzheimer ' s Disease with the Disease State Index}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25523428},
volume = {12},
year = {2015}
}
@article{Bartlett2002,
abstract = {We investigate the use of certain data-dependent estimates of the complexity of a function class, called Rademacher and Gaussian complexities. In a decision theoretic setting, we prove general risk bounds in terms of these complexities. We consider function classes that can be expressed as combinations of functions from basis classes and show how the Rademacher and Gaussian complexities of such a function class can be bounded in terms of the complexity of the basis classes. We give examples of the application of these techniques in finding data-dependent risk bounds for decision trees, neural networks and support vector machines.},
author = {Bartlett, Pl Peter L and Mendelson, Shahar},
doi = {10.1162/153244303321897690},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/bartlett02a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {data-dependent complexity,error bounds,maxi-,rademacher averages},
number = {3},
pages = {463--482},
title = {{Rademacher and Gaussian Complexities: Risk Bounds and Structural Results}},
url = {http://www.crossref.org/jmlr{\_}DOI.html$\backslash$nhttp://dl.acm.org/citation.cfm?id=944944},
volume = {3},
year = {2002}
}
@article{Cortes2015,
abstract = {This paper presents an algorithm, Voted Kernel Regularization , that provides the flexibility of using potentially very complex kernel functions such as predictors based on much higher-degree polynomial kernels, while benefitting from strong learning guarantees. The success of our algorithm arises from derived bounds that suggest a new regularization penalty in terms of the Rademacher complexities of the corresponding families of kernel maps. In a series of experiments we demonstrate the improved performance of our algorithm as compared to baselines. Furthermore, the algorithm enjoys several favorable properties. The optimization problem is convex, it allows for learning with non-PDS kernels, and the solutions are highly sparse, resulting in improved classification speed and memory requirements.},
archivePrefix = {arXiv},
arxivId = {1509.04340},
author = {Cortes, Corinna and Goyal, Prasoon and Kuznetsov, Vitaly and Mohri, Mehryar},
eprint = {1509.04340},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1509.04340.pdf:pdf},
pages = {1--16},
title = {{Voted Kernel Regularization}},
url = {http://arxiv.org/abs/1509.04340},
year = {2015}
}
@article{Moeller2016,
abstract = {Multiple Kernel Learning, or MKL, extends (kernelized) SVM by attempting to learn not only a classifier/regressor but also the best kernel for the training task, usually from a combination of existing kernel functions. Most MKL methods seek the combined kernel that performs best over every training example, sacrificing performance in some areas to seek a global optimum. Localized kernel learning (LKL) overcomes this limitation by allowing the training algorithm to match a component kernel to the examples that can exploit it best. Several approaches to the localized kernel learning problem have been explored in the last several years. We unify many of these approaches under one simple system and design a new algorithm with improved performance. We also develop enhanced versions of existing algorithms, with an eye on scalability and performance.},
archivePrefix = {arXiv},
arxivId = {1603.01374},
author = {Moeller, John and Swaminathan, Sarathkrishna and Venkatasubramanian, Suresh},
eprint = {1603.01374},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1603.01374.pdf:pdf},
pages = {1--14},
title = {{A Unified View of Localized Kernel Learning}},
url = {http://arxiv.org/abs/1603.01374},
year = {2016}
}
@article{Powers2011,
abstract = {Commonly used evaluation measures including Recall, Precision, F-Measure and Rand Accuracy are biased and should not be used without clear understanding of the biases, and corresponding identification of chance or base case levels of the statistic. Using these measures a system that performs worse in the objective sense of Informedness, can appear to perform better under any of these commonly used measures. We discuss several concepts and measures that reflect the probability that prediction is informed versus chance. Informedness and introduce Markedness as a dual measure for the probability that prediction is marked versus chance. Finally we demonstrate elegant connections between the concepts of Informedness, Markedness, Correlation and Significance as well as their intuitive relationships with Recall and Precision, and outline the extension from the dichotomous case to the general multi-class case.},
author = {Powers, D M W},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2{\_}1{\_}1{\_}JMLT.pdf:pdf},
journal = {Journal of Machine Learning Technologies},
keywords = {Correlation,DeltaP,F-Measure,Informedness and Markedness,Kappa,Rand Accuracy,Significance,–Recall and Precision},
number = {1},
pages = {2229--3981},
title = {{Evaluation: From Precision, Recall and F-Measure to ROC., Informedness, Markedness {\&} Correlation}},
url = {http://www.bioinfo.in/contents.php?id=51},
volume = {2},
year = {2011}
}
@article{Weston2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1410.3916v7},
author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
eprint = {arXiv:1410.3916v7},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1410.3916v7.pdf:pdf},
pages = {1--15},
title = {{Memory Networks}},
year = {2015}
}
@article{Ng1998,
abstract = {We consder eature selection in the "wrapper" model of featue election. This typically involves an NP-hard optimizion problem that is aoximated byeustic search fr a "good" feature subset. First considering the ializtion where this ptmization is perf exactly, we give a rigorous bound for generalization error unde feature selection. The search heuristics cally used are then immediately seen as tryng to achieve the error given in our bounds, and succeesng to the extent that hey succeed in solving the optimization.The bound suggests that,n thepresence of man "irrelevant" features, the main surc of error in wraper model feature elecion is from "ovrfitting" hold-out or cross-alidation data. This motes a new algorithmthat, again under the idealizationof performing seractly, has sample omplexity (and error) that ro logarithmicaly in the number of "irrelevant" features which meanst can tolerate havng anur of "irrelevant" features exponential in the number of training examples- and search heuristics are again seen to be directly tryng ot reach this bound. Experimental resuts on a problem using mulated data how the new algorithm having muc highr tolerance to irrelevant featuresha the standard wrapper model. Lastly, we also discuss ramificationsthat sample compley logarithmc in the number of irelevant featres might have for feature dsign in actual applications of learning.},
author = {Ng, Andrew Y},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/icml98-fs.pdf:pdf},
journal = {Proc. 15th Interntional Conference on Machine Learning},
pages = {404--412},
title = {{On Feature selection: Learning with Exponentially many Irrelevant Features Training Examples}},
year = {1998}
}
@article{Vasilescu2009,
author = {Vasilescu, Mao},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/vasilescu-thesis.pdf:pdf},
title = {{A Multilinear (Tensor) Algebraic Framework for Computer Graphics, Computer Vision, and Machine Learning}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Multilinear+(Tensor)+Algebraic+Framework+for+Computer+Graphics,+Computer+Vision,+and+Machine+Learning{\#}0},
year = {2009}
}
@article{Cortes2009b,
abstract = {This paper studies the general problem of learning kernels based on a polynomial combination of base kernels. We analyze this problem in the case of regression and the kernel ridge regression algorithm. We examine the corresponding learning kernel optimization problem, show how that minimax problem can be reduced to a simpler minimization problem, and prove that the global solution of this problem always lies on the boundary. We give a projection-based gradient descent algorithm for solving the optimization problem, shown empirically to converge in few iterations. Finally, we report the results of extensive experiments with this algorithm using several publicly available datasets demonstrating the effectiveness of our technique.},
author = {Cortes, Corinna and Mohri, M and Rostamizadeh, A},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes, Mohri, Rostamizadeh - 2009 - Learning non-linear combinations of kernels.pdf:pdf},
isbn = {9781615679119},
journal = {Advances in neural information processing systems},
pages = {1--9},
title = {{Learning non-linear combinations of kernels}},
url = {http://www.cs.nyu.edu/{~}mohri/pub/nlk.pdf},
year = {2009}
}
@article{Qi2015a,
author = {Qi, Chengming and Wang, Yuping and Tian, Wenjie and Wang, Qun},
doi = {10.1016/j.chaos.2015.10.024},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0960077915003343-main.pdf:pdf},
issn = {09600779},
journal = {Chaos, Solitons {\&} Fractals},
keywords = {Classification,Ensemble learning,Kullback?Leibler},
pages = {1--12},
publisher = {Elsevier Ltd},
title = {{Multiple kernel boosting framework based on information measure for classification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0960077915003343},
volume = {000},
year = {2015}
}
@article{Zhou2013,
author = {Zhou, Ke and Zha, Hongyuan and Song, Le},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ICML12-13/zhou13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {1301--1309},
title = {{Learning Triggering Kernels for Multi-dimensional Hawkes Processes}},
url = {http://jmlr.org/proceedings/papers/v28/zhou13.html},
volume = {28},
year = {2013}
}
@article{Lampert2008,
author = {Lampert, C and Blaschko, M},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/DAGM2008-Lampert-Blaschko{\_}5072[0].pdf:pdf},
isbn = {978-3-540-69320-8},
journal = {Proceedings of the 30th DAGM symposium on Pattern Recognition},
pages = {31--40},
title = {{A multiple kernel learning approach to joint multi-class object detection}},
url = {http://www.springerlink.com/index/1960J7H4W507R82G.pdf},
volume = {5096},
year = {2008}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
eprint = {1409.2329},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1409.2329v5.pdf:pdf},
isbn = {078036404X},
journal = {Icrl},
number = {2013},
pages = {1--8},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{Rodriguez-Lujan2010,
author = {Rodriguez-Lujan, I and Huerta, R},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/jmlr.org{\_}papers{\_}volume11{\_}rodriguez-lujan10a{\_}rodriguez-lujan10a.pdf:pdf},
journal = {The Journal of Machine {\ldots}},
keywords = {feature selection,high-,large data set,nystr,om method,quadratic programming},
pages = {1491--1516},
title = {{Quadratic programming feature selection}},
url = {http://dl.acm.org/citation.cfm?id=1859900},
volume = {11},
year = {2010}
}
@article{Srebro2006,
abstract = {Consider the problem of learning a kernel for use in SVM classification. We bound the estimation error of a large margin classifier when the kernel, relative to which this margin is defined, is chosen from a family of kernels based on the training sample. For a kernel family with pseudodimension dϕ, we present a bound of on the estimation error for SVMs with margin $\gamma$. This is the first bound in which the relation between the margin term and the family-of-kernels term is additive rather then multiplicative. The pseudodimension of families of linear combinations of base kernels is the number of base kernels. Unlike in previous (multiplicative) bounds, there is no non-negativity requirement on the coefficients of the linear combinations. We also give simple bounds on the pseudodimension for families of Gaussian kernels.},
author = {Srebro, Nathan and Ben-David, Shai},
doi = {10.1007/11776420_15},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/147-kbound.pdf:pdf},
isbn = {978-3-540-35294-5},
issn = {03029743},
journal = {19th Annual Conference on Learning Theory, COLT 2006},
pages = {169--183},
title = {{Learning Bounds for Support Vector Machines with Learned Kernels}},
year = {2006}
}
@article{Hochreiter2001,
abstract = {Introduction Recurrent networks (crossreference Chapter 12) can, in principle, use their feedback connections to store representations of recent input events in the form of activations. The most widely used algorithms for learning what to put in short-term memory, however, take too much time to be feasible or do not work well at all, especially when minimal time lags between inputs and corresponding teacher signals are long. Although theoretically fascinating, they do not provide clear practical advantages over, say, backprop in feedforward networks with limited time windows (see crossreference Chapters 11 and 12). With conventional {\&}034;algorithms based on the computation of the complete gradient{\&}034;, such as {\&}034;Back-Propagation Through Time{\&}034; (BPTT, e.g., 22, 27, 26) or {\&}034;Real-Time Recurrent Learning{\&}034; (RTRL, e.g., 21) error signals {\&}034;flowing backwards in time{\&}034; tend to either (1) blow up or (2) vanish: the temporal evolution of the backpropagated error ex},
author = {Hochreiter, S and Bengio, Y and Frasconi, P and Schmidhuber, J},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/10.1.1.24.7321.pdf:pdf},
journal = {A Field Guide to Dynamical Recurrent Networks},
pages = {237--243},
title = {{Gradient flow in recurrent nets: the difficulty of learning long-term dependencies}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.7321{\&}rep=rep1{\&}type=pdf},
year = {2001}
}
@article{Hino2013,
author = {Hino, Hideitsu},
doi = {10.1109/MLSP.2013.6661956},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/06661956.pdf:pdf},
isbn = {9781479911806},
issn = {21610363},
journal = {IEEE International Workshop on Machine Learning for Signal Processing, MLSP},
keywords = {discriminant analysis,entropy power inequality,multiple kernel learning},
title = {{Gaussian multiple kernel learning with entropy power inequality}},
year = {2013}
}
@article{Suk2013,
author = {Suk, Heung-Il and Shen, Dinggang},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/chp{\%}3A10.1007{\%}2F978-3-642-40763-5{\_}72.pdf:pdf},
isbn = {978-3-642-40763-5; 978-3-642-40762-8},
journal = {Medical Image Computing and Computer-Assisted Intervention - Miccai 2013, Pt Ii},
pages = {583--590},
title = {{Deep Learning-Based Feature Representation for AD/MCI Classification}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000342835100072},
volume = {8150},
year = {2013}
}
@article{Montavon2011,
abstract = {When training deep networks it is common knowledge that an efficient and well generalizing representation of the problem is formed. In this paper we aim to elucidate what makes the emerging representation successful. We analyze the layer-wise evolution of the representation in a deep network by building a sequence of deeper and deeper kernels that subsume the mapping performed by more and more layers of the deep network and measuring how these increasingly complex kernels fit the learning problem. We observe that deep networks create increasingly better representations of the learning problem and that the structure of the deep network controls how fast the representation of the task is formed layer after layer.},
author = {Montavon, Gregoire and Braun, Mikio and M{\"{u}}ller, Klaus-Robert},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/montavon11a.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
keywords = {Theory {\&} Algorithms},
pages = {2563--2581},
title = {{Kernel Analysis of Deep Networks}},
url = {http://eprints.pascal-network.org/archive/00009497/},
volume = {12},
year = {2011}
}
@article{Scott2014,
author = {Scott, Clayton},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/15{\_}rademacher{\_}kernel.pdf:pdf},
pages = {1--4},
title = {{Rademacher Complexity of Kernel Classes}},
year = {2014}
}
@article{Combettes2016,
abstract = {We analyze a class of norms defined via an optimal interpolation problem involving the composition of norms and a linear operator. This framework encompasses a number of norms which have been used as regularizers in machine learning and statistics. In particular, these include the latent group lasso, the overlapping group lasso, and certain norms used for learning tensors. We establish basic properties of this class of regularizers and we provide the dual norm. We present a novel stochastic block-coordinate version of the Douglas-Rachford algorithm to solve regularization problems with these regularizers. A unique feature of the algorithm is that it yields iterates that converge to a solution in the case of non smooth losses and random block updates. Finally, we present numerical experiments on the latent group lasso.},
archivePrefix = {arXiv},
arxivId = {1603.09273},
author = {Combettes, Patrick L. and McDonald, Andrew M. and Micchelli, Charles A. and Pontil, Massimiliano},
eprint = {1603.09273},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1603.09273.pdf:pdf},
pages = {1--23},
title = {{Learning with Optimal Interpolation Norms: Properties and Algorithms}},
url = {http://arxiv.org/abs/1603.09273},
year = {2016}
}
@article{Castro2014,
abstract = {FMRI data are acquired as complex-valued spatiotemporal images. Despite the fact that several studies have identified the presence of novel information in the phase images, they are usually discarded due to their noisy nature. Several approaches have been devised to incorporate magnitude and phase data, but none of them has performed between-group inference or classification. Multiple kernel learning (MKL) is a powerful field of machine learning that finds an automatic combination of kernel functions that can be applied to multiple data sources. By analyzing this combination of kernels, the most informative data sources can be found, hence providing a better understanding of the analyzed learning task. This paper presents a methodology based on a new MKL algorithm ($\nu$-MKL) capable of achieving a tunable sparse selection of features' sets (brain regions' patterns) that improves the classification accuracy rate of healthy controls and schizophrenia patients by 5{\%} when phase data is included. In addition, the proposed method achieves accuracy rates that are equivalent to those obtained by the state of the art lp-norm MKL algorithm on the schizophrenia dataset and we argue that it better identifies the brain regions that show discriminative activation between groups. This claim is supported by the more accurate detection achieved by $\nu$-MKL of the degree of information present on regions of spatial maps extracted from a simulated fMRI dataset. In summary, we present an MKL-based methodology that improves schizophrenia characterization by using both magnitude and phase fMRI data and is also capable of detecting the brain regions that convey most of the discriminative information between patients and controls. {\textcopyright} 2013 Elsevier Inc.},
author = {Castro, Eduardo and G{\'{o}}mez-Verdejo, Vanessa and Mart{\'{i}}nez-Ram{\'{o}}n, Manel and Kiehl, Kent a. and Calhoun, Vince D.},
doi = {10.1016/j.neuroimage.2013.10.065},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Castro et al. - 2014 - A multiple kernel learning approach to perform classification of groups from complex-valued fMRI data analysis Ap.pdf:pdf},
isbn = {1095-9572 (Electronic)$\backslash$n1053-8119 (Linking)},
issn = {10538119},
journal = {NeuroImage},
keywords = {Complex-valued fMRI data,Feature selection,Independent component analysis,Multiple kernel learning,Schizophrenia,Support vector machines},
pages = {1--17},
pmid = {24225489},
publisher = {Elsevier Inc.},
title = {{A multiple kernel learning approach to perform classification of groups from complex-valued fMRI data analysis: Application to schizophrenia}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2013.10.065},
volume = {87},
year = {2014}
}
@article{Ailon2010,
abstract = {We study the problem of learning to rank from pairwise preferences, and solve a long-standing open problem that has led to development of many heuristics but no provable results for our particular problem. Given a set {\$}V{\$} of {\$}n{\$} elements, we wish to linearly order them given pairwise preference labels. A pairwise preference label is obtained as a response, typically from a human, to the question "which if preferred, u or v?{\$} for two elements {\$}u,v$\backslash$in V{\$}. We assume possible non-transitivity paradoxes which may arise naturally due to human mistakes or irrationality. The goal is to linearly order the elements from the most preferred to the least preferred, while disagreeing with as few pairwise preference labels as possible. Our performance is measured by two parameters: The loss and the query complexity (number of pairwise preference labels we obtain). This is a typical learning problem, with the exception that the space from which the pairwise preferences is drawn is finite, consisting of {\$}{\{}n$\backslash$choose 2{\}}{\$} possibilities only. We present an active learning algorithm for this problem, with query bounds significantly beating general (non active) bounds for the same error guarantee, while almost achieving the information theoretical lower bound. Our main construct is a decomposition of the input s.t. (i) each block incurs high loss at optimum, and (ii) the optimal solution respecting the decomposition is not much worse than the true opt. The decomposition is done by adapting a recent result by Kenyon and Schudy for a related combinatorial optimization problem to the query efficient setting. We thus settle an open problem posed by learning-to-rank theoreticians and practitioners: What is a provably correct way to sample preference labels? To further show the power and practicality of our solution, we show how to use it in concert with an SVM relaxation.},
archivePrefix = {arXiv},
arxivId = {1011.0108},
author = {Ailon, Nir},
eprint = {1011.0108},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ailon12a.pdf:pdf},
isbn = {9781618395993},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {active learning,pairwise ranking,preferences,ranking,statistical learning theory},
pages = {137--164},
title = {{An Active Learning Algorithm for Ranking from Pairwise Preferences with an Almost Optimal Query Complexity}},
url = {http://arxiv.org/abs/1011.0108},
volume = {13},
year = {2010}
}
@article{Aiolli2014,
author = {Aiolli, Fabio and Donini, Michele and Poletti, Enea and Grisan, Enrico},
doi = {10.1007/978-3-319-00846-2_65},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Sito mdonini/authenticity/paper/donini{\_}medicon13.pdf:pdf},
isbn = {9783319008455},
issn = {16800737},
journal = {IFMBE Proceedings},
keywords = {Annotation,Brain,Magnetic resonance imaging,Stacked learning,Textures},
pages = {261--264},
title = {{Stacked models for efficient annotation of brain tissues in MR volumes}},
volume = {41},
year = {2014}
}
@article{Kloft2011,
abstract = {The goal of machine learning is to learn unknown concepts from data. In real-world applications such as bioinformatics and computer vision, data frequently arises from multiple heterogeneous sources or is represented by various complementary views, the right choiceor even combinationof which being unknown. To this end, the multiple kernel learning (MKL) framework provides a mathematically sound solution. Previous approaches to learning with multiple kernels promote sparse kernel combinations to support interpretability and scalability. Unfortunately, classical approaches to learning with multiple kernels are rarely observed to outperform trivial baselines in practical applications. In this thesis, I approach learning with multiple kernels from a unifying view which shows previous works to be only particular instances of a much more general family of multi-kernel methods. To allow for more eective kernel mixtures, I have developed the `p-norm multiple kernel learning methodology, which, to sum it up, is both more ecient and more accurate than previous approaches to multiple kernel learning, as demonstrated on several data sets. In particular, I derive optimization algorithms that are much faster than the commonly used ones, allowing to deal with up to ten thousands of data points and thousands of kernels at the same time. Empirical applications of `p-norm MKL to diverse, challenging problems from the domains of bioinformatics and computer vision show that `p-norm MKL achieves accuracies that surpass the state-of- the-art. The proposed techniques are underpinned by deep foundations in the theory of learning: I prove tight lower and upper bounds on the local and global Rademacher complexities of the hypothesis class associated with `p-norm MKL, which yields excess risk bounds with fast convergence rates, thus being tighter than existing bounds for MKL, which only achieve slow convergence rates. I also connect the minimal values of the bounds with the soft sparsity of the underlying Bayes hypothesis, proving that for a large range of learning scenarios `p-norm MKL attains substantial stronger general- ization guarantees than classical approaches to learning with multiple kernels. Using a methodology based on the theoretical bounds, and exemplied by means of a controlled toy experiment, I investigate why MKL is eective in real applications. Data sets, source code and implementations of the algorithms, additional scripts for model selection, and further information are freely available online.},
author = {Kloft, Marius},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/kloft11a.pdf:pdf},
isbn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {learning,statistics {\&} optimisation},
number = {3},
pages = {953--997},
title = {{Lp-Norm Multiple Kernel Learning}},
url = {http://eprints.pascal-network.org/archive/00009404/},
volume = {12},
year = {2011}
}
@article{Pasa2015,
author = {Pasa, Luca and Testolin, Alberto and Sperduti, Alessandro},
doi = {10.1016/j.neucom.2014.11.081},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0925231215003689-main.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Curriculum learning,Hidden Markov Model,Pre‐Training,RNNRBM,Recurrent Neural Networks,Sequential Data},
pages = {323--333},
publisher = {Elsevier},
title = {{Neural Networks for Sequential Data: a Pre‐training Approach based on Hidden Markov Models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215003689},
volume = {169},
year = {2015}
}
@article{Micchelli2010,
abstract = {Give a family of penalty $\backslash$Sigma( $\backslash$beta , $\backslash$Lambda ), and When $\backslash$Lambda are Box, Wedge =={\textgreater} L1+L2 norm and automatical partition group lasso.},
author = {Micchelli, Charles A. and Morales, Jean M. and Pontil, Massimiliano},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/4137-a-family-of-penalty-functions-for-structured-sparsity.pdf:pdf},
isbn = {9781617823800},
journal = {Nips'10},
keywords = {sparse structure,structured sparse},
pages = {1612--1623},
title = {{A Family of Penalty Functions for Structured Sparsity}},
year = {2010}
}
@article{Varma2009,
author = {Varma, Manik and Babu, Bodla Rakesh},
doi = {10.1145/1553374.1553510},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varma, Babu - 2009 - More generality in efficient multiple kernel learning.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
title = {{More generality in efficient multiple kernel learning}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553510},
year = {2009}
}
@article{Cortes2015a,
author = {Cortes, Corinna},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/cortes15a.pdf:pdf},
keywords = {feature extraction,kernel methods,learning theory,rademacher complexity},
pages = {72--89},
title = {{Kernel Extraction via Voted Risk Minimization}},
volume = {44},
year = {2015}
}
@article{Bach2008,
author = {Bach, Francis R. and Ecole, Inria and Sup, Normale},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/8332-38560-1-PB.pdf:pdf},
keywords = {Novel Machine Learning Algorithms},
number = {August},
pages = {1975--1981},
title = {{Multiple Kernel Learning}},
year = {2008}
}
@book{Devroye1997,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Devroye, Luc and Gyorfi, Laszlo and Lugosi, Gabor},
booktitle = {Applications of Mathematics Stochastic Modelling and Applied Probability},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Luc Devroye, Laszlo Gyorfi, Gabor Lugosi - A Probabilistic Theory of Pattern Recognition copy.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{A Probabilistic Theory of Pattern Recognition}},
year = {1997}
}
@article{Xu2010,
author = {Xu, Z and Jin, R and Yang, H and King, I and Lyu, M R},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/icml2010-simple.pdf:pdf},
isbn = {9781605589077},
journal = {International Conference on Machine Learning (ICML)},
pages = {1191--1198},
title = {{Simple and efficient multiple kernel learning by group lasso}},
year = {2010}
}
@article{Bach2008a,
author = {Bach, Francis},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Talk{\_}online{\_}hkl.pdf:pdf},
journal = {Nips},
number = {May},
pages = {1--18},
title = {{Hierarchical kernel learning}},
url = {http://www.di.ens.fr/{~}fbach/Talk{\_}online{\_}hkl.pdf$\backslash$nhttps://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning.pdf},
year = {2008}
}
@article{Pasa,
author = {Pasa, Luca and Sperduti, Alessandro},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{Pre-training of Recurrent Neural Networks via Linear Autoencoders}}
}
@article{Guyon2002,
abstract = {DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues. In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal patients, we build a classifier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address this problem select genes with correlation techniques. We propose a new method of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE). We demonstrate experimentally that the genes selected by our techniques yield better classification performance and are biologically relevant to cancer. In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leave-one-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error). In the colon cancer database, using only 4 genes our method is 98{\%} accurate, while the baseline method is only 86{\%} accurate.},
author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
doi = {10.1023/A:1012487302797},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1023{\%}2FA{\%}3A1012487302797.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {Cancer classification,DNA micro-array,Diagnosis,Diagnostic tests,Drug discovery,Feature selection,Gene selection,Genomics,Proteomics,RNA expression,Recursive feature elimination,Support vector machines},
number = {1-3},
pages = {389--422},
pmid = {21889629},
title = {{Gene selection for cancer classification using support vector machines}},
volume = {46},
year = {2002}
}
@article{Koltchinskii2001,
abstract = {We suggest a penalty function to be used in various problems of$\backslash$nstructural risk minimization. This penalty is data dependent and is$\backslash$nbased on the sup-norm of the so-called Rademacher process indexed by the$\backslash$nunderlying class of functions (sets). The standard complexity penalties,$\backslash$nused in learning problems and based on the VC-dimensions of the classes,$\backslash$nare conservative upper bounds (in a probabilistic sense, uniformly over$\backslash$nthe set of all underlying distributions) for the penalty we suggest.$\backslash$nThus, for a particular distribution of training examples, one can expect$\backslash$nbetter performance of learning algorithms with the data-driven$\backslash$nRademacher penalties. We obtain oracle inequalities for the theoretical$\backslash$nrisk of estimators, obtained by structural minimization of the empirical$\backslash$nrisk with Rademacher penalties. The inequalities imply some form of$\backslash$noptimality of the empirical risk minimizers. We also suggest an$\backslash$niterative approach to structural risk minimization with Rademacher$\backslash$npenalties, in which the hierarchy of classes is not given in advance,$\backslash$nbut is determined in the data-driven iterative process of risk$\backslash$nminimization. We prove probabilistic oracle inequalities for the$\backslash$ntheoretical risk of the estimators based on this approach as well},
author = {Koltchinskii, V.},
doi = {10.1109/18.930926},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/00930926.pdf:pdf},
isbn = {0018-9448 VO  - 47},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Classification,Empirical process,Iterative structural risk minimization,Oracle inequalities,Rademacher penalty,Structural risk minimization},
number = {5},
pages = {1902--1914},
title = {{Rademacher penalties and structural risk minimization}},
volume = {47},
year = {2001}
}
@article{Anguita2013f,
abstract = {The exploitation of smartphones for Human Activity Recognition (HAR) has been an active research area in which the development of fast and efficient Machine Learning approaches is crucial for preserving battery life and reducing computational requirements. In this work, we present a HAR system which incorporates smartphone-embedded inertial sensors and uses Support Vector Machines (SVM) for the classification of Activities of Daily Living (ADL). By exploiting a publicly available benchmark HAR dataset, we show the benefits of adding smartphones gyroscope signals into the recognition system against the traditional accelerometer-based approach, and explore two feature selection mechanisms for allowing a radically faster recognition: the utilization of exclusively time domain features and the adaptation of the L1 SVM model which performs comparably to non-linear approaches while neglecting a large number of non-informative features.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and Reyes-Ortiz, Jorge Luis},
doi = {10.1007/978-3-642-40728-4_54},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C17.pdf:pdf},
isbn = {9783642407277},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Feature Selection,Human Activity Recognition,L1 SVM,SVM,Smartphones},
pages = {426--433},
title = {{Training computationally efficient smartphone-based human activity recognition models}},
volume = {8131 LNCS},
year = {2013}
}
@article{Cires2012,
abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
archivePrefix = {arXiv},
arxivId = {1202.2745v1},
author = {Cires, Dan and Meier, Ueli},
doi = {10.1109/CVPR.2012.6248110},
eprint = {1202.2745v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1202.2745v1.pdf:pdf},
isbn = {9781467312288},
issn = {1063-6919},
journal = {Applied Sciences},
number = {February},
pages = {20},
pmid = {18225950},
title = {{Multi-column Deep Neural Networks for Image Classification}},
url = {http://arxiv.org/abs/1202.2745},
year = {2012}
}
@article{Anguita2014b,
abstract = {In this paper, we derive a deep connection between the Vapnik-Chervonenkis (VC) entropy and the Rademacher complexity. For this purpose, we first refine some previously known relationships between the two notions of complexity and then derive new results, which allow computing an admissible range for the Rademacher complexity, given a value of the VC-entropy, and vice versa. The approach adopted in this paper is new and relies on the careful analysis of the combinatorial nature of the problem. The obtained results improve the state of the art on this research topic.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, L. and Ridella, Sandro},
doi = {10.1109/TNNLS.2014.2307359},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Journal/J06 - TNNLS.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {12},
pages = {2202--2211},
title = {{A Deep Connection Between the Vapnik-Chervonenkis Entropy and the Rademacher Complexity}},
url = {http://ieeexplore.ieee.org/ielx7/5962385/6960135/06762903.pdf?tp={\&}arnumber=6762903{\&}isnumber=6960135$\backslash$nhttp://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6762903{\&}sortType=asc{\_}p{\_}Sequence{\&}filter=AND(p{\_}IS{\_}Number:6960135)},
volume = {25},
year = {2014}
}
@article{Tu2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05310v1},
author = {Tu, Stephen and Roelofs, Rebecca and Venkataraman, Shivaram and Recht, Benjamin},
eprint = {arXiv:1602.05310v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1602.05310.pdf:pdf},
title = {{Large Scale Kernel Learning using Block Coordinate Descent}},
year = {2016}
}
@book{Wang2014,
author = {Wang, Fei and Sun, Jimeng},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1007/s10618-014-0356-z},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2Fs10618-014-0356-z.pdf:pdf},
isbn = {1061801403},
issn = {13845810},
keywords = {Data mining,Dimensionality reduction,Distance metric learning},
pages = {534--564},
title = {{Survey on distance metric learning and dimensionality reduction in data mining}},
year = {2014}
}
@article{Kakade2009,
abstract = {There is growing body of learning problems for which it is natural to organize the parameters into matrix, so as to appropriately regularize the parameters under some matrix norm (in order to impose some more sophisticated prior knowledge). This work describes and analyzes a systematic method for constructing such matrix-based, regularization methods. In particular, we focus on how the underlying statistical properties of a given problem can help us decide which regularization function is appropriate.   Our methodology is based on the known duality fact: that a function is strongly convex with respect to some norm if and only if its conjugate function is strongly smooth with respect to the dual norm. This result has already been found to be a key component in deriving and analyzing several learning algorithms. We demonstrate the potential of this framework by deriving novel generalization and regret bounds for multi-task learning, multi-class learning, and kernel learning.},
archivePrefix = {arXiv},
arxivId = {0910.0610},
author = {Kakade, Sham M. and Shalev-Shwartz, Shai and Tewari, Ambuj},
eprint = {0910.0610},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakade, Shalev-Shwartz, Tewari - 2009 - Regularization Techniques for Learning with Matrices.pdf:pdf},
issn = {15324435},
keywords = {generalization bounds,learning,multi-class learning,multi-task,multiple kernel learning,regret bounds,regularization,strong convexity},
pages = {1865--1890},
title = {{Regularization Techniques for Learning with Matrices}},
url = {http://arxiv.org/abs/0910.0610},
volume = {13},
year = {2009}
}
@article{Koral1992a,
abstract = {Dynamic systems defined on the scale of neural ensembles are well-suited to model the spatiotemporal dynamics of electroencephalographic (EEG) and magnetoencephalographic (MEG) data. We develop a methodological framework, which defines the activity of neural ensembles, the neural field, on a sphere in three dimensions. Using Magnetic Resonance Imaging (MRI) we map the neural field dynamics from the sphere onto the folded cortical surface of a hemisphere. The neural field represents the current flow perpendicular to the cortex and thus allows the calculation of the electric potentials on the surface of the skull and the magnetic fields outside the skull to be measured by EEG and MEG, respectively. For demonstration of the dynamics, we present the propagation of activation at a single cortical site resulting from a transient input. Non-trivial mappings between the multiple levels of observation are obtained which would not be predicted by inverse solution techniques. Considering recent results mapping large-scale brain dynamics (EEG, MEG) onto behavioral motor patterns, this paper provides a discussion of the causal chain starting from local neural ensemble dynamics through encephalographic data to behavior.},
author = {Koral, Kenneth F.},
doi = {10.1097/00003072-199201000-00023},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/bok{\%}3A978-3-319-19992-4.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/chp{\%}3A10.1007{\%}2F978-3-319-19992-4{\_}30.pdf:pdf},
isbn = {3-540-54246-9},
issn = {0363-9762},
journal = {Clinical Nuclear Medicine},
pages = {69},
pmid = {26221676},
title = {{Information Processing in Medical Imaging}},
url = {http://www.springerlink.com/index/10.1007/3-540-45729-1},
volume = {17},
year = {1992}
}
@article{Baldassarre2016,
author = {Baldassarre, Luca and Pontil, Massimiliano and Mour, Janaina},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/neurosparse{\_}journal{\_}version.pdf:pdf},
keywords = {model selection,reproducibility,sparse methods,structured sparsity},
title = {{The role of sparsity and stability for fMRI brain decoding}},
year = {2016}
}
@article{Guido,
author = {Guido, Serha},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guido - Unknown - Multiple Kernel Learning.pdf:pdf},
title = {{Multiple Kernel Learning}}
}
@article{Larochelle2009,
abstract = {Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization often appears to get stuck in poor solutions. Hinton et al. recently proposed a greedy layer-wise unsupervised learning procedure relying on the training algorithm of restricted Boltzmann machines (RBM) to initialize the parameters of a deep belief network (DBN), a generative model with many layers of hidden causal variables. This was followed by the proposal of another greedy layer-wise procedure, relying on the usage of autoassociator networks. In the context of the above optimization problem, we study these algorithms empirically to better understand their success. Our experiments confirm the hypothesis that the greedy layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, but also implicitly acts as a sort of regularization that brings better generalization and encourages internal distributed representations that are high-level abstractions of the input. We also present a series of experiments aimed at evaluating the link between the performance of deep neural networks and practical aspects of their topology, for example, demonstrating cases where the addition of more depth helps. Finally, we empirically explore simple variants of these training algorithms, such as the use of different RBM input unit distributions, a simple way of combining gradient estimators to improve performance, as well as on-line versions of those algorithms. },
author = {Larochelle, Hugo and Larochelle, Hugo and Bengio, Yoshua and Bengio, Yoshua and Lourador, Jerome and Lourador, Jerome and Lamblin, Pascal and Lamblin, Pascal},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/jmlr-larochelle09a.pdf:pdf},
isbn = {3531207857},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {1--40},
title = {{Exploring Strategies for Training Deep Neural Networks}},
volume = {10},
year = {2009}
}
@article{Parikh2014,
author = {Parikh, Neal and Boyd, Stephen},
doi = {10.1007/s12532-013-0061-8},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/block{\_}splitting.pdf:pdf},
isbn = {1253201300618},
issn = {18672949},
journal = {Mathematical Programming Computation},
keywords = {Alternating direction method of multipliers,Cone programming,Distributed optimization,Machine learning,Operator splitting,Proximal operators},
number = {1},
pages = {77--102},
title = {{Block splitting for distributed optimization}},
volume = {6},
year = {2014}
}
@article{Anguita2004a,
abstract = {We derive here new generalization bounds, based on Rademacher Complexity the- ory, for model selection and error estimation of linear (kernel) classiﬁers, which exploit the availability of unlabeled samples. In particular, two results are ob- tained: the ﬁrst one shows that, using the unlabeled samples, the conﬁdence term of the conventional bound can be reduced by a factor of three; the second one shows that the unlabeled samples can be used to obtain much tighter bounds, by building localized versions of the hypothesis class containing the optimal classifier.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C05.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C05{\_}P.pdf:pdf},
isbn = {9781618395993},
number = {1},
pages = {1--9},
title = {{The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers}},
volume = {141},
year = {2004}
}
@article{Anguita2012e,
author = {Anguita, D and Ghio, a and Oneto, L},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Workshop/W4 - applepies 2012.pdf:pdf},
isbn = {9781467331333},
journal = {Applepies},
pages = {6--8},
title = {{Activity Recognition Using Smartphone Inertial Sensors}},
year = {2012}
}
@article{Learning1997,
abstract = {Abstract. Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
author = {Learning, Machine and Publishers, Kluwer Academic and Science, Computer and Caruana, R},
doi = {10.1109/TCBB.2010.22},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/mlj97.pdf:pdf},
issn = {1557-9964},
journal = {Machine learning},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel,multitask learning,parallel transfer,regression,supervised learning},
pages = {41--75},
pmid = {20421687},
title = {{Multitask Learning *}},
url = {http://link.springer.com/article/10.1023/A:1007379606734},
volume = {75},
year = {1997}
}
@article{Orabona2011,
abstract = {Proceedings of the International Conference on Machine Learning 2010},
author = {Orabona, Francesco and Jie, Luo},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/11icml{\_}a.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
keywords = {mi,multi kernel learning,stochastic optimization},
pages = {249--256},
title = {{Ultra-Fast Optimization Algorithm for Sparse Multi Kernel Learning}},
year = {2011}
}
@article{Kedem2011a,
author = {Kedem, Dor and Xu, Zhixiang Eddie and Weinberger, Kilian Q},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/gb-lmnn.pdf:pdf},
journal = {Workshops of Advances in Neural Information Processing Systems},
number = {1},
pages = {10--12},
title = {{Gradient Boosted Large Margin Nearest Neighbors}},
year = {2011}
}
@article{Wang2003,
abstract = {Based on statistical learning theory, Support Vector Machine (SVM) is a novel type of learning machine, and it contains polynomial, neural network and radial basis function (RBF) as special cases. In the RBF case, the Gaussian kernel is commonly used, while the spread parameter ?? in the Gaussian kernel is essential to generalization performance of SVMs. In this paper, determination of ?? is studied based on discussions of the influence of ?? on generalization performance. For classification problems, the optimal ?? can be computed on the basis of Fisher discrimination. And for regression problems, based on scale space theory, we demonstrate the existence of a certain range of ??, within which the generalization performance is stable. An appropriate ?? within the range can be achieved via dynamic evaluation. In addition, the lower bound of iterating step size of ?? is given. Simulation results show the effectiveness of the presented method. ?? 2002 Published by Elsevier B.V.},
author = {Wang, Wenjian and Xu, Zongben and Lu, Weizhen and Zhang, Xiaoyun},
doi = {10.1016/S0925-2312(02)00632-X},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2003 - Determination of the spread parameter in the Gaussian kernel for classification and regression.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Fisher discrimination,Scale space theory,Spread parameter,Support vector machine,The Gaussian kernel},
pages = {643--663},
title = {{Determination of the spread parameter in the Gaussian kernel for classification and regression}},
volume = {55},
year = {2003}
}
@article{V.2005,
author = {V., Graf, H.P., Cosatto, E., Bottou, L., Durdanovic, I {\&} Vapnik,},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2608-parallel-support-vector-machines-the-cascade-svm.pdf:pdf},
title = {{Parallel support vector machines: The Cascade SVM}},
url = {http://papers.nips.cc/paper/2608-parallel-support-vector-machines-the-cascade-svm.pdf},
year = {2005}
}
@article{Jawanpuria2015,
author = {Jawanpuria, Pratik},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/jawanpuria15a.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {active set method,ensemble learning,mixed-norm regularization,multi-task learning,multiple kernel learning,rule},
pages = {617--652},
title = {{Generalized Hierarchical Kernel Learning}},
volume = {16},
year = {2015}
}
@article{Jain2012a,
author = {Jain, Ashesh and Vishwanathan, S. V. N. and Varma, Manik},
doi = {10.1145/2339530.2339648},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/pubs-jain12.pdf:pdf},
isbn = {9781450314626},
journal = {Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
keywords = {multiple kernel learning,spec-,support vector machines,tral projected gradient descent},
pages = {750--758},
title = {{SPG-GMKL : Generalized Multiple Kernel Learning with a Million Kernels}},
year = {2012}
}
@article{Bousquet2003a,
author = {Bousquet, Olivier and Herrmann, Djl},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2300-on-the-complexity-of-learning-the-kernel-matrix.pdf:pdf},
journal = {Advances in neural information {\ldots}},
title = {{On the complexity of learning the kernel matrix}},
url = {https://papers.nips.cc/paper/2300-on-the-complexity-of-learning-the-kernel-matrix.pdf},
year = {2003}
}
@article{Fiot2015a,
abstract = {We explore the application of kernel-based multi-task learning techniques to forecast the demand of electricity in multiple nodes of a distribution network. We show that recently developed output kernel learning techniques are particularly well suited to solve this problem, as they allow to flexibly model the complex seasonal effects that characterize electricity demand data, while learning and exploiting correlations between multiple demand profiles. We also demonstrate that kernels with a multiplicative structure yield superior predictive performance with respect to the widely adopted (generalized) additive models. Our study is based on residential and industrial smart meter data provided by the Irish Commission for Energy Regulation (CER).},
archivePrefix = {arXiv},
arxivId = {1512.08178},
author = {Fiot, Jean-Baptiste and Dinuzzo, Francesco},
eprint = {1512.08178},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fiot, Dinuzzo - 2015 - Electricity Demand Forecasting by Multi-Task Learning.pdf:pdf},
pages = {1--16},
title = {{Electricity Demand Forecasting by Multi-Task Learning}},
url = {http://arxiv.org/abs/1512.08178},
year = {2015}
}
@article{Johnson2008,
abstract = {In this paper, we consider a framework for semi-supervised learning using spectral decomposition-based unsupervised kernel design. We relate this approach to previously proposed semi-supervised learning methods on graphs. We examine various theoretical properties of such methods. In particular, we present learning bounds and derive optimal kernel representation by minimizing the bound. Based on the theoretical analysis, we are able to demonstrate why spectral kernel design based methods can improve the predictive performance. Empirical examples are included to illustrate the main consequences of our analysis.},
author = {Johnson, Rie and Zhang, Tong},
doi = {10.1109/TIT.2007.911294},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson, Zhang - 2008 - Graph-based semi-supervised learning and spectral kernel design.pdf:pdf},
isbn = {0018-9448},
issn = {00189448},
journal = {IEEE Transactions on Information Theory},
keywords = {Graph-based semi-supervised learning,Kernel design,Transductive learning},
number = {1},
pages = {275--288},
title = {{Graph-based semi-supervised learning and spectral kernel design}},
volume = {54},
year = {2008}
}
@article{Gisbrecht2015b,
abstract = {Novel non-parametric dimensionality reduction techniques such as t-distributed stochastic neighbor embedding (t-SNE) lead to a powerful and flexible visualization of high-dimensional data. One drawback of non-parametric techniques is their lack of an explicit out-of-sample extension. In this contribution, we propose an efficient extension of t-SNE to a parametric framework, kernel t-SNE, which preserves the flexibility of basic t-SNE, but enables explicit out-of-sample extensions. We test the ability of kernel t-SNE in comparison to standard t-SNE for benchmark data sets, in particular addressing the generalization ability of the mapping for novel data. In the context of large data sets, this procedure enables us to train a mapping for a fixed size subset only, mapping all data afterwards in linear time. We demonstrate that this technique yields satisfactory results also for large data sets provided missing information due to the small size of the subset is accounted for by auxiliary information such as class labels, which can be integrated into kernel t-SNE based on the Fisher information.},
author = {Gisbrecht, Andrej and Schulz, Alexander and Hammer, Barbara},
doi = {10.1016/j.neucom.2013.11.045},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0925231214007036-main.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Dimensionality reduction,Fisher information,Out-of-sample extension,T-SNE,Visualization},
number = {1},
pages = {71--82},
publisher = {Elsevier},
title = {{Parametric nonlinear dimensionality reduction using kernel t-SNE}},
url = {http://dx.doi.org/10.1016/j.neucom.2013.11.045},
volume = {147},
year = {2015}
}
@article{Zhang2015,
author = {Zhang, Xiao and Mahoor, Mohammad H.},
doi = {10.1016/j.patcog.2015.08.026},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0031320315003143-main.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Facial action unit detection,Multi-task multiple kernel learning,Support vector machines},
publisher = {Elsevier},
title = {{Task-dependent multi-task multiple kernel learning for facial action unit detection}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320315003143},
year = {2015}
}
@article{Hussain2011,
abstract = {The paper $\backslash$cite{\{}hs-11{\}} presented a bound on the generalisation error of classifiers learned through multiple kernel learning. The bound has (an improved) $\backslash$emph{\{}additive{\}} dependence on the number of kernels (with the same logarithmic dependence on this number). However, parts of the proof were incorrectly presented in that paper. This note remedies this weakness by restating the problem and giving a detailed proof of the Rademacher complexity bound from $\backslash$cite{\{}hs-11{\}}.},
archivePrefix = {arXiv},
arxivId = {1106.6258},
author = {Hussain, Zakria and Shawe-Taylor, John},
eprint = {1106.6258},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/AISTATS2011{\_}HussainS11.pdf:pdf},
issn = {15324435},
number = {2004},
pages = {370--377},
title = {{Improved Loss Bounds for Multiple Kernel Learning}},
url = {http://arxiv.org/abs/1106.6258},
volume = {15},
year = {2011}
}
@article{Bigot2014,
author = {Bigot, Damien and Mengin, Jerome and Zanuttini, Bruno},
doi = {10.3233/978-1-61499-421-3-81},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/learning PCP-nets.pdf:pdf},
isbn = {9781614994206},
issn = {09226389},
journal = {Frontiers in Artificial Intelligence and Applications},
keywords = {Learning,PCP-net,preference,recommandation},
pages = {81--90},
title = {{Learning Probabilistic CP-nets from Observations of Optimal Items}},
volume = {264},
year = {2014}
}
@article{Huang2014a,
author = {Huang, Po Sen and Avron, Haim and Sainath, Tara N. and Sindhwani, Vikas and Ramabhadran, Bhuvana},
doi = {10.1109/ICASSP.2014.6853587},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Kernel{\_}DNN{\_}ICASSP2014.pdf:pdf},
isbn = {9781479928927},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {deep learning,distributed computing,large-scale kernel machines,random features,speech recognition},
pages = {205--209},
title = {{Kernel methods match deep neural networks on TIMIT}},
year = {2014}
}
@article{York,
author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/York - Unknown - Corinna Cortes Google Research 76 Ninth Avenue New York , NY 10012 Courant Institute and New York , NY 10012.pdf:pdf},
journal = {Power},
title = {{LEARNING SEQUENCE KERNELS}}
}
@article{Zhang2015a,
author = {Zhang, Xiangrong and Hu, Longying},
doi = {10.1016/j.neucom.2015.11.078},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1-s2.0-S0925231215019049-main.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Financial distress prediction,Listed company,Multiple kernels learning,Subspace learning,financial distress prediction,multiple kernels learning},
pages = {1--7},
publisher = {Elsevier},
title = {{A nonlinear subspace multiple kernel learning for financial distress prediction of Chinese listed companies}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215019049},
year = {2015}
}
@article{Anguita2012b,
abstract = {In-sample approaches to model selection and error estimation of support vector machines (SVMs) are not as widespread as out-of-sample methods, where part of the data is removed from the training set for validation and testing purposes, mainly because their practical application is not straightforward and the latter provide, in many cases, satisfactory results. In this paper, we survey some recent and not-so-recent results of the data-dependent structural risk minimization framework and propose a proper reformulation of the SVM learning algorithm, so that the in-sample approach can be effectively applied. The experiments, performed both on simulated and real-world datasets, show that our in-sample approach can be favorably compared to out-of-sample methods, especially in cases where the latter ones provide questionable results. In particular, when the number of samples is small compared to their dimensionality, like in classification of microarray data, our proposal can outperform conventional out-of-sample approaches such as the cross validation, the leave-one-out, or the Bootstrap methods.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
doi = {10.1109/TNNLS.2012.2202401},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Journal/J01 - TNNLS.pdf:pdf},
issn = {2162237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Bootstrap,cross validation,error estimation,leave one out,model selection,statistical learning theory (SLT),structural risk minimization (SRM),support vector machine (SVM)},
number = {9},
pages = {1390--1406},
pmid = {24807923},
title = {{In-sample and out-of-sample model selection and error estimation for support vector machines}},
volume = {23},
year = {2012}
}
@article{Anguita2014a,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C18.pdf:pdf;:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C18{\_}P.pdf:pdf},
number = {April},
pages = {23--25},
title = {{Learning with Few Bits on Small – Scale Devices : from Regularization to Energy Efficiency}},
year = {2014}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Tesi/Papers/bengio{\_}Representation{\_}Learning.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {1993},
pages = {1--30},
pmid = {23459267},
title = {{Representation learning: A review and new perspectives}},
url = {http://arxiv.org/abs/1206.5538$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6472238},
year = {2013}
}
@article{Yousefi2015,
abstract = {When faced with learning a set of inter-related tasks from a limited amount of usable data, learning each task independently may lead to poor generalization performance. Multi-Task Learning (MTL) exploits the latent relations between tasks and overcomes data scarcity limitations by co-learning all these tasks simultaneously to offer improved performance. We propose a novel Multi-Task Multiple Kernel Learning framework based on Support Vector Machines for binary classification tasks. By considering pair-wise task affinity in terms of similarity between a pair's respective feature spaces, the new framework, compared to other similar MTL approaches, offers a high degree of flexibility in determining how similar feature spaces should be, as well as which pairs of tasks should share a common feature space in order to benefit overall performance. The associated optimization problem is solved via a block coordinate descent, which employs a consensus-form Alternating Direction Method of Multipliers algorithm to optimize the Multiple Kernel Learning weights and, hence, to determine task affinities. Empirical evaluation on seven data sets exhibits a statistically significant improvement of our framework's results compared to the ones of several other Clustered Multi-Task Learning methods.},
archivePrefix = {arXiv},
arxivId = {1508.03329},
author = {Yousefi, Niloofar and Georgiopoulos, Michael and Anagnostopoulos, Georgios C.},
eprint = {1508.03329},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1508.03329.pdf:pdf},
keywords = {Multi-task Learning, Kernel Methods, Generalizatio,generalization bound,kernel methods,multi-task learning,support vector machines},
pages = {1--16},
title = {{Multi-Task Learning with Group-Specific Feature Space Sharing}},
url = {http://arxiv.org/abs/1508.03329},
year = {2015}
}
@article{Wang2016,
author = {Wang, Bo and Guo, Jichang and Zhang, Yan and Li, Chongyi},
doi = {10.1007/s00371-016-1215-2},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2Fs00371-016-1215-2.pdf:pdf},
issn = {0178-2789},
journal = {The Visual Computer},
keywords = {Feature concatenation,Hierarchical learning,Image categorization,Kernel sparse representation,Linear support vector machine,feature concatenation,hierarchical learning,image categorization,kernel sparse repre-,linear support vector machine,sentation},
publisher = {Springer Berlin Heidelberg},
title = {{Hierarchical feature concatenation-based kernel sparse representations for image categorization}},
url = {http://link.springer.com/10.1007/s00371-016-1215-2},
year = {2016}
}
@article{Lin2016,
author = {Lin, Fan and Wang, Jingbin and Zhang, Nian and Xiahou, Jianbing and McDonald, Nancy},
doi = {10.1007/s00521-015-2164-9},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/art{\%}3A10.1007{\%}2Fs00521-015-2164-9.pdf:pdf},
isbn = {0052101521649},
issn = {0941-0643},
journal = {Neural Computing and Applications},
keywords = {Alternate optimization,Cutting plane algorithm,Multi-kernel learning,Multivariate performance measures,Pattern classification,algorithm,learning {\'{a}} alternate,optimization {\'{a}} cutting plane,pattern classification {\'{a}} multivariate,performance measures {\'{a}} multi-kernel},
publisher = {Springer London},
title = {{Multi-kernel learning for multivariate performance measures optimization}},
url = {http://link.springer.com/10.1007/s00521-015-2164-9},
year = {2016}
}
@article{Kolda2008,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition:CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors.},
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/07070111x.pdf:pdf},
isbn = {0036-1445},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {15A69,15a69,65F99,65f99,ams subject classifications,candecomp,canonical decomposition,canonical decomposition (CANDECOMP),computer software,decomposition (mathematics),higher-order principal components analysis,higher-order principal components analysis (Tucker,higher-order singular value decomposition,higher-order singular value decomposition (HOSVD),hosvd,multilinea,multilinear algebra,multiway arrays,orthogonal arrays,parafac,parallel factors,parallel factors (PARAFAC),principal components analysis,tensor decompositions,tensor products,tucker},
number = {3},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://epubs.siam.org/doi/abs/10.1137/07070111X},
volume = {51},
year = {2008}
}
@article{Bousquet2003,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.1158v1},
author = {Bousquet, Olivier and Herrmann, D.J.L.},
eprint = {arXiv:1411.1158v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1411.1158v1.pdf:pdf},
isbn = {0262025507},
issn = {1049-5258},
journal = {Advances in Neural Information Processing Systems 15: Proceedings of the 2002 Conference},
number = {4},
pages = {415},
title = {{On the complexity of learning the kernel matrix}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=AAVSDw4Rw9UC{\&}oi=fnd{\&}pg=PA415{\&}dq=On+the+Complexity+of+Learning+the+Kernel+Matrix{\&}ots=U5oczAkyBR{\&}sig=GwO-ACnFDz5A7-SSNHo{\_}Fzxi-GM},
volume = {15},
year = {2003}
}
@article{Mohri2011,
author = {Mohri, Mehryar},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/kernel-nips09.pdf:pdf},
journal = {ICML 2011 tutorial},
title = {{Learning Kernels - slides}},
year = {2011}
}
@article{Belanche2013,
abstract = {In kernel-based machines, the integration of a number of different kernels to build more flexible learning methods is a promising avenue for research. In multiple kernel learning, a compound kernel is build by learning a kernel that is a positively weighted arithmetic mean of several sources. We show in this paper that the only feasible average for kernel learning is precisely the arithmetic average. We investigate general families of averaging processes and how they relate to the development of kernels. Specifically, a number of multivariate and univariate kernels are developed based on the notion of generalized means. These results can be used in more general kernel optimization procedures. {\textcopyright} 2013 Elsevier B.V.},
author = {Belanche, Llu{\'{i}}s a. and Tosi, Alessandra},
doi = {10.1016/j.neucom.2012.11.044},
file = {:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belanche, Tosi - 2013 - Averaging of kernel functions.pdf:pdf},
isbn = {9782874190490},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Averaging processes,Kernels,Support vector machines},
number = {April},
pages = {19--25},
title = {{Averaging of kernel functions}},
volume = {112},
year = {2013}
}
@article{Martens2010,
abstract = {We develop a 2nd-order optimization method based on the Hessian - free approach, and apply it to training deep auto-encoders. Without using pre-training, we obtain results superior to those reported by Hinton Salakhutdinov (2006) on the same tasks they considered. Our ...},
author = {Martens, James},
doi = {10.1155/2011/176802},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/458.pdf:pdf},
isbn = {9781605589077},
issn = {20901283},
journal = {27th International Conference on Machine Learning},
pages = {735--742},
pmid = {21512589},
title = {{Deep learning via Hessian-free optimization}},
url = {http://www.cs.toronto.edu/{~}asamir/cifar/HFO{\_}James.pdf},
volume = {951},
year = {2010}
}
@article{Anguita2011,
abstract = {The Structural Risk Minimization framework has been recently proposed as a practical method for model selection in Support Vector Machines (SVMs). The main idea is to effectively measure the complexity of the hypothesis space, as defined by the set of possible classifiers, and to use this quantity as a penalty term for guiding the model selection process. Unfortunately, the conventional SVM formulation defines a hypothesis space centered at the origin, which can cause undesired effects on the selection of the optimal classifier. We propose here a more flexible SVM formulation, which addresses this drawback, and describe a practical method for selecting more effective hypothesis spaces, leading to the improvement of the generalization ability of the final classifier.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
doi = {10.1109/IJCNN.2011.6033356},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C03.pdf:pdf},
isbn = {9781457710865},
issn = {2161-4393},
journal = {Proceedings of the International Joint Conference on Neural Networks},
pages = {1169--1176},
title = {{Selecting the hypothesis space for improving the generalization ability of support vector machines}},
year = {2011}
}
@article{Coraddu2010,
author = {Coraddu, a and Oneto, L and Ghio, a and Savio, S and Anguita, D and Figari, M},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Journal/J07 - JEME.pdf:pdf},
keywords = {asset decay forecast,codlag,combined diesel electric and,condition-based maintenance,gas,gas turbine,learning,machine,propulsion plant},
title = {{Machine Learning Approaches for Improving Condition – Based Maintenance of Naval Propulsion Plants}},
year = {2010}
}
@article{Bach2009,
abstract = {For supervised and unsupervised learning, positive definite kernels allow to use
large and potentially infinite dimensional feature spaces with a computational cost
that only depends on the number of observations. This is usually done through
the penalization of predictor functions by Euclidean or Hilbertian norms. In this
paper, we explore penalizing by sparsity-inducing norms such as the ℓ1-norm or
the block ℓ1-norm. We assume that the kernel decomposes into a large sum of
individual basis kernels which can be embedded in a directed acyclic graph; we
show that it is then possible to perform kernel selection through a hierarchical
multiple kernel learning framework, in polynomial time in the number of selected
kernels. This framework is naturally applied to non linear variable selection; our
extensive simulations on synthetic datasets and datasets from the UCI repository
show that efficiently exploring the large feature space through sparsity-inducing
norms leads to state-of-the-art predictive performance.},
archivePrefix = {arXiv},
arxivId = {0809.1493},
author = {Bach, Francis},
eprint = {0809.1493},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/nips2008{\_}fbach{\_}hkl.pdf:pdf;:C$\backslash$:/Users/Michele/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach - 2009 - Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in neural information processing systems},
keywords = {Learning/Statistics {\&} Optimisation},
number = {2},
pages = {105----112},
title = {{Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning}},
url = {http://eprints.pascal-network.org/archive/00004524/},
year = {2009}
}
@article{Argyriou2013,
abstract = {During the past years there has been an explosion of interest in learning methods based on sparsity regularization. In this paper, we discuss a general class of such methods, in which the regularizer can be expressed as the composition of a convex function {\$}\backslashomega{\$} with a linear function. This setting includes several methods such the group Lasso, the Fused Lasso, multi-task learning and many more. We present a general approach for solving regularization problems of this kind, under the assumption that the proximity operator of the function {\$}\backslashomega{\$} is available. Furthermore, we comment on the application of this approach to support vector machines, a technique pioneered by the groundbreaking work of Vladimir Vapnik.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.6086v1},
author = {Argyriou, Andreas and Baldassarre, Luca},
eprint = {arXiv:1303.6086v1},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1303.6086.pdf:pdf},
journal = {{\ldots} Inference: Festschrift in {\ldots}},
pages = {1--12},
title = {{On Sparsity Inducing Regularization Methods for Machine Learning}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-41136-6{\_}18/fulltext.html},
year = {2013}
}
@article{Yu2010b,
author = {Yu, Yaoliang and Yang, Min and Xu, Linli and White, Martha and Schuurmans, Dale},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/3936-relaxed-clipping-a-global-training-method-for-robust-regression-and-classification.pdf:pdf},
isbn = {9781617823800},
journal = {Nips},
pages = {1--9},
title = {{Relaxed Clipping: A Global Training Method for Robust Regression and Classification.}},
url = {https://papers.nips.cc/paper/3936-relaxed-clipping-a-global-training-method-for-robust-regression-and-classification.pdf},
year = {2010}
}
@article{Society2016,
author = {Benjamini, Yoav and Hochberg, Yosef},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/2346101.pdf:pdf},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {289--300},
title = {{Controlling the False Discovery Rate : A Practical and Powerful Approach to Multiple Testing}},
volume = {57},
year = {2016}
}
@article{MariusKloft2011,
author = {{Marius Kloft}},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Multiple Kernel Learning/Papers/PhDthesis.pdf:pdf},
title = {{p -Norm Multiple Kernel Learning}},
year = {2011}
}
@article{Ouyang2013,
author = {Ouyang, Hua and He, Niao and Tran, Long and Gray, Alexander},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/ouyang13.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
pages = {80--88},
title = {{Stochastic alternating direction method of multipliers}},
year = {2013}
}
@inproceedings{Donini,
abstract = {Feature selection and weighting has been an active research area in the last few decades finding success in many different applications. With the advent of Big Data, the adequate identification of the relevant features has converted feature selection in an even more indispensable step. On the other side, in kernel methods features are implicitly represented by means of feature mappings and kernels. It has been shown that the correct selection of the kernel is a crucial task, as long as an erroneous selection can lead to poor performance. Unfortunately, manually searching for an optimal kernel is a time-consuming and a sub-optimal choice. This tutorial is concerned with the use of data to learn features and kernels automatically. We provide a survey of recent methods developed for feature selection/learning and their application to real world problems, together with a review of the contributions to the ESANN 2015 special session on Feature and Kernel Learning.},
author = {Donini, Michele and Aiolli, Fabio and Trieste, Via},
booktitle = {ESANN 2015 proceedings, European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Sito mdonini/authenticity/paper/donini{\_}ESANN15.pdf:pdf},
isbn = {9782875870148},
number = {April},
pages = {22--24},
title = {{Feature and kernel learning}},
year = {2015}
}
@article{Chapelle2011,
author = {Chapelle, Olivier and Chang, Yi},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/chapelle11a.pdf:pdf},
journal = {JMLR: Workshop and Conference Proceedings},
pages = {1--24},
title = {{Yahoo ! Learning to Rank Challenge Overview}},
volume = {14},
year = {2011}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, Jurgen},
doi = {10.1162/neco.1997.9.8.1735},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/Hochreiter97{\_}lstm.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural Computation},
number = {8},
pages = {1--32},
pmid = {9377276},
title = {{Long short-term memory}},
volume = {9},
year = {1997}
}
@article{Ghio2012,
author = {Ghio, Alessandro and Anguita, Davide and Oneto, Luca and Ridella, Sandro and Schatten, Carlotta},
doi = {10.1007/978-3-642-33266-1_20},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Luca Oneto/Publication/Conference/C09.pdf:pdf},
isbn = {9783642332654},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convex Constrained Quadratic Programming,Sequential Minimal Optimization,Support Vector Machine},
number = {PART 2},
pages = {156--163},
title = {{Nested sequential minimal optimization for support vector machines}},
volume = {7553 LNCS},
year = {2012}
}
@article{Wilson2015,
abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost {\$}O(n){\$} for {\$}n{\$} training points, and predictions cost {\$}O(1){\$} per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.},
archivePrefix = {arXiv},
arxivId = {1511.02222},
author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
eprint = {1511.02222},
file = {:C$\backslash$:/Users/Michele/Dropbox/PHDUNIPD/Paper Consigliati/1511.02222.pdf:pdf},
number = {1998},
pages = {1--19},
title = {{Deep Kernel Learning}},
url = {http://arxiv.org/abs/1511.02222},
year = {2015}
}
