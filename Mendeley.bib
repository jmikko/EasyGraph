@article{Fan2008,
abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
author = {Fan, Re and Chang, Kw and Hsieh, Cj},
doi = {10.1038/oby.2011.351},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Chang, Hsieh - 2008 - LIBLINEAR A library for large linear classification.pdf:pdf},
isbn = {089791497X},
issn = {15324435},
journal = {The Journal of Machine Learning},
keywords = {large-scale linear classification,logistic regression,machine learning,open,source,support vector machines},
number = {2008},
pages = {1871--1874},
pmid = {22173572},
title = {{LIBLINEAR: A library for large linear classification}},
url = {http://dl.acm.org/citation.cfm?id=1442794},
volume = {9},
year = {2008}
}
@article{Pedregosa2012,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
archivePrefix = {arXiv},
arxivId = {1201.0490},
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
eprint = {1201.0490},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {2825--2830},
title = {{Scikit-learn: Machine Learning in Python}},
volume = {12},
year = {2011}
}
@article{Cavallanti2007,
author = {Cavallanti, Giovanni and Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio},
doi = {10.1007/s10994-007-5003-0},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cavallanti, Cesa-Bianchi, Gentile - 2007 - Tracking the best hyperplane with a simple budget Perceptron.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {feb},
number = {2-3},
pages = {143--167},
title = {{Tracking the best hyperplane with a simple budget Perceptron}},
url = {http://link.springer.com/10.1007/s10994-007-5003-0},
volume = {69},
year = {2007}
}
@article{Yanardag2015,
address = {New York, New York, USA},
author = {Yanardag, Pinar and Vishwanathan, S.V.N.},
doi = {10.1145/2783258.2783417},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Yanardag, Vishwanathan - 2015 - Deep Graph Kernels.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
keywords = {bioinformatics,deep learning,graph kernels,r-convolution kernels,social networks,string kernels,structured data},
pages = {1365--1374},
publisher = {ACM Press},
title = {{Deep Graph Kernels}},
url = {http://dl.acm.org/citation.cfm?doid=2783258.2783417},
year = {2015}
}
@article{Chi2013b,
author = {Chi, Lianhua and Li, Bin and Zhu, Xingquan},
doi = {10.1007/978-3-642-37453-1_19},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chi, Li, Zhu - 2013 - Fast graph stream classification using discriminative clique hashing.pdf:pdf},
isbn = {9783642374524},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
keywords = {Cliques,Graph classification,Graph streams,Hashing},
pages = {225--236},
title = {{Fast graph stream classification using discriminative clique hashing}},
volume = {7818},
year = {2013}
}
@inproceedings{Aggarwal2004,
address = {Toronto, Canada},
author = {Aggarwal, C C and Han, J and Wang, J and Yu, P},
booktitle = {Proc. of the 2004 Internl. Conf. on Very Large Data Bases (VLDB'04)},
pages = {852--863},
title = {{A Framework for Projected Clustering of High Dimensional Data Streams}},
year = {2004}
}
@inproceedings{Joachims2000,
address = {Stanford, US},
author = {Joachims, Thorsten},
booktitle = {Proceedings of {\{}ICML{\}}-00, 17th International Conference on Machine Learning},
editor = {Langley, Pat},
pages = {431--438},
publisher = {Morgan Kaufmann Publishers, San Francisco, US},
title = {{Estimating the Generalization Performance of a {\{}SVM{\}} Efficiently}},
year = {2000}
}
@article{Kloft2011,
author = {Kloft, Marius and Blanchard, Gilles},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kloft, Blanchard - 2011 - The Local Rademacher complexity of ell{\_}p-norm multiple kernel learning.pdf:pdf},
isbn = {9781618395993},
journal = {Advances in Neural Information Processing Systems 24},
pages = {2438--2446},
title = {{The Local Rademacher complexity of $\backslash$ell{\_}p-norm multiple kernel learning}},
year = {2011}
}
@article{grossi2011,
author = {Grossi, V and Turini, F},
journal = {Accepted as full paper by Internl. Journ. of Knowl. and Inform. Sys., forthcoming, draft available at www.di.unipi.it/$\backslash${\~{}}{\~{}}vgrossi},
title = {{Stream Mining: a Novel Architecture for Ensemble Based Classification}},
year = {2011}
}
@article{Evdokimov1999,
author = {Evdokimov, Sergei and Karpinski, Marek and Ponomarenko, Ilia},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Evdokimov, Karpinski, Ponomarenko - 1999 - On a new high dimensional Weisfeiler-Lehman algorithm.pdf:pdf},
journal = {Journal of Algebraic {\ldots}},
keywords = {cellular algebra,graph isomorphism problem,permutation group},
number = {1},
pages = {29--45},
title = {{On a new high dimensional Weisfeiler-Lehman algorithm}},
url = {http://link.springer.com/article/10.1023/A:1018672019177},
volume = {10},
year = {1999}
}
@article{Turing1950,
author = {Turing, A. M.},
journal = {Mind},
number = {236},
pages = {433--460},
title = {{Computing Machinery and Intelligence}},
volume = {59},
year = {1950}
}
@article{Carter2001,
abstract = {Currently there is no successful computational approach for identification of genes encoding novel functional RNAs (fRNAs) in genomic sequences. We have developed a machine learning approach using neural networks and support vector machines to extract common features among known RNAs for prediction of new RNA genes in the unannotated regions of prokaryotic and archaeal genomes. The Escherichia coli genome was used for development, but we have applied this method to several other bacterial and archaeal genomes. Networks based on nucleotide composition were 80-90{\%} accurate in jackknife testing experiments for bacteria and 90-99{\%} for hyperthermophilic archaea. We also achieved a significant improvement in accuracy by combining these predictions with those obtained using a second set of parameters consisting of known RNA sequence motifs and the calculated free energy of folding. Several known fRNAs not included in the training datasets were identified as well as several hundred predicted novel RNAs. These studies indicate that there are many unidentified RNAs in simple genomes that can be predicted computationally as a precursor to experimental study. Public access to our RNA gene predictions and an interface for user predictions is available via the web.},
author = {Carter, R J and Dubchak, I and Holbrook, S R},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Computational Biology,Computational Biology: methods,Escherichia coli,Escherichia coli: genetics,Forecasting,Genes, Archaeal,Genes, Bacterial,Genome, Archaeal,Genome, Bacterial,Neural Networks (Computer),RNA, Messenger,RNA, Messenger: genetics,RNA, Untranslated,RNA, Untranslated: genetics},
month = {oct},
number = {19},
pages = {3928--38},
pmid = {11574674},
title = {{A computational approach to identify genes for functional RNAs in genomic sequences.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=60242{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {29},
year = {2001}
}
@article{Forster2001,
author = {Forster, Jurgen and Schmitt, Niels and Simon, Hans Ulrich and Mathematik, Lehrstuhl and Bochum, Ruhr-universitat},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Forster et al. - 2001 - Estimating the Optimal Margins of Embeddings in Euclidean Half Spaces.pdf:pdf},
title = {{Estimating the Optimal Margins of Embeddings in Euclidean Half Spaces}},
year = {2001}
}
@inproceedings{bgf:Lixto,
address = {Rome, Italy},
author = {Baumgartner, Robert and Gottlob, Georg and Flesca, Sergio},
booktitle = {Proceedings of the 27th International Conference on Very Large Databases},
month = {sep},
pages = {119--128},
publisher = {Morgan Kaufmann},
title = {{Visual Information Extraction with {\{}Lixto{\}}}},
year = {2001}
}
@inproceedings{li2008,
address = {Chengdu, China},
author = {Li, P and Hu, X and Wu, X},
booktitle = {Proc. of the 4th Internl. Conf. on Advanced Data Mining and Applications (ADMA'08)},
pages = {733--740},
title = {{Mining Concept-Drifting Data Streams with Multiple Semi-Random Decision Trees}},
year = {2008}
}
@article{Broder,
author = {Broder, Andrei Z},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Broder - Unknown - On the resemblance and containment of documents.pdf:pdf},
journal = {Systems Research},
pages = {1--9},
title = {{On the resemblance and containment of documents}}
}
@article{Bernardi,
author = {Bernardi, Mario Luca and Cimitile, Marta and Maggi, Fabrizio M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bernardi, Cimitile, Maggi - Unknown - Discovering Cross-Organizational Business Rules from the Cloud.pdf:pdf},
title = {{Discovering Cross-Organizational Business Rules from the Cloud}}
}
@inproceedings{NELL-aaai15,
abstract = {Whereas people learn many different types of knowledge from diverse experiences over many years, most current machine learning systems acquire just a single function or data model from just a single data set. We propose a never-ending learning paradigm for machine learning, to better reflect the more ambitious and encompassing type of learning performed by humans. As a case study, we describe the Never-Ending Language Learner (NELL), which achieves some of the desired properties of a never-ending learner, and we discuss lessons learned. NELL has been learning to read the web 24 hours/day since January 2010, and so far has acquired a knowledge base with over 80 million confidence-weighted beliefs (e.g., servedWith(tea, biscuits)). NELL has also learned millions of features and parameters that enable it to read these beliefs from the web. Additionally, it has learned to reason over these beliefs to infer new beliefs, and is able to extend its ontology by synthesizing new relational predicates. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.},
author = {Mitchell, Tom and Cohen, William and Hruschka, Estevam and Talukdar, Partha and Betteridge, Justin and Carlson, Andrew and Mishra, Bhavana Dalvi and Gardner, Matthew and Kisiel, Bryan and Krishnamurthy, Jayant and Lao, Ni and Mazaitis, Kathryn and Mohamed, Thahir and Nakashole, Ndapa and Platanios, Emmanouil and Ritter, Alan and Samadi, Mehdi and Settles, Burr and Wang, Richard and Wijaya, Derry and Gupta, Abhinav and Chen, Xinlei and Saparov, Abulhair and Greaves, Malcolm and Welling, Joel},
booktitle = {AAAI Conference on Artificial Intelligence},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mitchell et al. - 2015 - Never-Ending Learning.pdf:pdf},
keywords = {machine learning,never ending learning,read the web},
title = {{Never-Ending Learning}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10049},
year = {2015}
}
@inproceedings{Rahimi2007,
abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
author = {Rahimi, Ali and Recht, Ben},
booktitle = {Advances in Neural Infomration Processing Systems},
doi = {10.1.1.145.8736},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rahimi, Recht - 2007 - Random Features for Large-Scale Kernel Machines.pdf:pdf},
isbn = {160560352X},
number = {1},
title = {{Random Features for Large-Scale Kernel Machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2007{\_}833.pdf},
year = {2007}
}
@article{Parker2011,
abstract = {Regulatory RNA structures are often members of families with multiple paralogous instances across the genome. Family members share functional and structural properties, which allow them to be studied as a whole, facilitating both bioinformatic and experimental characterization. We have developed a comparative method, EvoFam, for genome-wide identification of families of regulatory RNA structures, based on primary sequence and secondary structure similarity. We apply EvoFam to a 41-way genomic vertebrate alignment. Genome-wide, we identify 220 human, high-confidence families outside protein-coding regions comprising 725 individual structures, including 48 families with known structural RNA elements. Known families identified include both noncoding RNAs, e.g., miRNAs and the recently identified MALAT1/MEN $\beta$ lincRNA family; and cis-regulatory structures, e.g., iron-responsive elements. We also identify tens of new families supported by strong evolutionary evidence and other statistical evidence, such as GO term enrichments. For some of these, detailed analysis has led to the formulation of specific functional hypotheses. Examples include two hypothesized auto-regulatory feedback mechanisms: one involving six long hairpins in the 3'-UTR of MAT2A, a key metabolic gene that produces the primary human methyl donor S-adenosylmethionine; the other involving a tRNA-like structure in the intron of the tRNA maturation gene POP1. We experimentally validate the predicted MAT2A structures. Finally, we identify potential new regulatory networks, including large families of short hairpins enriched in immunity-related genes, e.g., TNF, FOS, and CTLA4, which include known transcript destabilizing elements. Our findings exemplify the diversity of post-transcriptional regulation and provide a resource for further characterization of new regulatory mechanisms and families of noncoding RNAs.},
author = {Parker, Brian J and Moltke, Ida and Roth, Adam and Washietl, Stefan and Wen, Jiayu and Kellis, Manolis and Breaker, Ronald and Pedersen, Jakob Skou},
doi = {10.1101/gr.112516.110},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Parker et al. - 2011 - New families of human regulatory RNA structures identified by comparative analysis of vertebrate genomes.pdf:pdf},
issn = {1549-5469},
journal = {Genome research},
keywords = {3' Untranslated Regions,Animals,Base Sequence,Conserved Sequence,Gene Expression Regulation,Genome,Genomics,Humans,Immunity,Immunity: genetics,Methionine Adenosyltransferase,Methionine Adenosyltransferase: genetics,Molecular Sequence Data,Nucleic Acid Conformation,Phylogeny,Protein Biosynthesis,RNA Editing,RNA Precursors,RNA Precursors: metabolism,RNA Processing, Post-Transcriptional,RNA Stability,RNA, Messenger,RNA, Messenger: metabolism,RNA, Transfer,RNA, Transfer: chemistry,RNA, Transfer: metabolism,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: genetics,Regulatory Sequences, Ribonucleic Acid,Sequence Alignment,Vertebrates,Vertebrates: genetics},
month = {nov},
number = {11},
pages = {1929--43},
pmid = {21994249},
title = {{New families of human regulatory RNA structures identified by comparative analysis of vertebrate genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3205577{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {21},
year = {2011}
}
@article{Tai2007,
abstract = {Motivation: Discriminant analysis for high-dimensional and low sample-sized
data has become a hot research topic in bioinformatics, mainly motivated
by its importance and challenge in applications to tumor classifications
for high-dimensional microarray data. Two of the popular methods
are the nearest shrunken centroids, also called predictive analysis
of microarray {\{}(PAM),{\}} and shrunken centroids regularized discriminant
analysis {\{}(SCRDA).{\}} Both methods are modifications to the classic
linear discriminant analysis {\{}(LDA){\}} in two aspects tailored to high-dimensional
and low-sample-sized data: one is the regularization of the covariance
matrix, and the other is variable selection through shrinkage. In
spite of their usefulness, there are potential limitations with each
method. The main concern is that both {\{}PAM{\}} and {\{}SCRDA{\}} are possibly
too extreme: the covariance matrix in the former is restricted to
be diagonal while in the latter there is barely any restriction.
Based on the biology of gene functions and given the feature of the
data, it may be beneficial to estimate the covariance matrix as an
intermediate between the two; furthermore, more effective shrinkage
schemes may be possible. Results: We propose modified {\{}LDA{\}} methods
to integrate biological knowledge of gene functions (or variable
groups) into classification of microarray data. Instead of simply
treating all the genes independently or imposing no restriction on
the correlations among the genes. we group the genes according to
their biological functions extracted from existing biological knowledge
or data, and propose regularized covariance estimators that encourages
between-group gene independence and within-group gene correlations
while maintaining the flexibility of any general covariance structure.
Furthermore, we propose a shrinkage scheme on groups of genes that
tends to retain or remove a whole group of the genes altogether,
in contrast to the standard shrinkage on individual genes. We show
that one of the proposed methods performed better than {\{}PAM{\}} and
{\{}SCRDA{\}} in a simulation study and several real data examples. Contact:
weip@biostat.umn.edu},
author = {Tai, Feng and Pan, Wei},
doi = {10.1093/bioinformatics/btm488},
journal = {Bioinformatics},
month = {oct},
pages = {1775--1782},
title = {{Incorporating prior knowledge of gene functional groups into regularized discriminant analysis of microarray data}},
volume = {23},
year = {2007}
}
@inproceedings{Suzuki2004,
address = {Morristown, NJ, USA},
author = {Suzuki, Jun and Isozaki, Hideki and Maeda, Eisaku},
booktitle = {ACL '04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
doi = {http://dx.doi.org/10.3115/1218955.1218971},
pages = {119},
publisher = {Association for Computational Linguistics},
title = {{Convolution kernels with feature selection for natural language processing tasks}},
url = {http://portal.acm.org/citation.cfm?doid=1218955.1218971},
year = {2004}
}
@article{Shalev-Shwartz2010,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan and Cotter, Andrew},
doi = {10.1007/s10107-010-0420-4},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shalev-Shwartz et al. - 2010 - Pegasos primal estimated sub-gradient solver for SVM.pdf:pdf},
issn = {0025-5610},
journal = {Mathematical Programming},
month = {oct},
number = {1},
pages = {3--30},
title = {{Pegasos: primal estimated sub-gradient solver for SVM}},
url = {http://link.springer.com/10.1007/s10107-010-0420-4},
volume = {127},
year = {2010}
}
@inproceedings{levesque:belief,
address = {Austin, Texas},
author = {Levesque, Hector{\~{}}J.},
booktitle = {Proceedings of the Fourth National Conference on Artificial Intelligence},
month = {aug},
pages = {198--202},
publisher = {American Association for Artificial Intelligence},
title = {{A logic of implicit and explicit belief}},
year = {1984}
}
@inproceedings{Moschitti2007,
address = {New York, NY, USA},
author = {Moschitti, Alessandro and Zanzotto, Fabio Massimo},
booktitle = {ICML '07: Proceedings of the 24th international conference on Machine learning},
doi = {http://doi.acm.org/10.1145/1273496.1273578},
isbn = {978-1-59593-793-3},
pages = {649--656},
publisher = {ACM},
title = {{Fast and effective kernels for relational learning from texts}},
year = {2007}
}
@article{Emmert-Streib2011,
abstract = {The purpose of this study is to survey the use of networks and network-based methods in systems biology. This study starts with an introduction to graph theory and basic measures allowing to quantify structural properties of networks. Then, the authors present important network classes and gene networks as well as methods for their analysis. In the last part of this study, the authors review approaches that aim at analysing the functional organisation of gene networks and the use of networks in medicine. In addition to this, the authors advocate networks as a systematic approach to general problems in systems biology, because networks are capable of assuming multiple roles that are very beneficial connecting experimental data with a functional interpretation in biological terms.},
author = {Emmert-Streib, F and Dehmer, M},
doi = {10.1049/iet-syb.2010.0025},
issn = {1751-8849},
journal = {IET systems biology},
keywords = {Animals,Artificial Intelligence,Gene Regulatory Networks,Humans,Information Theory,Mathematical Concepts,Metabolic Networks and Pathways,Models, Biological,Models, Genetic,Systems Biology},
month = {may},
number = {3},
pages = {185--207},
pmid = {21639592},
title = {{Networks for systems biology: conceptual connection of data and function.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21639592},
volume = {5},
year = {2011}
}
@article{Strauss2010,
author = {Strauss, Martin J},
doi = {10.1145/1862919.1862923},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Strauss - 2010 - Space-Optimal Heavy Hitters with Strong Error Bounds.pdf:pdf},
number = {4},
pages = {1--28},
title = {{Space-Optimal Heavy Hitters with Strong Error Bounds}},
volume = {35},
year = {2010}
}
@inproceedings{Perozzi2014,
address = {New York, New York, USA},
author = {Perozzi, Bryan and Al-Rfou, Rami and Skiena, Steven},
booktitle = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
doi = {10.1145/2623330.2623732},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Perozzi, Al-Rfou, Skiena - 2014 - DeepWalk online learning of social representations.pdf:pdf},
isbn = {9781450329569},
keywords = {deep learning,latent representations,learning with partial labels,network classification,online learning,social networks},
pages = {701--710},
publisher = {ACM Press},
title = {{DeepWalk: online learning of social representations}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2623732},
year = {2014}
}
@article{Hastie2004,
abstract = {The support vector machine (SVM) is a widely used tool for classification. Many efficient implementations exist for fitting a two-class SVM model. The user has to supply values for the tuning parameters: the regularization cost parameter, and the kernel parameters. It seems a common practice is to use a default value for the cost parameter, often leading to the least restrictive model. In this paper we argue that the choice of the cost parameter can be critical. We then derive an algorithm that can fit the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as fitting one SVM model. We illustrate our algorithm on some examples, and use our representation to give further insight into the range of SVM solutions.},
author = {Hastie, Trevor and Rosset, Saharon and Tibshirani, Robert and Zhu, Ji},
doi = {10.1145/347090.347165},
isbn = {1532-4435},
issn = {15324435},
journal = {JMLR},
keywords = {coefficient path,regularization,support vector machines},
number = {2},
pages = {1391--1415},
pmid = {17354896},
title = {{The Entire Regularization Path for the Support Vector Machine}},
url = {http://portal.acm.org/citation.cfm?id=1005332.1044706},
volume = {5},
year = {2004}
}
@inproceedings{Heinonen2009,
author = {Heinonen, Markus and Rousu, Juho and V{\"{a}}lim{\"{a}}ki, N and M{\"{a}}kinen, V},
booktitle = {BIOINFORMATICS 2012 - Proceedings of the International Conference on Bioinformatics Models, Methods and Algorithms},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Heinonen et al. - 2012 - Efficient Path Kernels for Reaction Function Prediction.pdf:pdf},
pages = {202--207},
title = {{Efficient Path Kernels for Reaction Function Prediction}},
url = {http://eprints.pascal-network.org/archive/00008592/},
year = {2012}
}
@article{Malik2000,
author = {Malik, J.},
doi = {10.1109/34.868688},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Malik - 2000 - Normalized cuts and image segmentation.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {888--905},
title = {{Normalized cuts and image segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=868688},
volume = {22},
year = {2000}
}
@inproceedings{Schlimmer86,
address = {Menlo Park, CA},
author = {Schlimmer, J C and Granger, R H},
booktitle = {Proc. of the 5th National Conf. on Artificial Intelligence},
pages = {502--507},
title = {{Beyond Incremental Processing: Tracking Concept Drift}},
year = {1986}
}
@inproceedings{Pighin2009,
abstract = {The combination of Support Vector Machines with very high dimensional
kernels, such as string or tree kernels, suffers from two major drawbacks:
first, the implicit representation of feature spaces does not allow
us to understand which features actually triggered the generalization;
second, the resulting computational burden may in some cases render
unfeasible to use large data sets for training. We propose an approach
based on feature space reverse engineering to tackle both problems.
Our experiments with Tree Kernels on a Semantic Role Labeling data
set show that the proposed approach can drastically reduce the computational
footprint while yielding almost unaffected accuracy.},
address = {Boulder, Colorado},
author = {Pighin, Daniele and Moschitti, Alessandro},
booktitle = {Proceedings of the Thirteenth Conference on Computational Natural Language Learning},
isbn = {978-1-932432-29-9},
keywords = {fast{\_}tree{\_}kernel,feature{\_}mining,kernel{\_}for{\_}structured{\_}data,moschitti,non adaptive feature spaces,tree kernel},
pages = {30--38},
publisher = {Association for Computational Linguistics},
title = {{Efficient linearization of tree kernel functions}},
year = {2009}
}
@book{han06,
author = {Han, J and Kamber, M},
publisher = {Morgan Kaufmann, San Francisco, CA},
title = {{Data Mining Concepts and Techniques, 2{\^{}}{\{}nd{\}} edition}},
year = {2006}
}
@article{Li2012,
author = {Li, Bin and Zhu, Xingquan and Chi, Lianhua and Zhang, Chengqi},
doi = {10.1109/ICDM.2012.101},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2012 - Nested Subtree Hash Kernels for Large-Scale Graph Classification over Streams.pdf:pdf},
isbn = {978-1-4673-4649-8},
journal = {2012 IEEE 12th International Conference on Data Mining},
keywords = {-graph classification,data stream mining,graph,hash kernels,nested subtree hashing},
month = {dec},
pages = {399--408},
publisher = {Ieee},
title = {{Nested Subtree Hash Kernels for Large-Scale Graph Classification over Streams}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6413884},
year = {2012}
}
@article{10.1109/ICDM.2009.30,
address = {Los Alamitos, CA, USA},
annote = {        From Duplicate 2 (                   A Linear-Time Graph Kernel                 - Hido, Shohei; Kashima, Hisashi )
                
        
        
      },
author = {Hido, Shohei and Kashima, Hisashi},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICDM.2009.30},
editor = {Wang, Wei and Kargupta, Hillol and Ranka, Sanjay and Yu, Philip S and Wu, Xindong},
isbn = {978-0-7695-3895-2},
issn = {1550-4786},
journal = {ICDM},
pages = {179--188},
publisher = {IEEE Computer Society},
title = {{A Linear-Time Graph Kernel}},
volume = {0},
year = {2009}
}
@inproceedings{Hiroshi2006,
abstract = {This paper proposes an efficient method of sentence retrieval based
on syntactic structure. Collins proposed Tree Kernel to calculate
structural similarity. However, structual retrieval based on Tree
Kernel is not practicable because the size of the index table by
Tree Kernel becomes impractical. We propose more efficient algorithms
approximating Tree Kernel: Tree Overlapping and Subpath Set. These
algorithms are more efficient than Tree Kernel because indexing is
possible with practical computation resources. The results of the
experiments comparing these three algorithms showed that structural
retrieval with Tree Overlapping and Subpath Set were faster than
that with Tree Kernel by 100 times and 1,000 times respectively.},
address = {Stroudsburg, PA, USA},
annote = {From Duplicate 1 ( 


Efficient sentence retrieval based on syntactic structure


- Hiroshi, Ichikawa; Keita, Hakoda; Taiichi, Hashimoto; Takenobu, Tokunaga )

},
author = {Hiroshi, Ichikawa and Keita, Hakoda and Taiichi, Hashimoto and Takenobu, Tokunaga},
booktitle = {Proceedings of the COLING/ACL on Main conference poster sessions},
number = {July},
pages = {399--406},
publisher = {Association for Computational Linguistics},
series = {COLING-ACL '06},
title = {{Efficient sentence retrieval based on syntactic structure}},
year = {2006}
}
@article{Saigo2008,
abstract = {Graph mining methods enumerate frequently appearing subgraph patterns,
which can be used as features for subsequent classification or regression.
However, frequent patterns are not necessarily informative for the
given learning problem. We propose a mathematical programming boosting
method (gBoost) that progressively collects informative patterns.
Compared to AdaBoost, gBoost can build the prediction rule with fewer
iterations. To apply the boosting method to graph data, a branch-and-bound
pattern search algorithm is developed based on the DFS code tree.
The constructed search space is reused in later iterations to minimize
the computation time. Our method can learn more efficiently than
the simpler method based on frequent substructure mining, because
the output labels are used as an extra information source for pruning
the search space. Furthermore, by engineering the mathematical program,
a wide range of machine learning problems can be solved without modifying
the pattern search algorithm.},
annote = {
        From Duplicate 1 ( 
        
        
          gBoost: a mathematical programming approach to graph classification and regression
        
        
         - Saigo, Hiroto; Nowozin, Sebastian; Kadowaki, Tadashi; Kudo, Taku; Tsuda, Koji )

        
        

        From Duplicate 2 ( 
        
        
          gBoost: a mathematical programming approach to graph classification and regression
        
        
         - Saigo, Hiroto; Nowozin, Sebastian; Kadowaki, Tadashi; Kudo, Taku; Tsuda, Koji )

        
        

        

        

        

        

      },
author = {Saigo, Hiroto and Nowozin, Sebastian and Kadowaki, Tadashi and Kudo, Taku and Tsuda, Koji},
doi = {10.1007/s10994-008-5089-z},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {classification,graph mining,mathematical programming,regression},
month = {nov},
number = {1},
pages = {69--89},
title = {{gBoost: a mathematical programming approach to graph classification and regression}},
url = {http://www.springerlink.com/index/10.1007/s10994-008-5089-z},
volume = {75},
year = {2008}
}
@article{Conte:fk,
author = {Conte, Donatello and Foggia, Pasquale and Vento, Mario"},
doi = {10.1.1.143.5654},
title = {{Challenging Complexity of Maximum Common Subgraph Detection Algorithms: A Performance Analysis of Three Algorithms on a Wide Database of Graphs}}
}
@inproceedings{NIPS2009_0533,
annote = {From Duplicate 1 ( 


Fast subtree kernels on graphs


- Shervashidze, Nino; Borgwardt, Karsten )




From Duplicate 2 ( 


Fast subtree kernels on graphs


- Shervashidze, Nino; Borgwardt, Karsten )












From Duplicate 2 ( 


Fast subtree kernels on graphs


- Shervashidze, Nino; Borgwardt, Karsten )

},
author = {Shervashidze, Nino and Borgwardt, Karsten},
booktitle = {NIPS},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shervashidze, Borgwardt - 2009 - Fast subtree kernels on graphs.pdf:pdf},
pages = {1660--1668},
title = {{Fast subtree kernels on graphs}},
year = {2009}
}
@unpublished{Hagenbuchner,
author = {Hagenbuchner, Markus},
title = {{Report of SOM-SD training for XML challenge 2}}
}
@article{Frasconi,
archivePrefix = {arXiv},
arxivId = {arXiv:1205.3981v3},
author = {Frasconi, Paolo and Costa, Fabrizio and Raedt, Luc De and Grave, Kurt De},
eprint = {arXiv:1205.3981v3},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Frasconi et al. - Unknown - kLog A Language for Logical and Relational Learning with Kernels.pdf:pdf},
title = {{kLog: A Language for Logical and Relational Learning with Kernels}}
}
@inproceedings{Borgwardt2007b,
abstract = {It is widely believed that comparing discrepancies in the protein-protein
interaction (PPI) networks of individuals will become an important
tool in understanding and preventing diseases. Currently PPI networks
for individuals are not available, but gene expression data is becoming
easier to obtain and allows us to represent individuals by a co-integrated
gene expression/protein interaction network. Two major problems hamper
the application of graph kernels$\backslash$,---$\backslash$,state-of-the-art methods for
whole-graph comparison$\backslash$,---$\backslash$,to compare PPI networks. First, these
methods do not scale to graphs of the size of a PPI network. Second,
missing edges in these interaction networks are biologically relevant
for detecting discrepancies, yet, these methods do not take this
into account. In this article we present graph kernels for biological
network comparison that are fast to compute and take into account
missing interactions. We evaluate their practical performance on
two datasets of co-integrated gene expression/PPI networks.},
address = {Maui, Hawaii},
annote = {From Duplicate 2 ( Graph kernels for disease outcome prediction from protein-protein interaction networks. - Borgwardt, Karsten M; Kriegel, Hans-Peter; Vishwanathan, S V N; Schraudolph, Nicol N )
},
author = {Borgwardt, Karsten M and Kriegel, Hans-Peter and Vishwanathan, S.{\~{}}V.{\~{}}N. and Schraudolphl, Nicol N},
booktitle = {Proc. Pacific Symposium on Biocomputing (PSB)},
isbn = {978-981-270-417-7},
issn = {1793-5091},
keywords = {Computational Biology,Databases,Disease Progression,Gene Expression Profiling,Gene Expression Profiling: statistics {\&} numerical,Genetic,Humans,Prognosis,Protein Array Analysis,Protein Array Analysis: statistics {\&} numerical dat,Protein Interaction Mapping,Protein Interaction Mapping: statistics {\&} numerica},
month = {jan},
pages = {4--15},
pmid = {17992741},
publisher = {World Scientific},
title = {{Graph Kernels for Disease Outcome Prediction from Protein-Protein Interaction Networks}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17992741},
volume = {12},
year = {2007}
}
@inproceedings{Domingos2000,
address = {Boston. MA},
author = {Domingos, P and Hulten, G},
booktitle = {Proc. of the 6th Internl. Conf. on Knowl. Disc. and Data Mining (KDD'00)},
pages = {71--80},
title = {{Mining High-Speed Data Streams}},
year = {2000}
}
@inproceedings{DaSanMartino2014,
author = {{Da San Martino}, Giovanni and Navarin, Nicol{\`{o}} and Sperduti, Alessandro},
booktitle = {Neural Information Processing},
doi = {10.1007/978-3-319-12640-1_12},
editor = {{Loo, ChuKiong and Yap, KeemSiah and Wong, KokWai and Teoh, Andrew and Huang}, Kaizhu},
pages = {93--100},
publisher = {Springer International Publishing},
title = {{Graph Kernels Exploiting Weisfeiler-Lehman Graph Isomorphism Test Extensions}},
url = {http://link.springer.com/10.1007/978-3-319-12640-1{\_}12},
year = {2014}
}
@inproceedings{Teo2006,
abstract = {String kernels which compare the set of all common substrings between
two given strings have recently been proposed by Vishwanathan {\&}
Smola (2004). Surprisingly, these kernels can be computed in linear
time and linear space using annotated suffix trees. Even though,
in theory, the suffix tree based algorithm requires O(n) space for
an n length string, in practice at least 40n bytes are required --
20n bytes for storing the suffix tree, and an additional 20n bytes
for the annotation. This large memory requirement coupled with poor
locality of memory access, inherent due to the use of suffix trees,
means that the performance of the suffix tree based algorithm deteriorates
on large strings. In this paper, we describe a new linear time yet
space efficient and scalable algorithm for computing string kernels,
based on suffix arrays. Our algorithm is a) faster and easier to
implement, b) on the average requires only 19n bytes of storage,
and c) exhibits strong locality of memory access. We show that our
algorithm can be extended to perform linear time prediction on a
test string, and present experiments to validate our claims.},
address = {Pittsburgh, Pennsylvania},
author = {Teo, Choon Hui and Vishwanathan, S V N},
booktitle = {Proceedings of the 23rd international conference on Machine learning},
doi = {10.1145/1143844.1143961},
isbn = {1-59593-383-2},
pages = {929--936},
publisher = {ACM},
title = {{Fast and space efficient string kernels using suffix arrays}},
year = {2006}
}
@book{Wahba00,
address = {Philadelphia, PA},
author = {Wahba, G},
keywords = {book,reference,regression,smooth,splines},
publisher = {Society for Industrial and Applied Mathematics (SIAM)},
series = {CBMS-NSF Regional Conference Series in Applied Mathematics},
title = {{Spline models for observational data}},
url = {http://www.ams.org/mathscinet-getitem?mr=1045442},
volume = {59},
year = {1990}
}
@article{Jawanpuria2015,
author = {Jawanpuria, Pratik and Nath, Jagarlapudi Saketha and Ramakrishnan, Ganesh},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Jawanpuria, Nath, Ramakrishnan - 2015 - Generalized Hierarchical Kernel Learning.pdf:pdf},
keywords = {active set method,ensemble learning,mixed-norm regularization,multi-task learning,multiple kernel learning,rule},
pages = {617--652},
title = {{Generalized Hierarchical Kernel Learning}},
volume = {16},
year = {2015}
}
@article{Tyomkyn2011,
author = {Tyomkyn, Mykhaylo},
doi = {10.1002/jgt},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Tyomkyn - 2011 - Walks and Paths in Trees ´.pdf:pdf},
journal = {Journal of Graph Theory},
keywords = {paths,trees,walks},
pages = {54--66},
title = {{Walks and Paths in Trees ´}},
volume = {0076},
year = {2011}
}
@article{Siolas2000,
address = {Los Alamitos, CA, USA},
author = {Siolas, Georges and D'Alche-Buc, Florence},
doi = {http://doi.ieeecomputersociety.org/10.1109/IJCNN.2000.861458},
issn = {1098-7576},
journal = {Neural Networks, IEEE - INNS - ENNS International Joint Conference on},
pages = {5205},
publisher = {IEEE Computer Society},
title = {{Support Vector Machines Based on a Semantic Kernel for Text Categorization}},
volume = {5},
year = {2000}
}
@inproceedings{Mahe2004,
address = {New York, New York, USA},
author = {Mah{\'{e}}, Pierre and Ueda, Nobuhisa and Akutsu, Tatsuya and Perret, Jean-Luc and Vert, Jean-Philippe},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015446},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mah{\'{e}} et al. - 2004 - Extensions of marginalized graph kernels(2).pdf:pdf},
isbn = {1581138285},
pages = {70},
publisher = {ACM Press},
title = {{Extensions of marginalized graph kernels}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015446},
year = {2004}
}
@article{Asai2002,
author = {Asai, T and Arimura, H and Abe, K and Kawasoe, S and Arikawa, S},
journal = {Data Mining, IEEE Internl. Conf. on},
pages = {27},
publisher = {IEEE Computer Society},
title = {{Online Algorithms for Mining Semi-structured Data Stream}},
year = {2002}
}
@article{LiuPWP09,
author = {Liu, Weifeng and Park, Il and Wang, Yiwen and Pr{\'{i}}ncipe, Jos{\'{e}} C},
journal = {IEEE Transactions on Signal Processing},
number = {10},
pages = {3801--3814},
title = {{Extended kernel recursive least squares algorithm}},
volume = {57},
year = {2009}
}
@article{Conte:2004uq,
author = {Conte, D and Foggia, P and Sansone, C and Vento, M},
doi = {10.1142/S0218001404003228},
title = {{International Journal of Pattern Recognition and Artificial Intelligence; Thirty Years Of Graph Matching In Pattern Recognition}},
year = {2004}
}
@article{Mordelet2008,
abstract = {Living cells are the product of gene expression programs that involve
the regulated transcription of thousands of genes. The elucidation
of transcriptional regulatory networks in thus needed to understand
the cell's working mechanism, and can for example be useful for the
discovery of novel therapeutic targets. Although several methods
have been proposed to infer gene regulatory networks from gene expression
data, a recent comparison on a large-scale benchmark experiment revealed
that most current methods only predict a limited number of known
regulations at a reasonable precision level. We propose {\{}SIRENE,{\}}
a new method for the inference of gene regulatory networks from a
compendium of expression data. The method decomposes the problem
of gene regulatory network inference into a large number of local
binary classification problems, that focus on separating target genes
from non-targets for each {\{}TF.{\}} {\{}SIRENE{\}} is thus conceptually simple
and computationally efficient. We test it on a benchmark experiment
aimed at predicting regulations in E. coli, and show that it retrieves
of the order of 6 times more known regulations than other state-of-the-art
inference methods.},
author = {Mordelet, Fantine and Vert, Jean-Philippe},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mordelet, Vert - 2008 - SIRENE Supervised Inference of Regulatory Networks.pdf:pdf},
issn = {1460-2059},
journal = {Bioinformatics (Oxford, England)},
month = {aug},
number = {16},
pages = {76--82},
shorttitle = {SIRENE},
title = {{SIRENE: Supervised Inference of Regulatory Networks}},
volume = {24},
year = {2008}
}
@techreport{Gantz2012,
author = {Gantz, By John and Reinsel, David and Shadows, Bigger Digital},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gantz, Reinsel, Shadows - 2012 - THE DIGITAL UNIVERSE IN 2020 Big Data , Bigger Digital Shadow s , and Biggest Growth in the Far East E.pdf:pdf},
institution = {IDC},
pages = {1--16},
title = {{THE DIGITAL UNIVERSE IN 2020 : Big Data , Bigger Digital Shadow s , and Biggest Growth in the Far East Executive Summary: A Universe of Opportunities and Challenges}},
url = {http://www.emc.com/leadership/digital-universe/iview/big-data-2020.htm},
volume = {2007},
year = {2012}
}
@incollection{Huang2007,
abstract = {We often meet the tree decomposition task in the tree kernel computing.
And tree decomposition tends to vary under different tree mapping
constraint. In this paper, we first introduce the general tree decomposition
function, and compare the three variants of the function corresponding
to different tree mapping. Then we will give a framework to generalize
the kernels based on tree-to-tree decomposition with the decomposition
function.},
annote = {10.1007/978-3-540-72393-6{\_}71},
author = {Huang, Peng and Zhu, Jie},
booktitle = {Advances in Neural Networks ISNN 2007},
editor = {Liu, Derong and Fei, Shumin and Hou, Zengguang and Zhang, Huaguang and Sun, Changyin},
pages = {593--601},
publisher = {Springer Berlin / Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Decomposition Method for Tree Kernels}},
volume = {4492},
year = {2007}
}
@incollection{chen06,
author = {Chen, Y W and Lin, C J},
booktitle = {Feature extraction, foundations and applications},
publisher = {Springer},
title = {{Combining SVMs with various feature selection strategies}},
year = {2006}
}
@inproceedings{Flouri2009,
address = {Piscataway, NJ, USA},
author = {Flouri, K and Beferull-Lozano, B and Tsakalides, P},
booktitle = {Proceedings of the 16th international conference on Digital Signal Processing},
isbn = {978-1-4244-3297-4},
keywords = {SVMs,consensus,convex optimization,gossip algorithms,wireless sensor networks distributed{\_}svm},
pages = {886--891},
publisher = {IEEE Press},
series = {DSP'09},
title = {{Optimal gossip algorithm for distributed consensus SVM training in wireless sensor networks}},
year = {2009}
}
@inproceedings{Domingos2001,
address = {Williamstown, MA},
author = {Domingos, P and Hulten, G},
booktitle = {Proc. of the 18th Internl. Conf. on Machine Learning (ICML'01)},
pages = {106--113},
title = {{A General Method for Scaling Up Machine Learning Algorithms and its Application to Clustering}},
year = {2001}
}
@book{Borgwardt:2007uq,
author = {Borgwardt, K M},
title = {{Graph kernels}},
year = {2007}
}
@article{springerlink:10.1007/s10115-007-0103-5,
annote = {10.1007/s10115-007-0103-5},
author = {Wale, Nikil and Watson, Ian and Karypis, George},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wale, Watson, Karypis - 2008 - Comparison of descriptor spaces for chemical compound retrieval and classification.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
number = {3},
pages = {347--375},
publisher = {Springer London},
title = {{Comparison of descriptor spaces for chemical compound retrieval and classification}},
volume = {14},
year = {2008}
}
@inproceedings{Kersting2013,
abstract = {Color refinement is a basic algorithmic routine for graph isomorphism testing and has recently been used for computing graph kernels as well as for lifting belief propagation and linear programming. So far, color refinement has been treated as a combinatorial problem. Instead, we treat it as a nonlinear continuous optimization problem and prove that it implements a conditional gradient optimizer that can be turned into graph clustering approaches using hashing and truncated power iterations. This shows that color refinement is easy to understand in terms of random walks, easy to implement (matrix-matrix/vector multiplications) and readily parallelizable. We support our theoretical results with experiments on real-world graphs with millions of edges. Copyright {\textcopyright} 2014, Association for the Advancement of Artificial Intelligence.},
author = {Kersting, Kristian and Mladenov, Martin and Garnett, Roman and Grohe, Martin},
booktitle = {28th AAAI Conference on Artificial Intelligence},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kersting et al. - 2013 - Power Iterated Color Refinement.pdf:pdf},
keywords = {Novel Machine Learning Algorithms},
pages = {1904--1910},
title = {{Power Iterated Color Refinement}},
year = {2013}
}
@article{Yanardag2015a,
author = {Yanardag, Pinar},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Yanardag - 2015 - A Structural Smoothing Framework For Robust.pdf:pdf},
journal = {Nips},
pages = {1--9},
title = {{A Structural Smoothing Framework For Robust}},
year = {2015}
}
@article{Anguita2012,
abstract = {In-sample approaches to model selection and error estimation of support vector machines (SVMs) are not as widespread as out-of-sample methods, where part of the data is removed from the training set for validation and testing purposes, mainly because their practical application is not straightforward and the latter provide, in many cases, satisfactory results. In this paper, we survey some recent and not-so-recent results of the data-dependent structural risk minimization framework and propose a proper reformulation of the SVM learning algorithm, so that the in-sample approach can be effectively applied. The experiments, performed both on simulated and real-world datasets, show that our in-sample approach can be favorably compared to out-of-sample methods, especially in cases where the latter ones provide questionable results. In particular, when the number of samples is small compared to their dimensionality, like in classification of microarray data, our proposal can outperform conventional out-of-sample approaches such as the cross validation, the leave-one-out, or the Bootstrap methods.},
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
doi = {10.1109/TNNLS.2012.2202401},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Anguita et al. - 2012 - In-sample and out-of-sample model selection and error estimation for support vector machines.pdf:pdf},
issn = {2162-237X},
journal = {IEEE transactions on neural networks and learning systems},
month = {sep},
number = {9},
pages = {1390--406},
pmid = {24807923},
title = {{In-sample and out-of-sample model selection and error estimation for support vector machines.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24807923},
volume = {23},
year = {2012}
}
@article{Wang2009,
author = {Wang, Zhuang and Vucetic, S},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Vucetic - 2009 - Twin Vector Machines for Online Learning on a Budget.pdf:pdf},
journal = {SDM},
pages = {906--917},
title = {{Twin Vector Machines for Online Learning on a Budget.}},
url = {https://siam.org/proceedings/datamining/2009/dm09{\_}083{\_}wangz.pdf},
year = {2009}
}
@inproceedings{Leslie2002,
author = {Leslie, Christina S and Eskin, Eleazar and Noble, William Stafford},
booktitle = {Pacific Symposium on Biocomputing},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Leslie, Eskin, Noble - 2002 - The Spectrum Kernel A String Kernel for SVM Protein Classification.pdf:pdf},
pages = {566--575},
title = {{The Spectrum Kernel: A String Kernel for SVM Protein Classification}},
year = {2002}
}
@article{Shin2011a,
author = {Shin, Kilho},
doi = {10.1109/ICDM.2011.115},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shin - 2011 - Partitionable Kernels for Mapping Kernels.pdf:pdf},
isbn = {978-1-4577-2075-8},
journal = {2011 IEEE 11th International Conference on Data Mining},
month = {dec},
pages = {645--654},
publisher = {Ieee},
title = {{Partitionable Kernels for Mapping Kernels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6137269},
year = {2011}
}
@article{Babai2015,
archivePrefix = {arXiv},
arxivId = {1512.03547},
author = {Babai, Laszlo},
eprint = {1512.03547},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Babai - 2015 - Graph Isomorphism in Quasiolynomial Time.pdf:pdf},
pages = {1--84},
title = {{Graph Isomorphism in Quasiolynomial Time}},
url = {http://people.cs.uchicago.edu/{~}laci/quasipoly.html},
volume = {7443327},
year = {2015}
}
@article{Eddy2001,
author = {Eddy, Sean R and Hughes, Howard},
journal = {Genetics},
number = {December},
title = {{NON-CODING RNA GENES AND THE MODERN WORLD}},
volume = {2},
year = {2001}
}
@inproceedings{Zhang2006,
address = {Amsterdam, The Netherlands, The Netherlands},
author = {Zhang, Shichao and Qin, Yongsong and Zhu, Xiaofeng and Zhang, Jilian and Zhang, Chengqi},
booktitle = {Proceeding of the 2006 conference on Advances in Intelligent IT},
isbn = {1-58603-615-7},
pages = {106--111},
publisher = {IOS Press},
title = {{Kernel-Based Multi-Imputation for Missing Data}},
year = {2006}
}
@inproceedings{MOA,
address = {$\backslash$mbox{\{}$\backslash$tt http://www.cs.waikato.ac.nz/ml/moa{\}}},
author = {{The University of Waikato}},
title = {{MOA: Massive Online Analysis, August 2009}},
year = {2009}
}
@inproceedings{Moschittia,
abstract = {In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments. Their main property is the ability to process structured representations. Support Vector Machines (SVMs), using a combination of such kernels and the flat feature kernel, classify Prop-Bank predicate arguments with accuracy higher than the current argument classification state-of-the-art.Additionally, experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement.},
address = {Morristown, NJ, USA},
annote = {        From Duplicate 1 (                   A study on convolution kernels for shallow semantic parsing                 - Moschitti, Alessandro )
                
        From Duplicate 1 (                   Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees                 - Moschitti, Alessandro )
                
        
        
        
        
      },
author = {Moschitti, Alessandro},
booktitle = {Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics - ACL '04},
doi = {10.3115/1218955.1218998},
editor = {F{\"{u}}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
isbn = {3-540-45375-X},
month = {sep},
pages = {335--es},
publisher = {Association for Computational Linguistics},
series = {Lecture Notes in Computer Science},
title = {{A study on convolution kernels for shallow semantic parsing}},
url = {http://portal.acm.org/citation.cfm?doid=1218955.1218998},
volume = {4212},
year = {2004}
}
@article{Hayes2013,
abstract = {MOTIVATION: Large amounts of biological network data exist for many species. Analogous to sequence comparison, network comparison aims to provide biological insight. Graphlet-based methods are proving to be useful in this respect. Recently some doubt has arisen concerning the applicability of graphlet-based measures to low edge density networks-in particular that the methods are 'unstable'-and further that no existing network model matches the structure found in real biological networks.$\backslash$n$\backslash$nRESULTS: We demonstrate that it is the model networks themselves that are 'unstable' at low edge density and that graphlet-based measures correctly reflect this instability. Furthermore, while model network topology is unstable at low edge density, biological network topology is stable. In particular, one must distinguish between average density and local density. While model networks of low average edge densities also have low local edge density, that is not the case with protein-protein interaction (PPI) networks: real PPI networks have low average edge density, but high local edge densities, and hence, they (and thus graphlet-based measures) are stable on these networks. Finally, we use a recently devised non-parametric statistical test to demonstrate that PPI networks of many species are well-fit by several models not previously tested. In addition, we model several viral PPI networks for the first time and demonstrate an exceptionally good fit between the data and theoretical models.},
author = {Hayes, Wayne and Sun, Kai and Pr{\v{z}}ulj, Nata{\v{s}}a},
doi = {10.1093/bioinformatics/bts729},
isbn = {1367-4811 (Electronic)$\backslash$n1367-4803 (Linking)},
issn = {13674803},
journal = {Bioinformatics},
number = {4},
pmid = {23349212},
title = {{Graphlet-based measures are suitable for biological network comparison}},
volume = {29},
year = {2013}
}
@article{Miyazaki1996,
author = {Miyazaki, Takunari},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Miyazaki - 1997 - The complexity of McKay's canonical labeling algorithm.pdf:pdf},
journal = {Groups and Computation II},
title = {{The complexity of McKay's canonical labeling algorithm}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=lyL8Ui36FKsC{\&}oi=fnd{\&}pg=PA239{\&}dq=The+complexity+of+McKay's+canonical+labeling+algorithm{\&}ots=in8tFybmM6{\&}sig=TqrPYth1teyCloq38y6IPDvdrlI},
year = {1997}
}
@article{Tsuda2005,
abstract = {Motivation: Support vector machines (SVMs) have been successfully
used to classify proteins into functional categories. Recently, to
integrate multiple data sources, a semidefinite programming (SDP)
based SVM method was introduced. In SDP/SVM, multiple kernel matrices
corresponding to each of data sources are combined with weights obtained
by solving an SDP. However, when trying to apply SDP/SVM to large
problems, the computational cost can become prohibitive, since both
converting the data to a kernel matrix for the SVM and solving the
SDP are time and memory demanding. Another application-specific drawback
arises when some of the data sources are protein networks. A common
method of converting the network to a kernel matrix is the diffusion
kernel method, which has time complexity of O(n3), and produces a
dense matrix of size n x n.Results: We propose an efficient method
of protein classification using multiple protein networks. Available
protein networks, such as a physical interaction network or a metabolic
network, can be directly incorporated. Vectorial data can also be
incorporated after conversion into a network by means of neighbor
point connection. Similar to the SDP/SVM method, the combination
weights are obtained by convex optimization. Due to the sparsity
of network edges, the computation time is nearly linear in the number
of edges of the combined network. Additionally, the combination weights
provide information useful for discarding noisy or irrelevant networks.
Experiments on function prediction of 3588 yeast proteins show promising
results: the computation time is enormously reduced, while the accuracy
is still comparable to the SDP/SVM method.Availability: Software
and data will be available on request.},
author = {Tsuda, Koji and Shin, HyunJung and Scholkopf, Bernhard},
doi = {10.1093/bioinformatics/bti1110},
journal = {Bioinformatics},
number = {suppl 2},
pages = {59--65},
title = {{Fast protein classification with multiple networks}},
volume = {21},
year = {2005}
}
@article{Oneto,
author = {Oneto, Luca and Ghio, Alessandro and Anguita, Davide and Ridella, Sandro},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Oneto et al. - Unknown - Fast Convergence of Extended Rademacher Complexity Bounds for Binary Classifiers.pdf:pdf},
journal = {Pattern Recognition Letters},
title = {{Fast Convergence of Extended Rademacher Complexity Bounds for Binary Classifiers}}
}
@inproceedings{Moschitti2004,
abstract = {In this paper we have designed and experimented novel convolution
kernels for automatic classification of predicate arguments. Their
main property is the ability to process structured representations.
Support Vector Machines (SVMs), using a combination of such kernels
and the flat feature kernel, classify Prop-Bank predicate arguments
with accuracy higher than the current argument classification state-of-the-art.Additionally,
experiments on FrameNet data have shown that SVMs are appealing for
the classification of semantic roles even if the proposed kernels
do not produce any improvement.},
address = {Morristown, NJ, USA},
author = {Moschitti, Alessandro},
booktitle = {ACL '04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
doi = {http://dx.doi.org/10.3115/1218955.1218998},
pages = {335},
publisher = {Association for Computational Linguistics},
title = {{A study on convolution kernels for shallow semantic parsing}},
year = {2004}
}
@article{Hammer2005,
author = {Hammer, Barbara and Saunders, Craig and Sperduti, Alessandro},
doi = {10.1016/j.neunet.2005.07.004},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hammer, Saunders, Sperduti - 2005 - Special issue on neural networks and kernel methods for structured domains.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Animals,Computer Simulation,Humans,Neural Networks (Computer),Pattern Recognition, Automated},
month = {oct},
number = {8},
pages = {1015--8},
pmid = {16182511},
title = {{Special issue on neural networks and kernel methods for structured domains.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16182511},
volume = {18},
year = {2005}
}
@inproceedings{Villa2007,
abstract = {Flexible and efficient variants of the Self Organizing Map algorithm
have been proposed for non vector data, including, for example, the
dissimilarity {\{}SOM{\}} (also called the Median {\{}SOM){\}} and several kernelized
versions of {\{}SOM.{\}} Although the first one is a generalization of
the batch version of the {\{}SOM{\}} algorithm to data described by a dissimilarity
measure, the various versions of the second ones are stochastic {\{}SOM.{\}}
We propose here to introduce a batch version of the kernel {\{}SOM{\}}
and to show how this one is related to the dissimilarity {\{}SOM.{\}} Finally,
an application to the classification of the vertices of a graph is
proposed and the algorithms are tested and compared on a simulated
data set.},
author = {Villa, Nathalie and Rossi, Fabrice},
booktitle = {Proceedings of the 6th Workshop on {\{}Self-Organizing{\}} Maps {\{}(WSOM{\}} 07)},
title = {{A comparison between dissimilarity {\{}SOM{\}} and kernel {\{}SOM{\}} for clustering the vertices of a graph}},
year = {2007}
}
@article{Feragen2013a,
author = {Feragen, Aasa and Kasenburg, Niklas and Group, Computational Biology and Petersen, Jens and Bruijne, Marleen De and Medical, Erasmus and Borgwardt, Karsten},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Feragen et al. - 2013 - Erratum to ”Scalable kernels for graphs with continuous attributes, NIPS 2013”.pdf:pdf},
pages = {1--2},
title = {{Erratum to ”Scalable kernels for graphs with continuous attributes, NIPS 2013”}},
year = {2013}
}
@article{Gama06,
author = {Gama, J and Fernandes, R and Rocha, R},
journal = {Intell. Data Analy.},
number = {1},
pages = {23--45},
title = {{Decision Trees for Mining Data Streams}},
volume = {10},
year = {2006}
}
@article{Lafferty1999,
author = {Lafferty, John},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Lafferty - 1999 - Diffusion Kernels on Graphs and Other Discrete Input Spaces.pdf:pdf},
journal = {Computer},
title = {{Diffusion Kernels on Graphs and Other Discrete Input Spaces}},
year = {1999}
}
@article{Li2015,
author = {Li, Meng and Leung, Howard and Liu, Zhiguang and Zhou, Liuyang},
doi = {10.1016/j.cag.2015.07.005},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2015 - 3D Human Motion Retrieval Using Graph Kernels Based on Adaptive Graph Construction.pdf:pdf},
issn = {00978493},
journal = {Computers {\&} Graphics},
keywords = {Adaptive graph,Graph kernel,Motion capture,Motion retrieval,Multiple kernel learning},
month = {jul},
publisher = {Elsevier},
title = {{3D Human Motion Retrieval Using Graph Kernels Based on Adaptive Graph Construction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0097849315001089},
year = {2015}
}
@article{Bifet2011,
address = {New York, New York, USA},
author = {Bifet, Albert and Holmes, Geoff and Pfahringer, Bernhard and Gavald{\`{a}}, Ricard},
doi = {10.1145/2020408.2020501},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bifet et al. - 2011 - Mining frequent closed graphs on evolving data streams.pdf:pdf},
isbn = {9781450308137},
journal = {Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '11},
keywords = {closed mining,concept drift,data streams,graphs},
pages = {591},
publisher = {ACM Press},
title = {{Mining frequent closed graphs on evolving data streams}},
url = {http://dl.acm.org/citation.cfm?doid=2020408.2020501},
year = {2011}
}
@article{Hartke,
author = {Hartke, Stephen G and Radcliffe, A J},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hartke, Radcliffe - Unknown - McKay ' s Canonical Graph Labeling Algorithm.pdf:pdf},
keywords = {canonical isomorph,canonical labeling,graph automorphism,graph isomorphism,nauty},
pages = {1--13},
title = {{McKay ' s Canonical Graph Labeling Algorithm}},
volume = {0000}
}
@inproceedings{Hiroshi2006,
abstract = {This paper proposes an efficient method of sentence retrieval based
on syntactic structure. Collins proposed Tree Kernel to calculate
structural similarity. However, structual retrieval based on Tree
Kernel is not practicable because the size of the index table by
Tree Kernel becomes impractical. We propose more efficient algorithms
approximating Tree Kernel: Tree Overlapping and Subpath Set. These
algorithms are more efficient than Tree Kernel because indexing is
possible with practical computation resources. The results of the
experiments comparing these three algorithms showed that structural
retrieval with Tree Overlapping and Subpath Set were faster than
that with Tree Kernel by 100 times and 1,000 times respectively.},
address = {Stroudsburg, PA, USA},
author = {Hiroshi, Ichikawa and Keita, Hakoda and Taiichi, Hashimoto and Takenobu, Tokunaga},
booktitle = {Proceedings of the COLING/ACL on Main conference poster sessions},
pages = {399--406},
publisher = {Association for Computational Linguistics},
series = {COLING-ACL '06},
title = {{Efficient sentence retrieval based on syntactic structure}},
year = {2006}
}
@article{Chang2013,
author = {Chang, Chih-chung and Lin, Chih-jen},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chang, Lin - 2013 - LIBSVM A Library for Support Vector Machines.pdf:pdf},
keywords = {classification,libsvm,optimization,regression,support vector ma-},
pages = {1--39},
title = {{LIBSVM : A Library for Support Vector Machines}},
year = {2013}
}
@inproceedings{Shin2013,
author = {Shin, Kilho},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shin - 2013 - A New Frontier of Kernel Design for Structured Data.pdf:pdf},
pages = {401--409},
title = {{A New Frontier of Kernel Design for Structured Data}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}shin13},
year = {2013}
}
@article{Duchi2009,
author = {Duchi, John and Singer, Yoram},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Duchi, Singer - 2009 - Efficient Online and Batch Learning Using Forward Backward Splitting.pdf:pdf},
keywords = {convex optimization,group sparsity,online learning,subgradient methods},
pages = {2899--2934},
title = {{Efficient Online and Batch Learning Using Forward Backward Splitting}},
volume = {10},
year = {2009}
}
@article{Dorigo2006,
abstract = {Swarm intelligence is a relatively new approach to problem solving that takes inspiration from the social behaviors of insects and of other animals. In particular, ants have inspired a number of methods and techniques among which the most studied and the most successful is the general purpose optimization technique known as ant colony optimization. Ant colony optimization (ACO) takes inspiration from the foraging behavior of some ant species. These ants deposit pheromone on the ground in order to mark some favorable path that should be followed by other members of the colony. Ant colony optimization exploits a similar mechanism for solving optimization problems. From the early nineties, when the first ant colony optimization algorithm was proposed, ACO attracted the attention of increasing numbers of researchers and many successful applications are now available. Moreover, a substantial corpus of theoretical results is becoming available that provides useful guidelines to researchers and practitioners in further applications of ACO. The goal of this article is to introduce ant colony optimization and to survey its most notable applications},
author = {Dorigo, M. and Birattari, M. and Stutzle, T.},
doi = {10.1109/MCI.2006.329691},
isbn = {0262042193},
issn = {1556-603X},
journal = {IEEE Computational Intelligence Magazine},
number = {4},
pages = {28--39},
pmid = {22074763},
title = {{Ant colony optimization}},
url = {http://ieeexplore.ieee.org/ielx5/10207/4129833/04129846.pdf?tp={\&}arnumber=4129846{\&}isnumber=4129833$\backslash$nhttps://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4129846},
volume = {1},
year = {2006}
}
@article{Borgwardt2007,
abstract = {It is widely believed that comparing discrepancies in the protein-protein interaction (PPI) networks of individuals will become an important tool in understanding and preventing diseases. Currently PPI networks for individuals are not available, but gene expression data is becoming easier to obtain and allows us to represent individuals by a co-integrated gene expression/protein interaction network. Two major problems hamper the application of graph kernels - state-of-the-art methods for whole-graph comparison - to compare PPI networks. First, these methods do not scale to graphs of the size of a PPI network. Second, missing edges in these interaction networks are biologically relevant for detecting discrepancies, yet, these methods do not take this into account. In this article we present graph kernels for biological network comparison that are fast to compute and take into account missing interactions. We evaluate their practical performance on two datasets of co-integrated gene expression/PPI networks.},
author = {Borgwardt, Karsten M and Kriegel, Hans-Peter and Vishwanathan, S V N and Schraudolph, Nicol N},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Borgwardt - 2007 - Graph kernels.pdf:pdf},
issn = {1793-5091},
journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
keywords = {Computational Biology,Databases, Genetic,Disease Progression,Gene Expression Profiling,Gene Expression Profiling: statistics {\&} numerical,Humans,Prognosis,Protein Array Analysis,Protein Array Analysis: statistics {\&} numerical dat,Protein Interaction Mapping,Protein Interaction Mapping: statistics {\&} numerica},
month = {jan},
pages = {4--15},
pmid = {17992741},
title = {{Graph kernels for disease outcome prediction from protein-protein interaction networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17992741},
year = {2007}
}
@article{Nowozin,
author = {Nowozin, Sebastian and Tsuda, Koji and Uno, Takeaki and Kudo, Taku and Bakir, Gokhan},
doi = {10.1109/CVPR.2007.383171},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Nowozin et al. - 2007 - Weighted Substructure Mining for Image Analysis.pdf:pdf},
isbn = {1-4244-1179-3},
journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {1--8},
publisher = {Ieee},
title = {{Weighted Substructure Mining for Image Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4270196},
year = {2007}
}
@article{Cover1965,
author = {Cover, TM},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cover - 1965 - Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition.pdf:pdf},
journal = {Electronic Computers, IEEE Transactions on},
pages = {326--334},
title = {{Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4038449},
year = {1965}
}
@inproceedings{Daum'eIII2004,
abstract = {We describe our entry into the DUC 2004 automatic document

summarization competition. We competed only in the single document,

headline generation task. Our system is based on a novel kernel

dubbed the tree position kernel, combined with two other well-known

kernels. Our system performs well on white-box evaluations, but does

very poorly in the overall DUC evaluation. However, the latter

results are offset by the fact that baseline systems consistently

outperform well engineered systems.},
address = {Boston, MA},
author = {{Daum{\'{e}} III}, Hal and Marcu, Daniel},
booktitle = {Proceedings of the Fourth Document Understanding Conference (DUC 2004)},
month = {may},
title = {{A Tree-Position Kernel for Document Compression}},
year = {2004}
}
@phdthesis{Martini2009,
author = {Martini, Matteo},
school = {University of Padova},
title = {{Netsim: Simulatore di reti genetiche}},
year = {2009}
}
@article{Ding2013,
author = {Ding, Yiliang and Tang, Yin and Kwok, Chun Kit and Zhang, Yu and Bevilacqua, Philip C. and Assmann, Sarah M.},
doi = {10.1038/nature12756},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ding et al. - 2013 - In vivo genome-wide profiling of RNA secondary structure reveals novel regulatory features.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = {nov},
publisher = {Nature Publishing Group},
title = {{In vivo genome-wide profiling of RNA secondary structure reveals novel regulatory features}},
url = {http://www.nature.com/doifinder/10.1038/nature12756},
year = {2013}
}
@article{Chi2015,
author = {Chi, Lianhua},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chi - 2015 - Hashing for Large-Scale Structured Data Classification.pdf:pdf},
journal = {Thesis},
pages = {143},
title = {{Hashing for Large-Scale Structured Data Classification}},
year = {2015}
}
@article{Kapoor2006,
abstract = {There have been many graph-based approaches for semi-supervised clas- sification. One problem is that of hyperparameter learning: performance depends greatly on the hyperparameters of the similarity graph, trans- formation of the graph Laplacian and the noise model. We present a Bayesian framework for learning hyperparameters for graph-based semi- supervised classification. Given some labeled data, which can contain inaccurate labels, we pose the semi-supervised classification as an in- ference problem over the unknown labels. Expectation Propagation is used for approximate inference and the mean of the posterior is used for classification. The hyperparameters are learned using EM for evidence maximization. We also show that the posterior mean can be written in terms of the kernel matrix, providing a Bayesian classifier to classify new points. Tests on synthetic and real datasets show cases where there are significant improvements in performance over the existing approaches.},
author = {Kapoor, a. and Kapoor, a. and Qi, Y. and Qi, Y. and Ahn, H. and Ahn, H. and Picard, R. and Picard, R.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kapoor et al. - 2006 - Hyperparameter and kernel learning for graph based semi-supervised classification.pdf:pdf},
isbn = {9780262232531},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {627},
title = {{Hyperparameter and kernel learning for graph based semi-supervised classification}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Hyperparameter+and+Kernel+Learning+for+Graph+Based+Semi-Supervised+Classification{\#}0},
volume = {18},
year = {2006}
}
@article{Yao2006,
abstract = {MOTIVATION: The recent discoveries of large numbers of non-coding RNAs and computational advances in genome-scale RNA search create a need for tools for automatic, high quality identification and characterization of conserved RNA motifs that can be readily used for database search. Previous tools fall short of this goal.

RESULTS: CMfinder is a new tool to predict RNA motifs in unaligned sequences. It is an expectation maximization algorithm using covariance models for motif description, featuring novel integration of multiple techniques for effective search of motif space, and a Bayesian framework that blends mutual information-based and folding energy-based approaches to predict structure in a principled way. Extensive tests show that our method works well on datasets with either low or high sequence similarity, is robust to inclusion of lengthy extraneous flanking sequence and/or completely unrelated sequences, and is reasonably fast and scalable. In testing on 19 known ncRNA families, including some difficult cases with poor sequence conservation and large indels, our method demonstrates excellent average per-base-pair accuracy--79{\%} compared with at most 60{\%} for alternative methods. More importantly, the resulting probabilistic model can be directly used for homology search, allowing iterative refinement of structural models based on additional homologs. We have used this approach to obtain highly accurate covariance models of known RNA motifs based on small numbers of related sequences, which identified homologs in deeply-diverged species.},
author = {Yao, Zizhen and Weinberg, Zasha and Ruzzo, Walter L},
doi = {10.1093/bioinformatics/btk008},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Yao, Weinberg, Ruzzo - 2006 - CMfinder--a covariance model based RNA motif finding algorithm.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Base Sequence,Computer Simulation,Conserved Sequence,Models, Genetic,Molecular Sequence Data,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Sequence Homology, Nucleic Acid,Software,Statistics as Topic},
month = {feb},
number = {4},
pages = {445--52},
pmid = {16357030},
title = {{CMfinder--a covariance model based RNA motif finding algorithm.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16357030},
volume = {22},
year = {2006}
}
@article{citeulike:10456962,
author = {Eskandari, Mojtaba and Hashemi, Sattar},
doi = {10.1016/j.jvlc.2012.02.002},
issn = {1045926X},
journal = {Journal of Visual Languages {\&} Computing},
month = {mar},
title = {{A graph mining approach for detecting unknown malwares}},
url = {http://dx.doi.org/10.1016/j.jvlc.2012.02.002},
year = {2012}
}
@article{Dietterich2008,
author = {Dietterich, Thomas G. and Domingos, Pedro and Getoor, Lise and Muggleton, Stephen and Tadepalli, Prasad},
doi = {10.1007/s10994-008-5079-1},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich et al. - 2008 - Structured machine learning the next ten years.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {dietterich,editors,g,hendrik blockeel,inductive logic programming,jude shavlik,learning,p,relational learning,statistical relational,structured machine learning,t,tadepalli},
month = {aug},
number = {1},
pages = {3--23},
title = {{Structured machine learning: the next ten years}},
url = {http://link.springer.com/10.1007/s10994-008-5079-1},
volume = {73},
year = {2008}
}
@article{Schafer2005,
author = {Sch{\"{a}}fer, Juliane and Strimmer, Korbinian},
doi = {10.2202/1544-6115.1175},
journal = {Statistical Applications in Genetics and Molecular Biology},
month = {nov},
number = {1},
title = {{A Shrinkage Approach to {\{}Large-Scale{\}} Covariance Matrix Estimation and Implications for Functional Genomics}},
volume = {4},
year = {2005}
}
@inproceedings{Schietgat2009,
author = {Schietgat, L and Costa, Fabrizio and Ramon, J and {De Raedt}, L},
booktitle = {7th International Workshop on Mining and Learning with Graphs},
pages = {1--3},
title = {{Maximum common subgraph mining: a fast and effective approach towards feature generation}},
year = {2009}
}
@misc{,
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Graph-based Natural Language Processing and Information Retrieval PDF.pdf.pdf:pdf},
title = {{Graph-based Natural Language Processing and Information Retrieval PDF.pdf}}
}
@inproceedings{Kirby2009,
author = {Bifet, A and Holmes, G and Pfahringer, B and Kirby, R and Gavald{\'{a}}, R},
booktitle = {Proc. of the 15th Internl. Conf. on Knowl. Disc. and Data Mining},
pages = {139--148},
title = {{New Ensemble methods for evolving data streams}},
year = {2009}
}
@article{Borgwardt2005a,
abstract = {MOTIVATION: Computational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.

RESULTS: Our graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.

AVAILABILITY: More information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.},
author = {Borgwardt, Karsten M and Ong, Cheng Soon and Sch{\"{o}}nauer, Stefan and Vishwanathan, S V N and Smola, Alex J and Kriegel, Hans-Peter},
doi = {10.1093/bioinformatics/bti1007},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Borgwardt et al. - 2005 - Protein function prediction via graph kernels.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Databases, Protein,Enzymes,Enzymes: chemistry,Models, Statistical,Protein Conformation,Protein Structure, Secondary,Sequence Analysis, Protein,Sequence Analysis, Protein: methods,Software},
month = {jun},
pages = {i47--56},
pmid = {15961493},
title = {{Protein function prediction via graph kernels.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15961493},
volume = {21 Suppl 1},
year = {2005}
}
@article{Graepel1998,
abstract = {We offer three algorithms for the generation of topographic mappings
to the practitioner of unsupervised data analysis. The algorithms
are each based on the minimization of a cost function which is performed
using an EM algorithm and deterministic annealing. The soft topographic
vector quantization algorithm (STVQ) - like the original self-organizing
map (SOM) - provides a tool for the creation of self-organizing maps
of Euclidean data. Its optimization scheme, however, offers an alternative
to the heuristic stepwise shrinking of the neighborhood width in
the SOM and makes it possible to use a fixed neighborhood function
solely to encode desired neighborhood relations between nodes. The
kernel-based soft topographic mapping (STMK) is a generalization
of STVQ and introduces new distance measures in data space, based
on kernel functions. Using the new distance measures corresponds
to performing the STVQ in a high-dimensional feature space, which
is related to data space by a nonlinear mapping. This preprocessing
can reveal structure in the data which may go unnoticed if the STVQ
is performed in the standard Euclidean space. The soft topographic
mapping for proximity data (STMP) is another generalization of STVQ
that enables the user to generate topographic maps for data which
are given in terms of pairwise proximities. It thus offers a flexible
alternative to multidimensional scaling methods and opens up a new
range of applications for SOMs. Both STMK and STMP share the robust
optimization properties of STVQ due to the application of deterministic
annealing. In our contribution we discuss the algorithms together
with their implementation and provide detailed pseudo-code and explanations.},
author = {Graepel, Thore and Burger, Matthias and Obermayer, Klaus},
doi = {10.1016/S0925-2312(98)00035-6},
journal = {Neurocomputing},
month = {nov},
number = {1-3},
pages = {173--190},
shorttitle = {Self-organizing maps},
title = {{Self-organizing maps: Generalizations and new optimization techniques}},
volume = {21},
year = {1998}
}
@techreport{Haussler1999,
author = {Haussler, David},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Haussler - 1999 - Convolution Kernels on Discrete Structures.pdf:pdf},
institution = {Department of Computer Science, University of California at Santa Cruz},
title = {{Convolution Kernels on Discrete Structures}},
year = {1999}
}
@inproceedings{Street01,
address = {San Francisco, CA},
author = {Street, W N and Kim, Y},
booktitle = {Proc. of the 7th Internl. Conf. on Knowl. Disc. and Data Mining (KDD'01)},
pages = {377--382},
title = {{A streaming ensemble algorithm ($\backslash$upshape{\{}SEA{\}}) for large-scale classification}},
year = {2001}
}
@article{Bifet2008,
address = {New York, New York, USA},
author = {Bifet, Albert and Gavald{\`{a}}, Ricard},
doi = {10.1145/1401890.1401900},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bifet, Gavald{\`{a}} - 2008 - Mining adaptively frequent closed unlabeled rooted trees in data streams.pdf:pdf},
isbn = {9781605581934},
journal = {Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD 08},
keywords = {closed mining,concept drift,data streams,patterns,trees},
pages = {34},
publisher = {ACM Press},
title = {{Mining adaptively frequent closed unlabeled rooted trees in data streams}},
url = {http://dl.acm.org/citation.cfm?doid=1401890.1401900},
year = {2008}
}
@inproceedings{Raina2007,
abstract = {We present a new machine learning framework called "self-taught learning"
for using unlabeled data in supervised classification tasks. We do
not assume that the unlabeled data follows the same class labels
or generative distribution as the labeled data. Thus, we would like
to use a large number of unlabeled images (or audio samples, or text
documents) randomly downloaded from the Internet to improve performance
on a given image (or audio, or text) classification task. Such unlabeled
data is significantly easier to obtain than in typical semi-supervised
or transfer learning settings, making self-taught learning widely
applicable to many practical learning problems. We describe an approach
to self-taught learning that uses sparse coding to construct higher-level
features using the unlabeled data. These features form a succinct
input representation and significantly improve classification performance.
When using an SVM for classification, we further show how a Fisher
kernel can be learned for this representation.},
address = {New York, NY, USA},
author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y},
booktitle = {ICML '07: Proceedings of the 24th international conference on Machine learning},
doi = {10.1145/1273496.1273592},
isbn = {978-1-59593-793-3},
pages = {759--766},
publisher = {ACM},
title = {{Self-taught learning: transfer learning from unlabeled data}},
year = {2007}
}
@article{Kelly1999,
author = {Kelly, MG and Hand, DJ and Adams, NM},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kelly, Hand, Adams - 1999 - The impact of changing populations on classifier performance.pdf:pdf},
journal = {Proceedings of the fifth ACM SIGKDD {\ldots}},
number = {2},
pages = {367--371},
title = {{The impact of changing populations on classifier performance}},
url = {http://dl.acm.org/citation.cfm?id=312285},
volume = {32},
year = {1999}
}
@article{He2011,
abstract = {Recent years have witnessed an incredibly increasing interest in the topic of incremental learning. Unlike conventional machine learning situations, data flow targeted by incremental learning becomes available continuously over time. Accordingly, it is desirable to be able to abandon the traditional assumption of the availability of representative training data during the training period to develop decision boundaries. Under scenarios of continuous data flow, the challenge is how to transform the vast amount of stream raw data into information and knowledge representation, and accumulate experience over time to support future decision-making process. In this paper, we propose a general adaptive incremental learning framework named ADAIN that is capable of learning from continuous raw data, accumulating experience over time, and using such knowledge to improve future learning and prediction performance. Detailed system level architecture and design strategies are presented in this paper. Simulation results over several real-world data sets are used to validate the effectiveness of this method.},
author = {He, Haibo and Chen, Sheng and Li, Kang and Xu, Xin},
doi = {10.1109/TNN.2011.2171713},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2011 - Incremental learning from stream data.pdf:pdf},
issn = {1941-0093},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Data Mining,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models, Theoretical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = {dec},
number = {12},
pages = {1901--14},
pmid = {22057060},
title = {{Incremental learning from stream data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22057060},
volume = {22},
year = {2011}
}
@book{bakir2007predicting,
author = {Bakir, G{\"{o}}khan and Hofman, Thomas and Scholkopf, Bernhard and Smola, Alexander J. and Taskar, Ben and Vishwanathan, S. V. N.},
publisher = {MIT press},
title = {{Predicting structured data}},
year = {2007}
}
@inproceedings{Vishwanathan2005,
abstract = {We present methods for dealing with missing variables in the context
of Gaussian Processes and Support Vector Machines. This solves an
important problem which has largely been ignored by kernel methods:
How to systematically deal with incomplete data? Our method can also
be applied to problems with partially observed labels as well as
to the transductive setting where we view the labels as missing data.},
author = {Vishwanathan, S V N and Smola, Alex J and Hofmann, Thomas},
booktitle = {AISTATS 2005, January 2005, Barbados},
title = {{Kernel Methods for Missing Variables}},
year = {2005}
}
@incollection{Joachims1999,
address = {Cambridge, MA, USA},
author = {Joachims, Thorsten},
isbn = {0-262-19416-3},
pages = {169--184},
publisher = {MIT Press},
title = {{Making large-scale support vector machine learning practical}},
year = {1999}
}
@article{Weislow1989,
author = {Weislow, Owen S and Kiser, Rebecca and Fine, Donald L and Bader, John and Shoemaker, Robert H and Boyd, Michael R},
issn = {0027-8874},
journal = {Journal of the National Cancer Institute},
number = {8},
pages = {577--586},
title = {{New soluble-formazan assay for HIV-1 cytopathic effects: application to high-flux screening of synthetic and natural products for AIDS-antiviral activity.}},
volume = {81},
year = {1989}
}
@article{Bottou2012,
author = {Bottou, L},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bottou - 2012 - Stochastic gradient descent tricks.pdf:pdf},
journal = {Neural Networks: Tricks of the Trade},
number = {1},
pages = {1--16},
title = {{Stochastic gradient descent tricks}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-35289-8{\_}25},
volume = {1},
year = {2012}
}
@article{Sun2008,
abstract = {BACKGROUND: Machine-learning tools have gained considerable attention during the last few years for analyzing biological networks for protein function prediction. Kernel methods are suitable for learning from graph-based data such as biological networks, as they only require the abstraction of the similarities between objects into the kernel matrix. One key issue in kernel methods is the selection of a good kernel function. Diffusion kernels, the discretization of the familiar Gaussian kernel of Euclidean space, are commonly used for graph-based data.

RESULTS: In this paper, we address the issue of learning an optimal diffusion kernel, in the form of a convex combination of a set of pre-specified kernels constructed from biological networks, for protein function prediction. Most prior work on this kernel learning task focus on variants of the loss function based on Support Vector Machines (SVM). Their extensions to other loss functions such as the one based on Kullback-Leibler (KL) divergence, which is more suitable for mining biological networks, lead to expensive optimization problems. By exploiting the special structure of the diffusion kernel, we show that this KL divergence based kernel learning problem can be formulated as a simple optimization problem, which can then be solved efficiently. It is further extended to the multi-task case where we predict multiple functions of a protein simultaneously. We evaluate the efficiency and effectiveness of the proposed algorithms using two benchmark data sets.

CONCLUSION: Results show that the performance of linearly combined diffusion kernel is better than every single candidate diffusion kernel. When the number of tasks is large, the algorithms based on multiple tasks are favored due to their competitive recognition performance and small computational costs.},
author = {Sun, Liang and Ji, Shuiwang and Ye, Jieping},
doi = {10.1186/1471-2105-9-162},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sun, Ji, Ye - 2008 - Adaptive diffusion kernel learning from biological networks for protein function prediction.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Gene Expression Profiling,Gene Expression Profiling: methods,Models, Biological,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Protein Interaction Mapping,Protein Interaction Mapping: methods,Proteome,Proteome: metabolism,Signal Transduction,Signal Transduction: physiology},
month = {jan},
pages = {162},
pmid = {18366736},
title = {{Adaptive diffusion kernel learning from biological networks for protein function prediction.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2409449{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Cormode2005,
author = {Cormode, Graham and Muthukrishnan, S.},
doi = {10.1016/j.jalgor.2003.12.001},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cormode, Muthukrishnan - 2005 - An improved data stream summary the count-min sketch and its applications.pdf:pdf},
issn = {01966774},
journal = {Journal of Algorithms},
month = {apr},
number = {1},
pages = {58--75},
title = {{An improved data stream summary: the count-min sketch and its applications}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0196677403001913},
volume = {55},
year = {2005}
}
@article{Raedt,
author = {Raedt, Luc De and Ramon, Jan},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Raedt, Ramon - Unknown - Deriving Distance Metrics from Generality Relations.pdf:pdf},
keywords = {02,20,40,89,distance metric,ff,general-specific ordering,pacs,pc},
number = {September 2008},
pages = {1--11},
title = {{Deriving Distance Metrics from Generality Relations}}
}
@inproceedings{DBLP:conf/dis/2005,
booktitle = {Discovery Science},
editor = {Hoffmann, Achim G and Motoda, Hiroshi and Scheffer, Tobias},
isbn = {3-540-29230-6},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Discovery Science, 8th International Conference, DS 2005, Singapore, October 8-11, 2005, Proceedings}},
volume = {3735},
year = {2005}
}
@article{delaunay,
author = {Su, Peter and Drysdale, Robert L Scot},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Su, Drysdale - 1995 - A comparison of sequential Delaunay triangulation algorithms.pdf:pdf},
pages = {1--24},
title = {{A Comparison of Sequential Delaunay Triangulation Algorithms}},
year = {1996}
}
@article{Mao2015,
address = {New York, New York, USA},
author = {Mao, Qi and Wang, Li and Goodison, Steve and Sun, Yijun},
doi = {10.1145/2783258.2783309},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mao et al. - 2015 - Dimensionality Reduction Via Graph Structure Learning.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '15},
keywords = {clus-,dimensionality reduction,graph structure learning,tering,unsupervised learning},
pages = {765--774},
publisher = {ACM Press},
title = {{Dimensionality Reduction Via Graph Structure Learning}},
url = {http://dl.acm.org/citation.cfm?doid=2783258.2783309},
year = {2015}
}
@article{Ahmed2013,
abstract = {Natural graphs, such as social networks, email graphs, or instant messaging patterns, have become pervasive through the internet. These graphs are massive, often containing hundreds of millions of nodes and billions of edges. While some theoretical models have been proposed to study such graphs, their analysis is still difficult due to the scale and nature of the data. We propose a framework for large-scale graph decomposition and inference. To resolve the scale, our framework is distributed so that the data are partitioned over a shared-nothing set of machines. We propose a novel factorization technique that relies on partitioning a graph so as to minimize the number of neighboring vertices rather than edges across partitions. Our decomposition is based on a streaming algorithm. It is network-aware as it adapts to the network topology of the underlying computational hardware. We use local copies of the variables and an efficient asynchronous communication protocol to synchronize the replicated values in order to perform most of the computation without having to incur the cost of network communication. On a graph of 200 million vertices and 10 billion edges, derived from an email communication network, our algorithm retains convergence properties while allowing for almost linear scalability in the number of computers.},
author = {Ahmed, Amr and Shervashidze, Nino and Narayanamurthy, Shravan and Josifovski, Vanja and Smola, Alexander J.},
doi = {10.1145/2488388.2488393},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ahmed et al. - 2013 - Distributed Large-scale Natural Graph Factorization.pdf:pdf},
isbn = {978-1-4503-2035-1},
journal = {Proceedings of the 22Nd International Conference on World Wide Web},
keywords = {asynchronous algorithms,distributed optimization,graph algorithms,graph factorization,large-scale machine learning,matrix factorization},
pages = {37--48},
title = {{Distributed Large-scale Natural Graph Factorization}},
url = {http://dl.acm.org/citation.cfm?id=2488388.2488393},
year = {2013}
}
@article{Mhaskar2016,
abstract = {We describe computational tasks - especially in vision - that correspond to compositional/hierarchical functions. While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower VC-dimension as well as the number of training parameters. This leads to the question of approximation by sparse polynomials (in the number of independent parameters) and, as a consequence, by deep networks. We also discuss connections between our results and learnability of sparse Boolean functions, settling an old conjecture by Bengio.},
archivePrefix = {arXiv},
arxivId = {1603.00988},
author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
eprint = {1603.00988},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mhaskar, Liao, Poggio - 2016 - Learning Real and Boolean Functions When Is Deep Better Than Shallow.pdf:pdf},
number = {45},
pages = {1--12},
title = {{Learning Real and Boolean Functions: When Is Deep Better Than Shallow}},
url = {http://arxiv.org/abs/1603.00988},
year = {2016}
}
@article{Vapnik1999,
abstract = {Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).},
author = {Vapnik, V N},
doi = {10.1109/72.788640},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
month = {jan},
number = {5},
pages = {988--99},
pmid = {18252602},
title = {{An overview of statistical learning theory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18252602},
volume = {10},
year = {1999}
}
@article{Eddy1994,
author = {Eddy, Sean R and Durbin, Richard and Road, Hills},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Eddy, Durbin, Road - 1994 - analysis using covariance models.pdf:pdf},
number = {11},
title = {analysis using covariance models},
volume = {22},
year = {1994}
}
@inproceedings{Crammer_Dekel_Keshet_Shalev-Shwartz_Singer_2006,
author = {Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram},
booktitle = {NIPS},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Crammer et al. - 2003 - Online Passive-Aggressive Algorithms.pdf:pdf},
publisher = {MIT Press},
title = {{Online Passive-Aggressive Algorithms}},
volume = {7},
year = {2003}
}
@book{Weifeng10,
author = {Liu, Weifeng and Principe, Jose C and Haykin, Simon},
edition = {1st},
howpublished = {Hardcover},
isbn = {0470447532},
month = {mar},
publisher = {Wiley},
title = {{Kernel Adaptive Filtering: A Comprehensive Introduction}},
year = {2010}
}
@article{Tan:2013:SSK:2524889.2524892,
address = {Inderscience Publishers, Geneva, SWITZERLAND},
author = {Tan, Mehmet and Polat, Faruk and Alhajj, Reda},
doi = {10.1504/IJDMB.2013.056080},
issn = {1748-5673},
journal = {Int. J. Data Min. Bioinformatics},
number = {3},
pages = {294--310},
publisher = {Inderscience Publishers},
title = {{Subtree Selection in Kernels for Graph Classification}},
url = {http://dx.doi.org/10.1504/IJDMB.2013.056080},
volume = {8},
year = {2013}
}
@article{vert_kernel_2005,
abstract = {Support vector machines and kernel methods are increasingly popular
in genomics and computational biology, due to their good performance
in real-world applications and strong modularity that makes them
suitable to a wide range of problems, from the classification of
tumors to the automatic annotation of proteins. Their ability to
work in high dimension, to process non-vectorial data, and the natural
framework they provide to integrate heterogeneous data are particularly
relevant to various problems arising in computational biology. In
this chapter we survey some of the most prominent applications published
so far, highlighting the particular developments in kernel methods
triggered by problems in biology, and mention a few promising research
directions likely to expand in the future.},
author = {Vert, Jean-Philippe},
journal = {q-bio/0510032},
month = {oct},
title = {{Kernel methods in genomics and computational biology}},
year = {2005}
}
@incollection{NIPS2008_3495,
author = {Rahimi, Ali and Recht, Benjamin},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {Koller, D and Schuurmans, D and Bengio, Y and Bottou, L},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rahimi, Recht - 2009 - Weighted Sums of Random Kitchen Sinks Replacing minimization with randomization in learning.pdf:pdf},
pages = {1313--1320},
publisher = {Curran Associates, Inc.},
title = {{Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning}},
url = {http://papers.nips.cc/paper/3495-weighted-sums-of-random-kitchen-sinks-replacing-minimization-with-randomization-in-learning.pdf},
year = {2009}
}
@article{Ralaivola2005,
abstract = {Increased availability of large repositories of chemical compounds is creating new challenges and opportunities for the application of machine learning methods to problems in computational chemistry and chemical informatics. Because chemical compounds are often represented by the graph of their covalent bonds, machine learning methods in this domain must be capable of processing graphical structures with variable size. Here, we first briefly review the literature on graph kernels and then introduce three new kernels (Tanimoto, MinMax, Hybrid) based on the idea of molecular fingerprints and counting labeled paths of depth up to d using depth-first search from each possible vertex. The kernels are applied to three classification problems to predict mutagenicity, toxicity, and anti-cancer activity on three publicly available data sets. The kernels achieve performances at least comparable, and most often superior, to those previously reported in the literature reaching accuracies of 91.5{\%} on the Mutag dataset, 65-67{\%} on the PTC (Predictive Toxicology Challenge) dataset, and 72{\%} on the NCI (National Cancer Institute) dataset. Properties and tradeoffs of these kernels, as well as other proposed kernels that leverage 1D or 3D representations of molecules, are briefly discussed.},
author = {Ralaivola, Liva and Swamidass, Sanjay J and Saigo, Hiroto and Baldi, Pierre},
doi = {10.1016/j.neunet.2005.07.009},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ralaivola et al. - 2005 - Graph kernels for chemical informatics.pdf:pdf},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Anticarcinogenic Agents,Artificial Intelligence,Computer Graphics,Databases, Genetic,Drug Toxicity,Information Storage and Retrieval,Models, Chemical,National Practitioner Data Bank,Pattern Recognition, Automated,Sequence Analysis, Protein,Structure-Activity Relationship,United States},
month = {oct},
number = {8},
pages = {1093--110},
pmid = {16157471},
title = {{Graph kernels for chemical informatics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16157471},
volume = {18},
year = {2005}
}
@article{Kashima2009,
author = {Kashima, Hisashi and Id{\'{e}}, Tsuyoshi and Kato, Tsuyoshi and Sugiyama, Masashi},
journal = {IEICE Transactions},
number = {7},
pages = {1338--1353},
title = {{Recent Advances and Trends in Large-Scale Kernel Methods}},
volume = {92-D},
year = {2009}
}
@article{Crammer2006,
author = {Wang, Zhuang and Vucetic, Slobodan and Crammer, K and Dekel, O},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2010 - Online Passive-Aggressive Algorithms on a Budget.pdf:pdf},
journal = {Journal of Machine Learning Research - Proceedings Track},
pages = {908--915},
title = {{Online Passive-Aggressive Algorithms on a Budget}},
url = {http://dl.acm.org/citation.cfm?id=1248566},
volume = {9},
year = {2010}
}
@phdthesis{Celebrin2004,
author = {Celebrin, Antonio},
school = {University of Padova, Department of Pure and Applied Mathematics},
title = {{Teoria della programmazione semidefinita}},
year = {2004}
}
@inproceedings{Smola2003,
author = {Smola, AJ and Kondor, Risi},
booktitle = {Proceedings of the conference on learning theory. COLT},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Smola, Kondor - 2003 - Kernels and regularization on graphs.pdf:pdf},
pages = {144--158},
title = {{Kernels and regularization on graphs}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-45167-9{\_}12},
year = {2003}
}
@inproceedings{DBLP:conf/alt/2008,
booktitle = {ALT},
editor = {Freund, Yoav and Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Tur{\'{a}}n, Gy{\"{o}}rgy and Zeugmann, Thomas},
isbn = {978-3-540-87986-2},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Algorithmic Learning Theory, 19th International Conference, ALT 2008, Budapest, Hungary, October 13-16, 2008. Proceedings}},
volume = {5254},
year = {2008}
}
@inproceedings{Bifet2006,
author = {Baena-Garcia, M and del Campo-Avila, J and Fidalgo, R and Bifet, A and Ravalda, R and Morales-Bueno, R},
booktitle = {Internl. Workshop on Knowl. Disc. from Data Streams},
title = {{Early drift detection method}},
year = {2006}
}
@inproceedings{Ghous2008,
address = {Glenelg, South Australia},
author = {Ghous, Hamid and Kennedy, Paul J and Catchpoole, Daniel R and Simoff, Simeon J},
booktitle = {Seventh Australasian Data Mining Conference (AusDM 2008)},
editor = {Roddick, John F and Li, Jiuyong and Christen, Peter and Kennedy, Paul J},
pages = {133--140},
publisher = {ACS},
series = {CRPIT},
title = {{Kernel-based Visualisation of Genes with the Gene Ontology}},
volume = {87},
year = {2008}
}
@inproceedings{Kashima2002,
author = {Kashima, Hisashi and Koyanagi, Teruo},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
isbn = {1-55860-873-7},
pages = {291--298},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Kernels for Semi-Structured Data}},
year = {2002}
}
@inproceedings{tavallese2009,
author = {Tavallaee, M and Bagheri, E and Lu, W and Ghorbani, A},
booktitle = {Proc. of the Second IEEE Internl. Conf. on Computational intelligence for security and defense applications (CISDA'09)},
pages = {53--58},
title = {{A Detailed Analysis of the KDD CUP 99 Data Set}},
year = {2009}
}
@inproceedings{Ramon03expressivityversus,
author = {Ramon, Jan and G{\"{a}}rtner, Thomas},
booktitle = {Proceedings of the First International Workshop on Mining Graphs, Trees and Sequences},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ramon, G{\"{a}}rtner - 2003 - Expressivity versus efficiency of graph kernels.pdf:pdf},
pages = {65--74},
title = {{Expressivity versus efficiency of graph kernels}},
year = {2003}
}
@article{Kashima:kx,
author = {Kashima, H and Inokuchi, A."},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kashima, Inokuchi - 2002 - Kernels for graph classification.pdf:pdf},
journal = {ICDM Workshop on Active Mining},
title = {{Kernels for graph classification}},
volume = {2002},
year = {2002}
}
@article{Braun2008,
author = {Braun, Mikio L and Buhmann, Joachim M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Braun, Buhmann - 2008 - On Relevant Dimensions in Kernel Feature Spaces.pdf:pdf},
keywords = {dimension reduction,effective dimensionality,feature space,kernel methods},
pages = {1875--1908},
title = {{On Relevant Dimensions in Kernel Feature Spaces}},
volume = {9},
year = {2008}
}
@article{Eddy2003,
author = {Eddy, SR},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Eddy - 2003 - Infernal user's guide.pdf:pdf},
journal = {Dispon{\'{i}} vel em http://infernal. janelia. org},
number = {December},
title = {{Infernal user's guide}},
url = {http://web.njit.edu/{~}wangj/Infernal.pdf},
year = {2003}
}
@article{Lu2008,
abstract = {In this paper, we propose a distributed parallel support vector machine
(DPSVM) training mechanism in a configurable network environment
for distributed data mining. The basic idea is to exchange support
vectors among a strongly connected network (SCN) so that multiple
servers may work concurrently on distributed data set with limited
communication cost and fast training speed. The percentage of servers
that can work in parallel and the communication overhead may be adjusted
through network configuration. The proposed algorithm further speeds
up through online implementation and synchronization. We prove that
the global optimal classifier can be achieved iteratively over an
SCN. Experiments on a real-world data set show that the computing
time scales well with the size of the training data for most networks.
Numerical results show that a randomly generated SCN may achieve
better performance than the state of the art method, cascade SVM,
in terms of total training time.},
author = {Lu, Yumao and Roychowdhury, Vwani and Vandenberghe, Lieven},
doi = {10.1109/TNN.2007.2000061},
issn = {1045-9227},
journal = {Neural Networks, IEEE Transactions on},
number = {7},
pages = {1167--1178},
title = {{Distributed Parallel Support Vector Machines in Strongly Connected Networks}},
volume = {19},
year = {2008}
}
@article{Knisley2012,
author = {Knisley, Debra and Knisley, Jeff and Ross, Chelsea and Rockney, Alissa},
doi = {10.5402/2012/157135},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Knisley et al. - 2012 - Classifying Multigraph Models of Secondary RNA Structure Using Graph-Theoretic Descriptors.pdf:pdf},
issn = {2090-7346},
journal = {ISRN Bioinformatics},
pages = {1--11},
title = {{Classifying Multigraph Models of Secondary RNA Structure Using Graph-Theoretic Descriptors}},
url = {http://www.hindawi.com/isrn/bioinformatics/2012/157135/},
volume = {2012},
year = {2012}
}
@inproceedings{Zhu2003,
abstract = {An approach to semi-supervised learning is proposed that is based
on a Gaussian random field model. Labeled and unlabeled data are
represented as vertices in a weighted graph, with edge weights encoding
the similarity between instances. The learning problem is then formulated
in terms of a Gaussian random field on this graph, where the mean
of the field is characterized in terms of harmonic functions, and
is efficiently obtained using matrix methods or belief propagation.
The resulting learning algorithms have intimate connections with
random walks, electric networks, and spectral graph theory. We discuss
methods to incorporate class priors and the predictions of classifiers
obtained by supervised learning. We also propose a method of parameter
learning by entropy minimization, and show the algorithm{\&}{\#}039;s
ability to perform feature selection. Promising experimental results
are presented for synthetic data, digit classification, and text
classification tasks. 1.},
author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
booktitle = {In ICML},
pages = {912--919},
title = {{Semi-supervised learning using Gaussian fields and harmonic functions}},
year = {2003}
}
@article{Ma2008,
abstract = {In bioinformatics studies, supervised classification with high-dimensional
input variables is frequently encountered. Examples routinely arise
in genomic, epigenetic and proteomic studies. Feature selection can
be employed along with classifier construction to avoid over-fitting,
to generate more reliable classifier and to provide more insights
into the underlying causal relationships. In this article, we provide
a review of several recently developed penalized feature selection
and classification techniques--which belong to the family of embedded
feature selection methods--for bioinformatics studies with high-dimensional
input. Classification objective functions, penalty functions and
computational algorithms are discussed. Our goal is to make interested
researchers aware of these feature selection and classification methods
that are applicable to high-dimensional bioinformatics data.},
annote = {{\{}PMID:{\}} 18562478},
author = {Ma, Shuangge and Huang, Jian},
doi = {10.1093/bib/bbn027},
issn = {1477-4054},
journal = {Briefings in Bioinformatics},
month = {sep},
number = {5},
pages = {392--403},
title = {{Penalized feature selection and classification in bioinformatics}},
volume = {9},
year = {2008}
}
@inproceedings{Vert2002,
author = {Vert, Jean-Philippe and Kanehisa, Minoru},
booktitle = {NIPS},
editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
isbn = {0-262-02550-7},
pages = {1425--1432},
publisher = {MIT Press},
title = {{Graph-Driven Feature Extraction From Microarray Data Using Diffusion Kernels and Kernel CCA}},
year = {2002}
}
@article{Deng2002,
author = {Deng, Hongmei and Li, Wei and Agrawal, Dharma P},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Deng, Li, Agrawal - 2002 - Routing Security in Wireless Ad Hoc Networks.pdf:pdf},
journal = {IEEE Communications Magazine},
number = {October},
pages = {70--75},
title = {{Routing Security in Wireless Ad Hoc Networks}},
year = {2002}
}
@article{Geurts2007,
abstract = {Elucidating biological networks between proteins appears nowadays
as one of the most important challenges in systems biology. Computational
approaches to this problem are important to complement high-throughput
technologies and to help biologists in designing new experiments.
In this work, we focus on the completion of a biological network
from various sources of experimental {\{}data.RESULTS:We{\}} propose a
new machine learning approach for the supervised inference of biological
networks, which is based on a kernelization of the output space of
regression trees. It inherits several features of tree-based algorithms
such as interpretability, robustness to irrelevant variables, and
input scalability. We applied this method to the inference of two
kinds of networks in the yeast S. cerevisiae: a protein-protein interaction
network and an enzyme network. In both cases, we obtained results
competitive with existing approaches. We also show that our method
provides relevant insights on input data regarding their potential
relationship with the existence of interactions. Furthermore, we
confirm the biological validity of our predictions in the context
of an analysis of gene expression {\{}data.CONCLUSION:Output{\}} kernel
tree based methods provide an efficient tool for the inference of
biological networks from experimental data. Their simplicity and
interpretability should make them of great value for biologists.},
author = {Geurts, Pierre and Touleimat, Nizar and Dutreix, Marie and D'Alche-Buc, Florence},
doi = {10.1186/1471-2105-8-S2-S4},
issn = {1471-2105},
journal = {{\{}BMC{\}} Bioinformatics},
number = {Suppl 2},
pages = {S4},
title = {{Inferring biological networks with output kernel trees}},
volume = {8},
year = {2007}
}
@article{Fouss2012,
abstract = {This paper presents a survey as well as an empirical comparison and evaluation of seven kernels on graphs and two related similarity matrices, that we globally refer to as "kernels on graphs" for simplicity. They are the exponential diffusion kernel, the Laplacian exponential diffusion kernel, the von Neumann diffusion kernel, the regularized Laplacian kernel, the commute-time (or resistance-distance) kernel, the random-walk-with-restart similarity matrix, and finally, a kernel first introduced in this paper (the regularized commute-time kernel) and two kernels defined in some of our previous work and further investigated in this paper (the Markov diffusion kernel and the relative-entropy diffusion matrix). The kernel-on-graphs approach is simple and intuitive. It is illustrated by applying the nine kernels to a collaborative-recommendation task, viewed as a link prediction problem, and to a semisupervised classification task, both on several databases. The methods compute proximity measures between nodes that help study the structure of the graph. Our comparisons suggest that the regularized commute-time and the Markov diffusion kernels perform best on the investigated tasks, closely followed by the regularized Laplacian kernel. {\textcopyright} 2012 Elsevier Ltd.},
author = {Fouss, Fran{\c{c}}ois and Francoisse, Kevin and Yen, Luh and Pirotte, Alain and Saerens, Marco},
doi = {10.1016/j.neunet.2012.03.001},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fouss et al. - 2012 - An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classificat.pdf:pdf},
isbn = {0-7695-2701-7},
issn = {08936080},
journal = {Neural Networks},
keywords = {Collaborative recommendation,Graph mining,Kernels on graphs,Semisupervised classification},
pages = {53--72},
pmid = {22497802},
publisher = {Elsevier Ltd},
title = {{An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification}},
url = {http://dx.doi.org/10.1016/j.neunet.2012.03.001},
volume = {31},
year = {2012}
}
@article{Gomi2012,
author = {Gomi, Tsutomu},
issn = {1881-4883},
journal = {Nihon Hoshasen Gijutsu Gakkai zasshi},
month = {jan},
number = {1},
pages = {136},
pmid = {22277825},
title = {{Con olution Kernels on Discrete Structures}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22278035},
volume = {68},
year = {2012}
}
@inproceedings{Cortes2013,
author = {Cortes, Corinna and Kloft, Marius and Mohri, Mehryar},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {Weinberger, C.J.C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Kloft, Mohri - 2013 - Learning kernels using local Rademacher complexity.pdf:pdf},
pages = {2760--2768},
title = {{Learning kernels using local Rademacher complexity}},
url = {http://papers.nips.cc/paper/4896-learning-kernels-using-local-rademacher-complexity},
year = {2013}
}
@article{DaSanMartino2010,
author = {{Da San Martino}, Giovanni and Sperduti, Alessandro},
doi = {10.1109/MCI.2009.935308},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Da San Martino, Sperduti - 2010 - Mining Structured Data.pdf:pdf},
issn = {1556-603X},
journal = {IEEE Computational Intelligence Magazine},
month = {feb},
number = {1},
pages = {42--49},
title = {{Mining Structured Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5386100},
volume = {5},
year = {2010}
}
@article{Pavlopoulos2011,
abstract = {Understanding complex systems often requires a bottom-up analysis towards a systems biology approach. The need to investigate a system, not only as individual components but as a whole, emerges. This can be done by examining the elementary constituents individually and then how these are connected. The myriad components of a system and their interactions are best characterized as networks and they are mainly represented as graphs where thousands of nodes are connected with thousands of vertices. In this article we demonstrate approaches, models and methods from the graph theory universe and we discuss ways in which they can be used to reveal hidden properties and features of a network. This network profiling combined with knowledge extraction will help us to better understand the biological significance of the system.},
author = {Pavlopoulos, Georgios a and Secrier, Maria and Moschopoulos, Charalampos N and Soldatos, Theodoros G and Kossida, Sophia and Aerts, Jan and Schneider, Reinhard and Bagos, Pantelis G},
doi = {10.1186/1756-0381-4-10},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Pavlopoulos et al. - 2011 - Using graph theory to analyze biological networks.pdf:pdf},
issn = {1756-0381},
journal = {BioData mining},
keywords = {biological network,biological network clustering analysis,clustering analysis,graph theory,node ranking},
month = {jan},
number = {1},
pages = {10},
pmid = {21527005},
publisher = {BioMed Central Ltd},
title = {{Using graph theory to analyze biological networks.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3101653{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {4},
year = {2011}
}
@inproceedings{Aggarwala,
author = {Aggarwal, Charu C and Zhao, Yuchen and Yu, Philip S},
booktitle = {SDM},
doi = {10.1.1.40.6757},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal, Zhao, Yu - 2010 - On Clustering Graph Streams.pdf:pdf},
pages = {478--489},
title = {{On Clustering Graph Streams}},
year = {2010}
}
@article{Summary,
author = {Summary, Project},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Summary - Unknown - Machine Learning for Sequences and Structured Data Tools for Non-Experts.pdf:pdf},
title = {{Machine Learning for Sequences and Structured Data: Tools for Non-Experts}}
}
@article{Bobadilla2013,
author = {Bobadilla, J. and Ortega, F. and Hernando, a. and Guti{\'{e}}rrez, a.},
doi = {10.1016/j.knosys.2013.03.012},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bobadilla et al. - 2013 - Recommender systems survey.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
month = {jul},
pages = {109--132},
publisher = {Elsevier B.V.},
title = {{Recommender systems survey}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950705113001044},
volume = {46},
year = {2013}
}
@article{Agarwal2006,
author = {Agarwal, Sameer and Branson, Kristin and Belongie, Serge},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Agarwal, Branson, Belongie - 2006 - Higher Order Learning with Graphs.pdf:pdf},
journal = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
title = {{Higher Order Learning with Graphs}},
year = {2006}
}
@article{Shervashidze2012,
author = {Shervashidze, Nino},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shervashidze - 2012 - Scalable graph kernels.pdf:pdf},
journal = {PhD thesis},
title = {{Scalable graph kernels}},
url = {http://tobias-lib.uni-tuebingen.de/volltexte/2012/6461/},
year = {2012}
}
@article{Lanckriet2004,
author = {Lanckriet, Gert R G and Cristianini, Nello and Bartlett, Peter and Ghaoui, Laurent El and Jordan, Michael I},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
pages = {27--72},
publisher = {JMLR.org},
title = {{Learning the Kernel Matrix with Semidefinite Programming}},
volume = {5},
year = {2004}
}
@incollection{Bloehdorn,
author = {Bloehdorn, Stephan and Rettinger, Achim},
booktitle = {The Semantic Web: Research and Applications},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bloehdorn, Rettinger - 2012 - Graph Kernels for RDF Data.pdf:pdf},
pages = {134--148},
title = {{Graph Kernels for RDF Data}},
year = {2012}
}
@article{Maggi2013,
author = {Maggi, Fabrizio Maria and Dumas, Marlon and Garc{\'{i}}a-Ba{\~{n}}uelos, Luciano and Montali, Marco},
doi = {10.1007/978-3-642-40176-3_8},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Maggi et al. - 2013 - Discovering data-aware declarative process models from event logs.pdf:pdf},
isbn = {9783642401756},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Automated Process Discovery,Declare,Linear Temporal Logic,Predicate Mining},
pages = {81--96},
title = {{Discovering data-aware declarative process models from event logs}},
volume = {8094 LNCS},
year = {2013}
}
@article{Muller2002,
abstract = {This paper provides an introduction to support vector machines, kernel
Fisher discriminant analysis, and kernel principal component analysis,
as examples for successful kernel-based learning methods. We first
give a short background about Vapnik-Chervonenkis theory and kernel
feature spaces and then proceed to kernel based learning in supervised
and unsupervised scenarios including practical and algorithmic considerations.
We illustrate the usefulness of kernel algorithms by discussing applications
such as optical character recognition and DNA analysis},
author = {Muller, K R and Mika, S and Ratsch, G and Tsuda, K and Scholkopf, B},
doi = {10.1109/72.914517},
journal = {Neural Networks, IEEE Transactions on},
month = {aug},
number = {2},
pages = {181--201},
title = {{An introduction to kernel-based learning algorithms}},
volume = {12},
year = {2002}
}
@inproceedings{Ortho,
author = {Flint, Ortho},
title = {{Private communications}},
year = {2010}
}
@inproceedings{DBLP:conf/nips/2001,
booktitle = {NIPS},
editor = {Dietterich, Thomas G and Becker, Suzanna and Ghahramani, Zoubin},
publisher = {MIT Press},
title = {{Advances in Neural Information Processing Systems 14 [Neural Information Processing Systems: Natural and Synthetic, NIPS 2001, December 3-8, 2001, Vancouver, British Columbia, Canada]}},
year = {2001}
}
@article{Collins2012,
author = {Collins, John E and White, Simon and Searle, Stephen M J and Stemple, Derek L},
doi = {10.1101/gr.137901.112.Freely},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Collins et al. - 2012 - Incorporating RNA-seq data into the zebrafish Ensembl genebuild.pdf:pdf},
pages = {2067--2078},
title = {{Incorporating RNA-seq data into the zebrafish Ensembl genebuild}},
year = {2012}
}
@article{Gartner2003,
address = {New York, NY, USA},
author = {G{\"{a}}rtner, Thomas},
doi = {10.1145/959242.959248},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
keywords = {inductive logic programming,ing,kernel methods,multi-relational data min-,structured data},
month = {jul},
number = {1},
pages = {49},
publisher = {ACM},
title = {{A survey of kernels for structured data}},
url = {http://portal.acm.org/ft{\_}gateway.cfm?id=959248{\&}type=pdf{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=82529775{\&}CFTOKEN=62382615 http://portal.acm.org/citation.cfm?doid=959242.959248},
volume = {5},
year = {2003}
}
@inproceedings{Aiolli2007a,
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro and Moschitti, Alessandro},
booktitle = {Proceedings of the 2007 IEEE Symposium on Computational Intelligence and Data Mining},
pages = {308--315},
title = {{Efficient Kernel-based Learning for Trees}},
year = {2007}
}
@article{Shervashidze2011,
author = {Shervashidze, Nino and Schweitzer, Pascal and van Leeuwen, Erik Jan and Mehlhorn, Kurt and Borgwardt, Karsten M.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shervashidze et al. - 2011 - Weisfeiler-Lehman Graph Kernels.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2539--2561},
title = {{Weisfeiler-Lehman Graph Kernels}},
volume = {12},
year = {2011}
}
@incollection{Gianniotis2009,
address = {Berlin, Heidelberg},
author = {Gianniotis, Nikolaos and Ti$\backslash$vno, Peter},
booktitle = {Similarity-Based Clustering},
doi = {http://dx.doi.org/10.1007/978-3-642-01805-3_7},
isbn = {978-3-642-01804-6},
pages = {118--137},
publisher = {Springer-Verlag},
title = {{Visualization of Structured Data via Generative Probabilistic Modeling}},
year = {2009}
}
@inproceedings{DBLP:conf/iconip/2004,
booktitle = {ICONIP},
editor = {Pal, Nikhil R and Kasabov, Nikola and Mudi, Rajani K and Pal, Srimanta and Parui, Swapan K},
isbn = {3-540-23931-6},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Neural Information Processing, 11th International Conference, ICONIP 2004, Calcutta, India, November 22-25, 2004, Proceedings}},
volume = {3316},
year = {2004}
}
@article{Guigo2006,
abstract = {In addition to protein-coding information, mRNAs harbor regulatory sequences necessary for appropriate processing of their precursors. Goren et al. (2006) and Wang et al. (2006) explore the diversity of these signals and the rules by which they function.},
author = {Guig{\'{o}}, Roderic and Valc{\'{a}}rcel, Juan},
doi = {10.1016/j.molcel.2006.07.003},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Guig{\'{o}}, Valc{\'{a}}rcel - 2006 - Unweaving the meanings of messenger RNA sequences.pdf:pdf},
issn = {1097-2765},
journal = {Molecular cell},
keywords = {Alternative Splicing,Gene Expression Regulation,RNA Processing, Post-Transcriptional,RNA, Messenger},
month = {jul},
number = {2},
pages = {150--1},
pmid = {16857580},
title = {{Unweaving the meanings of messenger RNA sequences.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16857580},
volume = {23},
year = {2006}
}
@article{Hofacker1989,
abstract = {Computer codes for computation and comparison of RNA secondary structures, the Vienna RNA package, are presented, that are based on dynamic programming algorithms and aim at predictions of structures with minimum free energies as well as at computations of the equilibrium partition functions and base pairing probabilities. An efficient heuristic for the inverse folding problem of RNA is introduced. In addition we present compact and efficient programs for the comparison of RNA secondary structures based on tree editing and alignment. All computer codes are written in ANSI C. They include implementations of modified algorithms on parallel computers with distributed memory. Performance analysis carried out on an Intel Hypercube shows that parallel computing becomes gradually more and more efficient the longer the sequences are.},
author = {Hofacker, I. L. and Fontana, W. and Stadler, P. F. and Bonhoeffer, L. S. and Tacker, M. and Schuster, P.},
doi = {10.1007/BF00818163},
isbn = {0026-9247},
issn = {00269247},
journal = {Monatshefte f??r Chemie Chemical Monthly},
keywords = {Inverse folding,RNA folding,RNA secondary structures,parallel computing,public domain software,tree editing},
pages = {167--188},
title = {{Fast folding and comparison of RNA secondary structures}},
volume = {125},
year = {1989}
}
@inproceedings{Orabona2008,
author = {Orabona, Francesco and Keshet, J and Caputo, Barbara},
booktitle = {ICML '08 Proceedings of the 25th international conference on Machine learning},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Orabona, Keshet, Caputo - 2008 - The projectron a bounded kernel-based perceptron.pdf:pdf},
pages = {720--727},
title = {{The projectron: a bounded kernel-based perceptron}},
url = {http://dl.acm.org/citation.cfm?id=1390247},
year = {2008}
}
@inproceedings{Costa2008,
author = {Costa, Fabrizio and Bringmann, Bj{\"{o}}rn},
booktitle = {ICDM Workshops},
doi = {10.1109/ICDM.Workshops.2008.86},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Costa, Bringmann - 2008 - Towards Combining Structured Pattern Mining and Graph Kernels.pdf:pdf},
isbn = {9780769535036},
pages = {192--201},
title = {{Towards Combining Structured Pattern Mining and Graph Kernels}},
year = {2008}
}
@article{Last2008,
address = {Amsterdam, The Netherlands},
author = {Cohen, L and Avrahami, G and Last, M and Kandel, A},
journal = {App. Soft Comput.},
number = {4},
pages = {1283--1294},
publisher = {Elsevier Science Publishers B. V.},
title = {{Info-fuzzy algorithms for mining dynamic data streams}},
volume = {8},
year = {2008}
}
@article{Kohler2008,
abstract = {The identification of genes associated with hereditary disorders has contributed to improving medical care and to a better understanding of gene functions, interactions, and pathways. However, there are well over 1500 Mendelian disorders whose molecular basis remains unknown. At present, methods such as linkage analysis can identify the chromosomal region in which unknown disease genes are located, but the regions could contain up to hundreds of candidate genes. In this work, we present a method for prioritization of candidate genes by use of a global network distance measure, random walk analysis, for definition of similarities in protein-protein interaction networks. We tested our method on 110 disease-gene families with a total of 783 genes and achieved an area under the ROC curve of up to 98{\%} on simulated linkage intervals of 100 genes surrounding the disease gene, significantly outperforming previous methods based on local distance measures. Our results not only provide an improved tool for positional-cloning projects but also add weight to the assumption that phenotypically similar diseases are associated with disturbances of subnetworks within the larger protein interactome that extend beyond the disease proteins themselves.},
author = {K{\"{o}}hler, Sebastian and Bauer, Sebastian and Horn, Denise and Robinson, Peter N},
doi = {10.1016/j.ajhg.2008.02.013},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{o}}hler et al. - 2008 - Walking the interactome for prioritization of candidate disease genes.pdf:pdf},
issn = {1537-6605},
journal = {American journal of human genetics},
keywords = {Animals,Chromosome Mapping,Chromosome Mapping: methods,Computational Biology,Computational Biology: methods,Databases, Genetic,Genetic Diseases, Inborn,Genetic Diseases, Inborn: genetics,Genetic Linkage,Genetic Predisposition to Disease,Genetic Predisposition to Disease: genetics,Humans,Internet,Mice,Pedigree,Protein Interaction Mapping,Software},
month = {apr},
number = {4},
pages = {949--58},
pmid = {18371930},
title = {{Walking the interactome for prioritization of candidate disease genes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2427257{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {82},
year = {2008}
}
@inproceedings{Lee2001,
author = {Lee, Yuh-jye and Mangasarian, Olvi L},
booktitle = {Data Mining Institute, Computer Sciences Department, University of Wisconsin},
doi = {10.1.1.102.3640},
pages = {1--17},
title = {{RSVM : Reduced Support Vector Machines}},
year = {2001}
}
@article{TINOCO1971,
author = {TINOCO, IGNACIO and UHLENBECK, OLKE C. and LEVINE, MARK D.},
doi = {10.1038/230362a0},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/TINOCO, UHLENBECK, LEVINE - 1971 - Estimation of Secondary Structure in Ribonucleic Acids.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
month = {apr},
number = {5293},
pages = {362--367},
title = {{Estimation of Secondary Structure in Ribonucleic Acids}},
url = {http://www.nature.com/doifinder/10.1038/230362a0},
volume = {230},
year = {1971}
}
@incollection{Gelfand2010,
author = {Gelfand, Andrew and Chen, Yutian and van der Maaten, Laurens and Welling, Max},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {Lafferty, J and Williams, C K I and Shawe-Taylor, J and Zemel, R S and Culotta, A},
pages = {694--702},
title = {{On Herding and the Perceptron Cycling Theorem}},
year = {2010}
}
@article{Li,
author = {Li, Geng and Semerci, Murat and Zaki, Mohammed J},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Li, Semerci, Zaki - Unknown - Effective Graph Classification based on Topological and Label Attributes.pdf:pdf},
pages = {1--24},
title = {{Effective Graph Classification based on Topological and Label Attributes}}
}
@article{Rousseau2015,
abstract = {In this paper, we consider the task of text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part - its main core - speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-Averaged F1-score compared to baseline approaches. {\textcopyright} 2015 Association for Computational Linguistics.},
author = {Rousseau, Francois and Kiagias, Emmanouil and Vazirgiannis, Michalis},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rousseau, Kiagias, Vazirgiannis - 2015 - Text Categorization as a Graph Classification Problem.pdf:pdf},
isbn = {9781941643723},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
pages = {1702--1712},
title = {{Text Categorization as a Graph Classification Problem}},
url = {http://www.aclweb.org/anthology/P15-1164},
year = {2015}
}
@article{Tranchevent2008,
abstract = {Endeavour (http://www.esat.kuleuven.be/endeavourweb; this web site is free and open to all users and there is no login requirement) is a web resource for the prioritization of candidate genes. Using a training set of genes known to be involved in a biological process of interest, our approach consists of (i) inferring several models (based on various genomic data sources), (ii) applying each model to the candidate genes to rank those candidates against the profile of the known genes and (iii) merging the several rankings into a global ranking of the candidate genes. In the present article, we describe the latest developments of Endeavour. First, we provide a web-based user interface, besides our Java client, to make Endeavour more universally accessible. Second, we support multiple species: in addition to Homo sapiens, we now provide gene prioritization for three major model organisms: Mus musculus, Rattus norvegicus and Caenorhabditis elegans. Third, Endeavour makes use of additional data sources and is now including numerous databases: ontologies and annotations, protein-protein interactions, cis-regulatory information, gene expression data sets, sequence information and text-mining data. We tested the novel version of Endeavour on 32 recent disease gene associations from the literature. Additionally, we describe a number of recent independent studies that made use of Endeavour to prioritize candidate genes for obesity and Type II diabetes, cleft lip and cleft palate, and pulmonary fibrosis.},
author = {Tranchevent, L{\'{e}}on-Charles and Barriot, Roland and Yu, Shi and {Van Vooren}, Steven and {Van Loo}, Peter and Coessens, Bert and {De Moor}, Bart and Aerts, Stein and Moreau, Yves},
doi = {10.1093/nar/gkn325},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Tranchevent et al. - 2008 - ENDEAVOUR update a web resource for gene prioritization in multiple species.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Animals,Caenorhabditis elegans,Caenorhabditis elegans: genetics,Drosophila melanogaster,Drosophila melanogaster: genetics,Genes,Genetic Predisposition to Disease,Humans,Internet,Mice,Models, Animal,Rats,Software,Zebrafish,Zebrafish: genetics},
month = {jul},
number = {Web Server issue},
pages = {W377--84},
pmid = {18508807},
title = {{ENDEAVOUR update: a web resource for gene prioritization in multiple species.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2447805{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {36},
year = {2008}
}
@inproceedings{Collins:2001fk,
author = {Collins, Michael and Duffy, Nigel"},
booktitle = {ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS},
doi = {10.1.1.19.8980},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Collins, Duffy - 2001 - Convolution Kernels for Natural Language.pdf:pdf},
pages = {625--632},
title = {{Convolution Kernels for Natural Language}},
volume = {14},
year = {2001}
}
@article{Kolter07,
address = {Cambridge, MA, USA},
author = {Kolter, J Z and Maloof, M A},
journal = {Journ. of Mach. Learn. Res.},
pages = {2755--2790},
publisher = {MIT Press},
title = {{Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts}},
volume = {8},
year = {2007}
}
@article{Harris2008,
abstract = {The growing body of DNA microarray data has the potential to advance
our understanding of the molecular basis of disease. However annotating
microarray datasets with clinically useful information is not always
possible, as this often requires access to detailed patient records.
In this study we introduce GLAD, a new Semi-Supervised Learning (SSL)
method for combining independent annotated datasets and unannotated
datasets with the aim of identifying more robust sample classifiers.In
our method, independent models are developed using subsets of genes
for the annotated and unannotated datasets. These models are evaluated
according to a scoring function that incorporates terms for classification
accuracy on annotated data, and relative cluster separation in unannotated
data. Improved models are iteratively generated using a genetic algorithm
feature selection technique.Our results show that the addition of
unannotated data into training, significantly improves classifier
robustness.},
author = {Harris, Cole and Ghaffari, Noushin},
doi = {10.1186/1471-2164-9-S2-S7},
issn = {1471-2164},
journal = {BMC Genomics},
number = {Suppl 2},
title = {{Biomarker discovery across annotated and unannotated microarray datasets using semi-supervised learning}},
volume = {9},
year = {2008}
}
@article{Griffiths-Jones2005,
abstract = {Rfam is a comprehensive collection of non-coding RNA (ncRNA) families, represented by multiple sequence alignments and profile stochastic context-free grammars. Rfam aims to facilitate the identification and classification of new members of known sequence families, and distributes annotation of ncRNAs in over 200 complete genome sequences. The data provide the first glimpses of conservation of multiple ncRNA families across a wide taxonomic range. A small number of large families are essential in all three kingdoms of life, with large numbers of smaller families specific to certain taxa. Recent improvements in the database are discussed, together with challenges for the future. Rfam is available on the Web at http://www.sanger.ac.uk/Software/Rfam/ and http://rfam.wustl.edu/.},
author = {Griffiths-Jones, Sam and Moxon, Simon and Marshall, Mhairi and Khanna, Ajay and Eddy, Sean R and Bateman, Alex},
doi = {10.1093/nar/gki081},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Griffiths-Jones et al. - 2005 - Rfam annotating non-coding RNAs in complete genomes.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Animals,Base Sequence,Databases, Nucleic Acid,Genome,Humans,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: classification,Sequence Alignment},
month = {jan},
number = {Database issue},
pages = {D121--4},
pmid = {15608160},
title = {{Rfam: annotating non-coding RNAs in complete genomes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=540035{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {33},
year = {2005}
}
@inproceedings{vanvaerenbergh2006sliding,
address = {Toulouse, France},
author = {{Van Vaerenbergh}, Steven and V$\backslash$'ia, Javier and Santamar$\backslash$'ia, Ignacio},
booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2006)},
month = {may},
title = {{A Sliding-Window Kernel {\{}RLS{\}} Algorithm and its Application to Nonlinear Channel Identification}},
year = {2006}
}
@article{Vitale2011,
author = {Vitale, Fabio and Zappella, Giovanni},
journal = {NIPS},
pages = {1--9},
title = {{See the Tree Through the Lines : The Shazoo Algorithm ∗}},
year = {2011}
}
@article{Gartner2002,
author = {G{\"{a}}rtner, Thomas and G, Thomas and Lloyd, John W and Flach, Peter A."},
doi = {10.1.1.7.3622},
isbn = {9789812814555},
journal = {Quality Assurance},
pages = {49--58},
title = {{Kernels for Structured Data}},
volume = {5},
year = {2002}
}
@article{Bianucci2000,
abstract = {We present the application of Cascade Correlation for structures to QSPR (quantitative structure-property relationships) and QSAR (quantitative structure-activity relationships) analysis. Cascade Correlation for structures is a neural network model recently proposed for the processing of structured data. This allows the direct treatment of chemical compounds as labeled trees, which constitutes a novel approach to QSPR/QSAR. We report the results obtained for QSPR on Alkanes (predicting the boiling point) and QSAR of a class of Benzodiazepines. Our approach compares favorably versus the traditional QSAR treatment based on equations and it is competitive with 'ad hoc' MLPs for the QSPR problem.},
author = {Bianucci, Anna Maria and Micheli, ALessio and Sperduti, Alessandro and Starita, Antonina},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bianucci et al. - 2000 - Application of Cascade Correlation Networks for Structures to Chemistry.pdf:pdf},
issn = {0924669X},
journal = {Applied Intelligence},
keywords = {cascade correlation networks,constructive algorithms,gradient descent,qsar,qspr},
number = {1-2},
pages = {117--146},
title = {{Application of Cascade Correlation Networks for Structures to Chemistry}},
volume = {12},
year = {2000}
}
@article{Caetano2009,
abstract = {As a fundamental problem in pattern recognition, graph matching has
applications in a variety of fields, from computer vision to computational
biology. In graph matching, patterns are modeled as graphs and pattern
recognition amounts to finding a correspondence between the nodes
of different graphs. Many formulations of this problem can be cast
in general as a quadratic assignment problem, where a linear term
in the objective function encodes node compatibility and a quadratic
term encodes edge compatibility. The main research focus in this
theme is about designing efficient algorithms for approximately solving
the quadratic assignment problem, since it is {\{}NP-hard.{\}} In this
paper we turn our attention to a different question: how to estimate
compatibility functions such that the solution of the resulting graph
matching problem best matches the expected solution that a human
would manually provide. We present a method for learning graph matching:
the training examples are pairs of graphs and the 'labels' are matches
between them. Our experimental results reveal that learning can substantially
improve the performance of standard graph matching algorithms. In
particular, we find that simple linear assignment with such a learning
scheme outperforms Graduated Assignment with bistochastic normalisation,
a state-of-the-art quadratic assignment relaxation algorithm.},
annote = {Complete {\{}PDF{\}} document was either not available or accessible. Please
make sure you{\&}{\#}039;re logged in to the digital library to retrieve the
complete {\{}PDF{\}} document.},
author = {Caetano, Tiberio S and McAuley, Julian J and Cheng, Li and Le, Quoc V and Smola, Alex J},
doi = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2009.28},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {graph{\_}matching,learning,optimization,structured{\_}estimation,support vector machines},
number = {6},
pages = {1048--1058},
title = {{Learning Graph Matching}},
volume = {31},
year = {2009}
}
@inproceedings{Wang2011,
author = {Wang, Zhuang and Djuric, Nemanja and Vucetic, Slobodan and Crammer, Koby},
booktitle = {17th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2011 - Trading Representability for Scalability Adaptive Multi-Hyperplane Machine for Nonlinear Classification.pdf:pdf},
isbn = {9781450308137},
keywords = {dient descent,large-scale learning,nonlinear classification,stochastic gra-,support vector machines},
title = {{Trading Representability for Scalability : Adaptive Multi-Hyperplane Machine for Nonlinear Classification}},
year = {2011}
}
@article{Weeks2010,
abstract = {RNA is arguably the most versatile biological macromolecule because of its ability both to encode and to manipulate genetic information. The diverse roles of RNA depend on its ability to fold back on itself to form biologically functional structures that bind small molecule and large protein ligands, to change conformation, and to affect the cellular regulatory state. These features of RNA biology can be structurally interrogated using chemical mapping experiments. The usefulness and applications of RNA chemical probing technologies have expanded dramatically over the past five years because of several critical advances. These innovations include new sequence-independent RNA chemistries, algorithmic tools for high-throughput analysis of complex data sets composed of thousands of measurements, new approaches for interpreting chemical probing data for both secondary and tertiary structure prediction, facile methods for following time-dependent processes, and the willingness of individual research groups to tackle increasingly bold problems in RNA structural biology.},
author = {Weeks, Kevin M},
doi = {10.1016/j.sbi.2010.04.001},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Weeks - 2010 - Advances in RNA structure analysis by chemical probing.pdf:pdf},
issn = {1879-033X},
journal = {Current opinion in structural biology},
keywords = {Animals,Base Sequence,Molecular Sequence Data,Nucleic Acid Conformation,RNA,RNA: analysis,RNA: chemistry,RNA: genetics,Software},
month = {jun},
number = {3},
pages = {295--304},
pmid = {20447823},
publisher = {Elsevier Ltd},
title = {{Advances in RNA structure analysis by chemical probing.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2916962{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {20},
year = {2010}
}
@article{DeOliveiraOliveira2005,
author = {{de Oliveira Oliveira}, Mateus and Greve, Fab{\'{i}}ola},
doi = {10.1016/j.endm.2005.05.050},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/de Oliveira Oliveira, Greve - 2005 - A new refinement procedure for graph isomorphism algorithms.pdf:pdf},
issn = {15710653},
journal = {Electronic Notes in Discrete Mathematics},
keywords = {graph isomorphism,refinement procedure,vertex classification},
month = {jun},
pages = {373--379},
title = {{A new refinement procedure for graph isomorphism algorithms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1571065305050493},
volume = {19},
year = {2005}
}
@inproceedings{Vishwanathan:uq,
author = {Vishwanathan, S.v.n. and Borgwardt, Karsten M and Schraudolph, Nicol N},
booktitle = {Advances in neural information processing systems 19},
editor = {Hoffman, B. Sch$\backslash$"{\{}o{\}}lkopf and J.C. Platt and T.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vishwanathan, Borgwardt, Schraudolph - 2007 - Fast computation of graph kernels.pdf:pdf},
pages = {1449----1456},
publisher = {MIT Press},
title = {{Fast computation of graph kernels}},
year = {2007}
}
@inproceedings{Kriegel05shortestpath,
abstract = {Data mining algorithms are facing the challenge to deal with an increasing number of complex objects. For graph data, a whole toolbox of data mining algorithms becomes available by defining a kernel function on instances of graphs. Graph kernels based on walks, subtrees and cycles in graphs have been proposed so far. As a general problem, these kernels are either computationally expensive or limited in their expressiveness. We try to overcome this problem by defining expressive graph kernels which are based on paths. As the computation of all paths and longest paths in a graph is {\{}NP-hard,{\}} we propose graph kernels based on shortest paths. These kernels are computable in polynomial time, retain expressivity and are still positive definite. In experiments on classification of graph models of proteins, our shortest-path kernels show significantly higher classificationaccuracy than walk-based kernels.},
address = {Los Alamitos, CA, USA},
annote = {
        

        From Duplicate 2 ( 
        

        
        

        
          

          Shortest-Path Kernels on Graphs
          

        
        

        
        

         - Borgwardt, Karsten M; Kriegel, Hans-Peter )

          

        
        

        

        

        

        

        

        

      },
author = {Borgwardt, K.M. and Kriegel, Hans-Peter},
booktitle = {Fifth IEEE International Conference on Data Mining (ICDM'05)},
doi = {10.1109/ICDM.2005.132},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Borgwardt, Kriegel - 2005 - Shortest-Path Kernels on Graphs.pdf:pdf},
isbn = {0-7695-2278-5},
issn = {1550-4786},
pages = {74--81},
publisher = {IEEE},
title = {{Shortest-Path Kernels on Graphs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565664},
volume = {0},
year = {2005}
}
@article{Hizukuri2004,
abstract = {Glycans, which are carbohydrate sugar chains attached to some lipids
or proteins, have a huge variety of structures and play a key role
in cell communication, protein interaction and immunity. The availability
of a number of glycan structures stored in the {\{}KEGG/GLYCAN{\}} database
makes it possible for us to conduct a large-scale comparative research
of glycans. In this paper, we present a novel approach to compare
glycan structures and extract characteristic glycan substructures
of certain organisms. In the algorithm we developed a new similarity
measure of glycan structures taking into account of several biological
aspects of glycan synthesis and glycosyltransferases, and we confirmed
the validity of our similarity measure by conducting experiments
on its ability to classify glycans between organisms in the framework
of a support vector machine. Finally, our method successfully extracted
a set of candidates of substructrues which are characteristic to
human, rat, mouse, bovine, pig, chicken, yeast, wheat and sycamore,
respectively. We confirmed that the characteristic substructures
extracted by our method correspond to the substructures which are
known as the species-specific sugar chain of gamma-glutamyltranspeptidases
in the kidney.},
annote = {{\{}PMID:{\}} 15712111},
author = {Hizukuri, Yoshiyuki and Yamanishi, Yoshihiro and Hashimoto, Kosuke and Kanehisa, Minoru},
issn = {0919-9454},
journal = {Genome Informatics. International Conference on Genome Informatics},
number = {1},
pages = {69--81},
title = {{Extraction of species-specific glycan substructures}},
volume = {15},
year = {2004}
}
@article{MartinoS10,
author = {{Da San Martino}, Giovanni and Sperduti, Alessandro},
journal = {IEEE Comp. Int. Mag.},
number = {1},
pages = {42--49},
title = {{Mining Structured Data}},
volume = {5},
year = {2010}
}
@article{Verbeke2014,
author = {Verbeke, Mathias and Frasconi, Paolo and {De Grave}, Kurt and Costa, Fabrizio and {De Raedt}, Luc},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Verbeke et al. - 2014 - kLogNLP Graph kernel–based relational learning of natural language.pdf:pdf},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
pages = {85--90},
title = {{kLogNLP: Graph kernel–based relational learning of natural language}},
url = {https://lirias.kuleuven.be/handle/123456789/451228},
year = {2014}
}
@article{Zhang2005,
author = {Zhang, Tong},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang - 2005 - Learning bounds for kernel regression using effective data dimensionality.pdf:pdf},
journal = {Neural Computation},
pages = {1--16},
title = {{Learning bounds for kernel regression using effective data dimensionality}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/0899766054323008},
year = {2005}
}
@article{Washio:ys,
author = {Washio, T T and Washio, T},
doi = {10.1145/959242.959249},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Washio, Washio - Unknown - State of the art of graph-based data mining.pdf:pdf},
title = {{State of the art of graph-based data mining}}
}
@article{Hastie2004,
abstract = {The support vector machine (SVM) is a widely used tool for classification. Many efficient implementations exist for fitting a two-class SVM model. The user has to supply values for the tuning parameters: the regularization cost parameter, and the kernel parameters. It seems a common practice is to use a default value for the cost parameter, often leading to the least restrictive model. In this paper we argue that the choice of the cost parameter can be critical. We then derive an algorithm that can fit the entire path of SVM solutions for every value of the cost parameter, with essentially the same computational cost as fitting one SVM model. We illustrate our algorithm on some examples, and use our representation to give further insight into the range of SVM solutions.},
author = {Hastie, Trevor and Rosset, Saharon and Tibshirani, Robert and Zhu, Ji},
doi = {10.1145/347090.347165},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hastie et al. - 2004 - The Entire Regularization Path for the Support Vector Machine.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {coefficient path,regularization,support vector machines},
number = {2},
pages = {1391--1415},
pmid = {17354896},
title = {{The Entire Regularization Path for the Support Vector Machine}},
url = {http://portal.acm.org/citation.cfm?id=1005332.1044706},
volume = {5},
year = {2004}
}
@article{Liebscher2012,
author = {Liebscher, Steffen and Kirschstein, Thomas and Becker, Claudia},
doi = {10.1007/s11222-012-9337-5},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {Breakdown point; Delaunay triangulation; Minimum c},
pages = {1--12},
publisher = {Springer US},
title = {{RDELA—a Delaunay-triangulation-based, location and covariance estimator with high breakdown point}},
url = {http://dx.doi.org/10.1007/s11222-012-9337-5},
year = {2012}
}
@inproceedings{Gama2006,
address = {Dijon, France},
author = {Gama, J and Pinto, C},
booktitle = {Proc. of the 2006 ACM symposium on Applied computing (SAC'06)},
pages = {662--667},
title = {{Discretization from data streams: applications to histograms and data mining}},
year = {2006}
}
@article{Kimura2012,
author = {Kimura, Daisuke and Kashima, Hisashi},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kimura, Kashima - 2012 - Fast Computation of Subpath Kernel for Trees.pdf:pdf},
title = {{Fast Computation of Subpath Kernel for Trees}},
year = {2012}
}
@article{Camastra2008,
author = {Camastra, Francesco and Petrosino, Alfredo},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Camastra, Petrosino - 2008 - Kernel Methods for Graphs A Comprehensive Approach.pdf:pdf},
pages = {662--669},
title = {{Kernel Methods for Graphs: A Comprehensive Approach}},
year = {2008}
}
@article{Vens,
author = {Vens, Celine and Costa, Fabrizio},
doi = {10.1109/ICDM.2011.121},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vens, Costa - 2011 - Random Forest Based Feature Induction.pdf:pdf;:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vens, Costa - 2011 - Random Forest Based Feature Induction(2).pdf:pdf},
isbn = {978-1-4577-2075-8},
journal = {2011 IEEE 11th International Conference on Data Mining},
keywords = {-feature induction,random forests},
month = {dec},
pages = {744--753},
publisher = {Ieee},
title = {{Random Forest Based Feature Induction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6137279},
year = {2011}
}
@article{Cawley2007,
author = {Cawley, GC and Talbot, NLC},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cawley, Talbot - 2007 - Preventing over-fitting during model selection via Bayesian regularisation of the hyper-parameters.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {bayesian regularisation,kernel methods,model selection},
pages = {841--861},
title = {{Preventing over-fitting during model selection via Bayesian regularisation of the hyper-parameters}},
url = {http://dl.acm.org/citation.cfm?id=1248690},
volume = {8},
year = {2007}
}
@article{Burattin2014a,
abstract = {Process Mining represents an important research field that connects Business Process Modeling and Data Mining. One of the most prominent task of Process Mining is the discovery of a control-flow starting from event logs. This paper focuses on the important problem of control-flow discovery starting from a stream of event data. We propose to adapt Heuristics Miner, one of the most effective control-flow discovery algorithms, to the treatment of streams of event data. Two adaptations, based on Lossy Counting and Lossy Counting with Budget, as well as a sliding window based version of Heuristics Miner, are proposed and experimentally compared against both artificial and real streams. Experimental results show the effectiveness of control-flow discovery algorithms for streams on artificial and real datasets.},
author = {Burattin, Andrea and Sperduti, Alessandro and van der Aalst, Wil M. P.},
doi = {10.1109/CEC.2014.6900341},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Burattin, Sperduti, van der Aalst - 2014 - Control-flow discovery from event streams.pdf:pdf},
isbn = {978-1-4799-1488-3},
journal = {2014 IEEE Congress on Evolutionary Computation (CEC)},
keywords = {Business,Computational modeling,Data mining,Data structures,Educational institutions,Frequency measurement,Heuristic algorithms,business data processing,business process modeling,control-flow discovery algorithms,data mining,event streams,heuristics miner,lossy counting with budget,process mining,sliding window based version},
pages = {2420--2427},
title = {{Control-flow discovery from event streams}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6900341},
year = {2014}
}
@book{Quinlan93,
address = {San Francisco, CA},
author = {Quinlan, J R},
publisher = {Morgan Kaufmann},
title = {{C4.5: Programs for Machine Learning}},
year = {1993}
}
@article{Yan2002,
abstract = { We investigate new approaches for frequent graph-based pattern mining in graph datasets and propose a novel algorithm called gSpan (graph-based substructure pattern mining), which discovers frequent substructures without candidate generation. gSpan builds a new lexicographic order among graphs, and maps each graph to a unique minimum DFS code as its canonical label. Based on this lexicographic order gSpan adopts the depth-first search strategy to mine frequent connected subgraphs efficiently. Our performance study shows that gSpan substantially outperforms previous algorithms, sometimes by an order of magnitude.},
author = {Yan, Xifeng Yan Xifeng and Han, Jiawei Han Jiawei},
doi = {10.1109/ICDM.2002.1184038},
isbn = {0-7695-1754-4},
issn = {00223751},
journal = {2002 IEEE International Conference on Data Mining, 2002. Proceedings.},
keywords = {Cited,DFS,Data Mining,Folder - 1.4 - Subgraph,Folder - 3.1 - Fragment Miners,Folder - theory - artificial intelligence - patter,Han Jiawei,ICDM,Influencial paper,Isomorphism,R,Relational,algorithm,algorithm-,arrm,canonical,code-,connected,data,datasets-,depth-first,discovery-,fpqm,frequent,frequent subgraph mining,gSpan-,grant,graph,graph mining,graph mining algorithm,graph-based,gspan,kun,label-,lexicographic,minimum,mining-,motif,order-,pattern,pattern mining,performance,read,search,searching-,strategy-,study-,subgraph,substructure,tree,unique},
mendeley-tags = {Cited,DFS,Data Mining,Folder - 1.4 - Subgraph,Folder - 3.1 - Fragment Miners,Folder - theory - artificial intelligence - patter,Han Jiawei,ICDM,Influencial paper,Isomorphism,R,Relational,algorithm,algorithm-,arrm,canonical,code-,connected,data,datasets-,depth-first,discovery-,fpqm,frequent,frequent subgraph mining,gSpan-,grant,graph,graph mining,graph mining algorithm,graph-based,gspan,kun,label-,lexicographic,minimum,mining-,motif,order-,pattern,pattern mining,performance,read,search,searching-,strategy-,study-,subgraph,substructure,tree,unique},
pmid = {16992075},
title = {{gSpan: graph-based substructure pattern mining}},
year = {2002}
}
@phdthesis{chu05,
author = {Chu, Fang},
school = {Supervisor Prof. Carlo Zaniolo, University of California},
title = {{Mining Techniques for Data Streams and Sequences}},
year = {2005}
}
@article{Ambroladze2004,
author = {Ambroladze, Amiran and Shawe-taylor, John},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ambroladze, Shawe-taylor - 2004 - Complexity of Pattern Classes and Lipschitz Property.pdf:pdf},
pages = {1--14},
title = {{Complexity of Pattern Classes and Lipschitz Property}},
year = {2004}
}
@article{GartnerTC.Garriga2006,
author = {{Gartner, T C. Garriga}, G and Meinl, T},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gartner, T C. Garriga, Meinl - 2006 - Mlg 2006.pdf:pdf},
journal = {Proceedings of the International Workshop on Mining and Learning with Graphs},
number = {September},
title = {{Mlg 2006}},
year = {2006}
}
@article{Kashima04,
address = {New York, New York, USA},
author = {Kashima, Hisashi and Tsuboi, Yuta},
doi = {10.1145/1015330.1015383},
isbn = {1581138285},
journal = {Twenty-first international conference on Machine learning - ICML '04},
pages = {58},
publisher = {ACM Press},
title = {{Kernel-based discriminative learning algorithms for labeling sequences, trees, and graphs}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015383 http://portal.acm.org/ft{\_}gateway.cfm?id=1015383{\&}type=pdf{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=87959688{\&}CFTOKEN=58794955},
year = {2004}
}
@inproceedings{GrossiS11,
author = {Grossi, Valerio and Sperduti, Alessandro},
booktitle = {IJCAI},
pages = {1281--1287},
title = {{Kernel-Based Selective Ensemble Learning for Streams of Trees}},
year = {2011}
}
@inproceedings{Birattari2002,
address = {London, UK},
author = {Birattari, Mauro and Caro, Gianni Di and Dorigo, Marco},
booktitle = {ANTS '02: Proceedings of the Third International Workshop on Ant Algorithms},
isbn = {3-540-44146-8},
pages = {188--201},
publisher = {Springer-Verlag},
title = {{Toward the Formal Foundation of Ant Programming}},
year = {2002}
}
@article{Sydorov2014,
author = {Sydorov, Vladyslav and Sakurada, Mayu and Lampert, Christoph H},
doi = {10.1109/CVPR.2014.182},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sydorov, Sakurada, Lampert - 2014 - Deep Fisher Kernels – End to End Learning of the Fisher Kernel GMM Parameters.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr},
pages = {1402--1409},
title = {{Deep Fisher Kernels – End to End Learning of the Fisher Kernel GMM Parameters}},
year = {2014}
}
@inproceedings{Bach,
author = {Bach, Francis},
booktitle = {In Proc. CVPR},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bach - 2007 - Image Classification with Segmentation Graph Kernels e.pdf:pdf},
title = {{Image Classification with Segmentation Graph Kernels e}},
year = {2007}
}
@inproceedings{Babcock2002,
address = {Madison, WI},
author = {Babcock, B and Babu, S and Datar, M and Motwani, R and Widom, J},
booktitle = {Proc. of the 21st ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS'02)},
pages = {1--16},
title = {{Models and issues in data stream systems}},
year = {2002}
}
@article{Gartner2004,
author = {G{\"{a}}rtner, Thomas T and Lloyd, John W. and Flach, Peter a.},
doi = {10.1023/B:MACH.0000039777.23772.30},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {higher-order logic,inductive logic programming,instance-based learning,kernel methods,structured data},
month = {dec},
number = {3},
pages = {205--232},
title = {{Kernels and Distances for Structured Data}},
url = {http://www.springerlink.com/openurl.asp?id=doi:10.1023/B:MACH.0000039777.23772.30},
volume = {57},
year = {2004}
}
@article{10.1109/ICPR.2006.57,
address = {Los Alamitos, CA, USA},
author = {Neuhaus, Michel and Bunke, Horst},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICPR.2006.57},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neuhaus, Bunke - 2006 - A Convolution Edit Kernel for Error-tolerant Graph Matching.pdf:pdf},
issn = {1051-4651},
journal = {Pattern Recognition, International Conference on},
pages = {220--223},
publisher = {IEEE Computer Society},
title = {{A Convolution Edit Kernel for Error-tolerant Graph Matching}},
volume = {4},
year = {2006}
}
@article{Furlanello2005,
abstract = {Class prediction and feature selection are two learning tasks that
are strictly paired in the search of molecular profiles from microarray
data. Researchers have become aware how easy it is to incur a selection
bias effect, and complex validation setups are required to avoid
overly optimistic estimates of the predictive accuracy of the models
and incorrect gene selections. This paper describes a semisupervised
pattern discovery approach that uses the by-products of complete
validation studies on experimental setups for gene profiling. In
particular, we introduce the study of the patterns of single sample
responses (sample-tracking profiles) to the gene selection process
induced by typical supervised learning tasks in microarray studies.
We originate sample-tracking profiles as the aggregated off-training
evaluation of {\{}SVM{\}} models of increasing gene panel sizes. Genes
are ranked by {\{}E-RFE,{\}} an entropy-based variant of the recursive
feature elimination for support vector machines {\{}(RFE-SVM).{\}} A dynamic
time warping {\{}(DTW){\}} algorithm is then applied to define a metric
between sample-tracking profiles. An unsupervised clustering based
on the {\{}DTW{\}} metric allows automating the discovery of outliers and
of subtypes of different molecular profiles. Applications are described
on synthetic data and in two gene expression studies.},
author = {Furlanello, Cesare and Serafini, Maria and Merler, Stefano and Jurman, Giuseppe},
doi = {10.1109/TCBB.2005.28},
issn = {1545-5963},
journal = {{\{}IEEE/ACM{\}} Transactions on Computational Biology and Bioinformatics},
number = {2},
pages = {110--118},
title = {{Semisupervised learning for molecular profiling}},
volume = {2},
year = {2005}
}
@inproceedings{DBLP:conf/nips/2004,
booktitle = {NIPS},
title = {{Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada]}},
year = {2004}
}
@article{Anguita2010,
author = {Anguita, Davide and Ghio, Alessandro and Greco, Noemi and Oneto, Luca and Ridella, Sandro},
doi = {10.1109/IJCNN.2010.5596450},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Anguita et al. - 2010 - Model selection for support vector machines Advantages and disadvantages of the Machine Learning Theory.pdf:pdf},
isbn = {978-1-4244-6916-1},
journal = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
month = {jul},
pages = {1--8},
publisher = {Ieee},
title = {{Model selection for support vector machines: Advantages and disadvantages of the Machine Learning Theory}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5596450},
year = {2010}
}
@article{Kayala,
author = {Kayala, Matthew A and Baldi, Pierre},
pages = {1--9},
title = {{A Machine Learning Approach to Predict Chemical Reactions}}
}
@article{Shin2008,
author = {Shin, Kilho and Mellon, Carnegie and Kuboyama, Tetsuji},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shin, Kuboyama - 2008 - A generalization of Haussler's convolution kernel mapping kernel.pdf:pdf},
journal = {Machine Learning},
title = {{A Generalization of Haussler ' s Convolution Kernel — Mapping Kernel}},
year = {2008}
}
@article{Russell2008,
author = {Russell, BC and Torralba, Antonio},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Russell, Torralba - 2008 - LabelMe a database and web-based tool for image annotation.pdf:pdf},
journal = {International journal of {\ldots}},
number = {1-3},
pages = {157--173},
title = {{LabelMe: a database and web-based tool for image annotation}},
url = {http://www.springerlink.com/index/76X9J562653K0378.pdf},
volume = {77},
year = {2008}
}
@book{Cristianini2000,
abstract = {This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software make it an ideal starting point for further study.},
author = {Cristianini, Nello and Shawe-Taylor, John},
booktitle = {booksgooglecom},
doi = {10.1525/jer.2008.3.1.toc},
isbn = {0521780195},
issn = {15562646},
pages = {204},
pmid = {19385776},
title = {{An Introduction to Support Vector Machines and Other Kernel-based Learning Methods}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0521780195},
volume = {1},
year = {2000}
}
@article{nebel:jair-2000,
author = {Nebel, Bernhard},
journal = {Journal of Artificial Intelligence Research},
pages = {271--315},
title = {{On the compilability and expressive power of propositional planning formalisms}},
volume = {12},
year = {2000}
}
@article{Cristianini2002,
author = {Cristianini, Nello and Shawe-Taylor, John and Lodhi, Huma},
journal = {J. Intell. Inf. Syst.},
number = {2-3},
pages = {127--152},
title = {{Latent Semantic Kernels}},
volume = {18},
year = {2002}
}
@article{Elzinga2012,
author = {Elzinga, Cees H. and Wang, Hui},
doi = {10.1016/j.patrec.2012.07.017},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Elzinga, Wang - 2012 - Kernels for acyclic digraphs.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
month = {dec},
number = {16},
pages = {2239--2244},
publisher = {Elsevier B.V.},
title = {{Kernels for acyclic digraphs}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865512002395},
volume = {33},
year = {2012}
}
@inproceedings{Tsymbal04,
author = {Tsymbal, A},
booktitle = {Technical Report TCD-CS-2004-15, Computer Science Department, {\{}$\backslash$normalfont{\{}Trinity College{\}}, Dublin, Ireland{\}}},
title = {{The problem of concept drift: definitions and related work}},
year = {2004}
}
@article{Micheli2005,
annote = {Trends in Neurocomputing: 12th European Symposium on Artificial Neural
Networks 2004},
author = {Micheli, Alessio and Portera, Filippo and Sperduti, Alessandro},
doi = {DOI: 10.1016/j.neucom.2004.11.013},
issn = {0925-2312},
journal = {Neurocomputing},
pages = {73--92},
title = {{A preliminary empirical comparison of recursive neural networks and tree kernel methods on regression tasks for tree structured domains}},
volume = {64},
year = {2005}
}
@article{Douglas2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1101.5211v1},
author = {Douglas, BL},
eprint = {arXiv:1101.5211v1},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Douglas - 2011 - The Weisfeiler-Lehman Method and Graph Isomorphism Testing.pdf:pdf},
journal = {arXiv preprint arXiv:1101.5211},
pages = {1--43},
title = {{The Weisfeiler-Lehman Method and Graph Isomorphism Testing}},
url = {http://arxiv.org/abs/1101.5211},
year = {2011}
}
@inproceedings{Chu04,
address = {Sydney, Australia},
author = {Chu, F and Zaniolo, C},
booktitle = {Proc. of the 8th Pacific-Asia Conf. Advances in Knowl. Disc. and Data Mining (PAKDD'04)},
pages = {282--292},
title = {{Fast and light boosting for adaptive mining of data streams}},
year = {2004}
}
@article{Ganian,
author = {Ganian, Robert and Mendez, Patrice Ossona De and Ramadurai, Reshma},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ganian, Mendez, Ramadurai - Unknown - When Trees Grow Low Shrubs and Fast MSO 1.pdf:pdf},
pages = {1--22},
title = {{When Trees Grow Low: Shrubs and Fast MSO 1}}
}
@article{Gori,
author = {Gori, M. and Maggini, M. and Sarti, L.},
doi = {10.1109/IJCNN.2003.1223892},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gori, Maggini, Sarti - Unknown - A recursive neural network model for processing directed acyclic graphs with labeled edges.pdf:pdf},
isbn = {0-7803-7898-9},
journal = {Proceedings of the International Joint Conference on Neural Networks, 2003.},
pages = {1351--1355},
publisher = {Ieee},
title = {{A recursive neural network model for processing directed acyclic graphs with labeled edges}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1223892}
}
@article{Teixeira2012,
author = {Teixeira, Chc and Silva, Arlei and Jr, Wagner Meira},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Teixeira, Silva, Jr - 2012 - Min-Hash Fingerprints for Graph Kernels A Trade-off among Accuracy, Efficiency, and Compression.pdf:pdf},
journal = {Journal of Information and {\ldots}},
keywords = {databases,graphs,indexing,kernel,querying},
number = {3},
pages = {227--242},
title = {{Min-Hash Fingerprints for Graph Kernels: A Trade-off among Accuracy, Efficiency, and Compression}},
url = {http://seer.lcc.ufmg.br/index.php/jidm/article/view/199},
volume = {3},
year = {2012}
}
@inproceedings{Moschitti2006,
abstract = {In recent years tree kernels have been proposed for the automatic
learning of natural language applications. Unfortunately, they show
(a) an inherent super linear complexity and (b) a lower accuracy
than traditional attribute/value methods.},
author = {Moschitti, Alessandro},
booktitle = {Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics},
title = {{Making Tree Kernels Practical for Natural Language Learning}},
year = {2006}
}
@article{FanChung12112007,
abstract = {The concept of pagerank was first started as a way for determining the ranking of Web pages by Web search engines. Based on relations in interconnected networks, pagerank has become a major tool for addressing fundamental problems arising in general graphs, especially for large information networks with hundreds of thousands of nodes. A notable notion of pagerank, introduced by Brin and Page and denoted by PageRank, is based on random walks as a geometric sum. In this paper, we consider a notion of pagerank that is based on the (discrete) heat kernel and can be expressed as an exponential sum of random walks. The heat kernel satisfies the heat equation and can be used to analyze many useful properties of random walks in a graph. A local Cheeger inequality is established, which implies that, by focusing on cuts determined by linear orderings of vertices using the heat kernel pageranks, the resulting partition is within a quadratic factor of the optimum. This is true, even if we restrict the volume of the small part separated by the cut to be close to some specified target value. This leads to a graph partitioning algorithm for which the running time is proportional to the size of the targeted volume (instead of the size of the whole graph).
},
author = {Chung, Fan},
doi = {10.1073/pnas.0708838104},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chung - 2007 - The heat kernel as the pagerank of a graph.pdf:pdf},
journal = {Proceedings of the National Academy of Sciences},
number = {50},
pages = {19735--19740},
title = {{The heat kernel as the pagerank of a graph}},
url = {http://www.pnas.org/content/104/50/19735.abstract},
volume = {104},
year = {2007}
}
@inproceedings{Zhang2004,
abstract = {Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classi cation, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both ecient and very simple to implement.},
author = {Zhang, Tong},
booktitle = {ICML 2004},
doi = {10.1.1.58.7},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang - 2004 - Solving Large Scale Linear Prediction Problems Using Stochastic.pdf:pdf},
pages = {919--926},
title = {{Solving Large Scale Linear Prediction Problems Using Stochastic}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377},
year = {2004}
}
@inproceedings{Cao2007,
abstract = {We address the problem of feature selection in a kernel space to select
the most discriminative and informative features for classification
and data analysis. This is a difficult problem because the dimension
of a kernel space may be infinite. In the past, little work has been
done on feature selection in a kernel space. To solve this problem,
we derive a basis set in the kernel space as a first step for feature
selection. Using the basis set, we then extend the margin-based feature
selection algorithms that are proven effective even when many features
are dependent. The selected features form a subspace of the kernel
space, in which different state-of-the-art classification algorithms
can be applied for classification. We conduct extensive experiments
over real and simulated data to compare our proposed method with
four baseline algorithms. Both theoretical analysis and experimental
results validate the effectiveness of our proposed method.},
address = {Corvalis, Oregon},
author = {Cao, Bin and Shen, Dou and Sun, Jian-Tao and Yang, Qiang and Chen, Zheng},
booktitle = {Proceedings of the 24th international conference on Machine learning},
doi = {10.1145/1273496.1273512},
isbn = {978-1-59593-793-3},
pages = {121--128},
publisher = {ACM},
title = {{Feature selection in a kernel space}},
year = {2007}
}
@article{Mahe2009,
abstract = {Support vector machines and kernel methods have recently gained considerable
attention in chemoinformatics. They offer generally good performance
for problems of supervised classification or regression, and provide
a flexible and computationally efficient framework to include relevant
information and prior knowledge about the data and problems to be
handled. In particular, with kernel methods molecules do not need
to be represented and stored explicitly as vectors or fingerprints,
but only to be compared to each other through a comparison function
technically called a kernel. While classical kernels can be used
to compare vector or fingerprint representations of molecules, completely
new kernels were developed in the recent years to directly compare
the {\{}2D{\}} or {\{}3D{\}} structures of molecules, without the need for an
explicit vectorization step through the extraction of molecular descriptors.
While still in their infancy, these approaches have already demonstrated
their relevance on several toxicity prediction and structure-activity
relationship problems.},
annote = {{\{}PMID:{\}} 19442068},
author = {Mah{\'{e}}, Pierre and Vert, Jean-Philippe},
issn = {1875-5402},
journal = {Combinatorial Chemistry {\&} High Throughput Screening},
month = {may},
number = {4},
pages = {409--423},
title = {{Virtual screening with support vector machines and structure kernels}},
volume = {12},
year = {2009}
}
@article{Debnath1991,
annote = {From Duplicate 1 ( Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. - K. Debnath R. L. Lopez de Compadre, G Debnath A J Shusterman; Hansch, C )
},
author = {Debnath, Asim Kumar and {Lopez de Compadre}, Rosa L. and Debnath, Gargi and Shusterman, Alan J. and Hansch, Corwin},
doi = {10.1021/jm00106a046},
issn = {0022-2623},
journal = {Journal of Medicinal Chemistry},
month = {feb},
number = {2},
pages = {786--797},
title = {{Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity}},
url = {http://pubs.acs.org/doi/abs/10.1021/jm00106a046},
volume = {34},
year = {1991}
}
@article{Cortes2010,
author = {Cortes, Corinna},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cortes - 2010 - Generalization bounds for learning kernels.pdf:pdf},
journal = {Proceedings of the {\ldots}},
title = {{Generalization bounds for learning kernels}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2010{\_}CortesMR10a.pdf},
year = {2010}
}
@article{Bottou1998,
author = {Bottou, L},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bottou - 1998 - Online learning and stochastic approximations.pdf:pdf},
journal = {On-line learning in neural networks},
title = {{Online learning and stochastic approximations}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=iu2v6C5nx4oC{\&}oi=fnd{\&}pg=PA9{\&}dq=Online+Learning+and+Stochastic+Approximations{\&}ots=oxNaUzeOV8{\&}sig=HSOyXY0Ll55JrSdaUzhYSKbAAC4},
year = {1998}
}
@inproceedings{Kimura2011,
author = {Kimura, Daisuke and Kuboyama, Tetsuji and Shibuya, Tetsuo and Kashima, Hisashi},
booktitle = {In Proceedings of the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), Shenzeng, China},
title = {{A Subpath Kernel for Rooted Unordered Trees}},
year = {2011}
}
@book{Cook2006,
abstract = {This text takes a focused and comprehensive look at mining data represented
as a graph, with the latest findings and applications in both theory
and practice provided. Even if you have minimal background in analyzing
graph data, with this book you???ll be able to represent data as
graphs, extract patterns and concepts from the data, and apply the
methodologies presented in the text to real datasets.

There is a misprint with the link to the accompanying Web page for
this book. For those readers who would like to experiment with the
techniques found in this book or test their own ideas on graph data,
the Web page for the book should be {\{}http://www.eecs.wsu.edu/MGD.{\}}},
author = {Cook, Diane J and Holder, Lawrence B},
month = {nov},
pages = {500},
publisher = {Wiley-Interscience},
title = {{Mining Graph Data}},
year = {2006}
}
@article{Aittokallio2006,
abstract = {Availability of large-scale experimental data for cell biology is
enabling computational methods to systematically model the behaviour
of cellular networks. This review surveys the recent advances in
the field of graph-driven methods for analysing complex cellular
networks. The methods are outlined on three levels of increasing
complexity, ranging from methods that can characterize global or
local structural properties of networks to methods that can detect
groups of interconnected nodes, called motifs or clusters, potentially
involved in common elementary biological functions. We also briefly
summarize recent approaches to data integration and network inference
through graph-based formalisms. Finally, we highlight some challenges
in the field and offer our personal view of the key future trends
and developments in graph-based analysis of large-scale datasets.},
author = {Aittokallio, Tero and Schwikowski, Benno},
doi = {10.1093/bib/bbl022},
journal = {Brief Bioinform},
number = {3},
pages = {243--255},
title = {{Graph-based methods for analysing networks in cell biology}},
volume = {7},
year = {2006}
}
@article{Martino2015,
abstract = {Kernel methods are considered an effective technique for on-line learning. Many approaches have been developed for compactly representing the dual solution of a kernel method when the problem imposes memory constraints. However, in literature no work is specifically tailored to streams of graphs. Motivated by the fact that the size of the feature space representation of many state-of-the-art graph kernels is relatively small and thus it is explicitly computable, we study whether executing kernel algorithms in the feature space can be more effective than the classical dual approach. We propose three different algorithms and various strategies for managing the budget. Efficiency and efficacy of the proposed approaches are experimentally assessed on relatively large graph streams exhibiting concept drift. It turns out that, when strict memory budget constraints have to be enforced, working in feature space, given the current state of the art on graph kernels, is more than a viable alternative to dual approaches, both in terms of speed and classification performance.},
archivePrefix = {arXiv},
arxivId = {1507.02158},
author = {Martino, Giovanni Da San and Navarin, Nicol{\`{o}} and Sperduti, Alessandro},
eprint = {1507.02158},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Martino, Navarin, Sperduti - 2015 - An Empirical Study on Budget-Aware Online Kernel Algorithms for Streams of Graphs.pdf:pdf},
month = {jul},
title = {{An Empirical Study on Budget-Aware Online Kernel Algorithms for Streams of Graphs}},
url = {http://arxiv.org/abs/1507.02158},
year = {2015}
}
@article{Kalousis,
author = {Kalousis, Alexandros and Hilario, Melanie},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kalousis, Hilario - Unknown - Matching Based Kernels for Labeled Graphs.pdf:pdf},
pages = {4--7},
title = {{Matching Based Kernels for Labeled Graphs}}
}
@inproceedings{grossi2011b,
address = {Rome, Italy},
author = {Grossi, V and Turini, F},
booktitle = {In Proc. of the Third Internl. Conf. on Agents and Artificial Intelligence (ICAART 2011)},
title = {{An Adaptive Selective Ensemble for Data Streams Classification}},
year = {2011}
}
@inproceedings{NeumannGK13,
author = {Neumann, Marion and Garnett, Roman and Kersting, Kristian},
booktitle = {Asian Conference on Machine Learning},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neumann, Garnett, Kersting - 2013 - Coinciding walk kernels Parallel absorbing random walks for learning with graphs and few labels.pdf:pdf},
keywords = {kernels on graphs,label,learning in graphs and,networks,random walks},
pages = {357--372},
title = {{Coinciding walk kernels: Parallel absorbing random walks for learning with graphs and few labels}},
url = {http://jmlr.org/proceedings/papers/v29/Neumann13.html},
volume = {1},
year = {2013}
}
@article{Yan2006,
address = {Los Alamitos, CA, USA},
author = {Yan, Shuicheng and Tang, Xiaoou},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICPR.2006.451},
issn = {1051-4651},
journal = {Pattern Recognition, International Conference on},
pages = {626--629},
publisher = {IEEE Computer Society},
title = {{Dimensionality Reduction with Adaptive Kernels}},
volume = {2},
year = {2006}
}
@incollection{Chen,
author = {Chen, Yi-wei and Lin, Chih-jen},
booktitle = {Feature Extraction},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Lin - 2007 - Combining SVMs with Various Feature Selection Strategies.pdf:pdf},
number = {1},
pages = {1--10},
title = {{Combining SVMs with Various Feature Selection Strategies}},
year = {2007}
}
@article{Adjiman2006,
author = {Adjiman, Philippe and Simon, Laurent},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Adjiman, Simon - 2006 - Distributed Reasoning in a Peer-to-Peer Setting Application to the Semantic Web.pdf:pdf},
journal = {Artificial Intelligence},
pages = {269--314},
title = {{Distributed Reasoning in a Peer-to-Peer Setting : Application to the Semantic Web}},
volume = {25},
year = {2006}
}
@inproceedings{Hotho2006,
abstract = { In social bookmark tools users are setting uplightweight conceptual
structures called folksonomies. Currently,the information retrieval
support is limited. We present a formalmodel and a new search algorithm
for folksonomies, calledFolkRank, that exploits the structure of
the folksonomy. Theproposed algorithm is also applied to find communities
within thefolksonomy and is used to structure search results. All
findings aredemonstrated on a large scale dataset. A long version
of this paperhas been published at the European Semantic Web Conference2006.},
author = {Hotho, Andreas and J{\"{a}}schke, Robert and Schmitz, Christoph and Stumme, Gerd},
booktitle = {Proc. FGIR 2006},
title = {{FolkRank: A Ranking Algorithm for Folksonomies}},
year = {2006}
}
@article{Berwanger2012,
author = {Berwanger, Dietmar and Dawar, Anuj and Hunter, Paul},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Berwanger, Dawar, Hunter - 2012 - The dag-width of directed graphs.pdf:pdf},
journal = {Journal of Combinatorial {\ldots}},
number = {November 2011},
title = {{The dag-width of directed graphs}},
url = {http://www.sciencedirect.com/science/article/pii/S0095895612000329},
year = {2012}
}
@inproceedings{Aggarwal11,
author = {Aggarwal, Charu C},
booktitle = {SDM},
pages = {652--663},
title = {{On Classification of Graph Streams}},
year = {2011}
}
@unpublished{Sanavia2009,
author = {Sanavia, Tiziana and Martini, Matteo},
month = {sep},
title = {{Vademecum di Genomica}},
year = {2009}
}
@article{Wettschereck,
author = {Wettschereck, Dietrich and Dietterich, Thomas G},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wettschereck, Dietterich - Unknown - Locally Adaptive Nearest Neighbor Algorithms.pdf:pdf},
pages = {184--191},
title = {{Locally Adaptive Nearest Neighbor Algorithms}}
}
@article{Friedman1940,
author = {Friedman, M},
journal = {Ann. of Math. Stat.},
pages = {86--92},
title = {{A comparison of alternative tests of significance for the problem of m rankings}},
volume = {11},
year = {1940}
}
@inproceedings{Zhang2007,
address = {Prague, Czech Republic},
author = {Zhang, M and Che, W and Aw, A and Tan, C L and Zhou, G and Liu, T and Li, S},
booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
month = {jun},
pages = {200--207},
publisher = {Association for Computational Linguistics},
title = {{A Grammar-driven Convolution Tree Kernel for Semantic Role Classification}},
year = {2007}
}
@article{Kriege2014,
author = {Kriege, Nils and Neumann, Marion and Kersting, Kristian and Mutzel, Petra},
doi = {10.1109/ICDM.2014.129},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kriege et al. - 2014 - Explicit Versus Implicit Graph Feature Maps A Computational Phase Transition for Walk Kernels.pdf:pdf},
isbn = {978-1-4799-4302-9},
journal = {2014 IEEE International Conference on Data Mining},
month = {dec},
pages = {881--886},
publisher = {Ieee},
title = {{Explicit Versus Implicit Graph Feature Maps: A Computational Phase Transition for Walk Kernels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7023417},
year = {2014}
}
@article{Giraud-carrier,
author = {Giraud-carrier, Christophe},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Giraud-carrier - 2000 - A Note on the Utility of Incremental Learning.pdf:pdf},
journal = {Ai Communications},
keywords = {incrementality,inductive learning},
pages = {215--223},
title = {{A Note on the Utility of Incremental Learning}},
volume = {13 (4)},
year = {2000}
}
@article{Sato2008a,
abstract = {Recent discoveries of a large variety of important roles for non-coding RNAs (ncRNAs) have been reported by numerous researchers. In order to analyze ncRNAs by kernel methods including support vector machines, we propose stem kernels as an extension of string kernels for measuring the similarities between two RNA sequences from the viewpoint of secondary structures. However, applying stem kernels directly to large data sets of ncRNAs is impractical due to their computational complexity.},
author = {Sato, Kengo and Mituyama, Toutai and Asai, Kiyoshi and Sakakibara, Yasubumi},
doi = {10.1186/1471-2105-9-318},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sato et al. - 2008 - Directed acyclic graph kernels for structural RNA analysis(2).pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Base Pairing,Base Sequence,Methods,Models, Molecular,Nucleic Acid Conformation,Probability,RNA, Untranslated,RNA, Untranslated: chemistry,Sequence Alignment,Sequence Alignment: methods},
month = {jan},
pages = {318},
pmid = {18647390},
title = {{Directed acyclic graph kernels for structural RNA analysis.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2515856{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Fisher2010,
author = {Fisher, Matthew and Savva, Manolis and Hanrahan, Pat},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fisher, Savva, Hanrahan - 2011 - Characterizing Structural Relationships in Scenes Using Graph Kernels.pdf:pdf},
journal = {ACM Transactions on Graphics (TOG) - Proceedings of ACM SIGGRAPH 2011},
keywords = {3d model search,graph kernel,scene modeling,struc-},
number = {4},
title = {{Characterizing Structural Relationships in Scenes Using Graph Kernels}},
volume = {30},
year = {2011}
}
@article{Cesa-Bianchi2014,
abstract = {A well-recognized limitation of kernel learning is the requirement to handle a kernel matrix, whose size is quadratic in the number of training examples. Many methods have been proposed to reduce this computational cost, mostly by using a subset of the kernel matrix entries, or some form of low-rank matrix approximation, or a random projection method. In this paper, we study lower bounds on the error attainable by such methods as a function of the number of entries observed in the kernel matrix or the rank of an approximate kernel matrix. We show that there are kernel learning problems where no such method will lead to non-trivial computational savings. Our results also quantify how the problem difficulty depends on parameters such as the nature of the loss function, the regularization parameter, the norm of the desired predictor, and the kernel matrix rank. Our results also suggest cases where more efficient kernel learning might be possible.},
archivePrefix = {arXiv},
arxivId = {1411.1158},
author = {Cesa-Bianchi, Nicol{\`{o}} and Mansour, Yishay and Shamir, Ohad},
eprint = {1411.1158},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cesa-Bianchi, Mansour, Shamir - 2014 - On the Complexity of Learning with Kernels.pdf:pdf},
month = {nov},
number = {4},
title = {{On the Complexity of Learning with Kernels}},
url = {http://arxiv.org/abs/1411.1158},
year = {2014}
}
@article{10.1109/ICDM.2003.1250974,
address = {Los Alamitos, CA, USA},
author = {Huan, Jun and Wang, Wei and Prins, Jan},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICDM.2003.1250974},
isbn = {0-7695-1978-4},
journal = {Data Mining, IEEE International Conference on},
pages = {549},
publisher = {IEEE Computer Society},
title = {{Efficient Mining of Frequent Subgraphs in the Presence of Isomorphism}},
volume = {0},
year = {2003}
}
@article{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram},
doi = {10.1007/s10994-007-5014-x},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shalev-Shwartz, Singer - 2007 - A primal-dual perspective of online learning algorithms.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {duality,mistake bounds,online learning,regret bounds},
month = {jul},
number = {2-3},
pages = {115--142},
title = {{A primal-dual perspective of online learning algorithms}},
url = {http://link.springer.com/10.1007/s10994-007-5014-x},
volume = {69},
year = {2007}
}
@article{Liu2007,
author = {Liu, Yong and Liao, Shizhong},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Liu, Liao - 2007 - Eigenvalues Ratio for Kernel Selection of Kernel Methods.pdf:pdf},
keywords = {Novel Machine Learning Algorithms Track},
pages = {2814--2820},
title = {{Eigenvalues Ratio for Kernel Selection of Kernel Methods}},
year = {2007}
}
@inproceedings{Kudo2003,
address = {Morristown, NJ, USA},
author = {Kudo, Taku and Matsumoto, Yuji},
booktitle = {ACL '03: Proceedings of the 41st Annual Meeting on Association for Computational Linguistics},
doi = {http://dx.doi.org/10.3115/1075096.1075100},
pages = {24--31},
publisher = {Association for Computational Linguistics},
title = {{Fast methods for kernel-based text analysis}},
year = {2003}
}
@article{Sripakdeevong2012,
address = {Berlin, Heidelberg},
author = {Sripakdeevong, Parin and Beauchamp, Kyle and Das, Rhiju},
doi = {10.1007/978-3-642-25740-7},
editor = {Leontis, Neocles and Westhof, Eric},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sripakdeevong, Beauchamp, Das - 2012 - RNA 3D Structure Analysis and Prediction.pdf:pdf},
isbn = {978-3-642-25739-1},
pages = {43--65},
publisher = {Springer Berlin Heidelberg},
series = {Nucleic Acids and Molecular Biology},
title = {{RNA 3D Structure Analysis and Prediction}},
url = {http://link.springer.com/10.1007/978-3-642-25740-7},
volume = {27},
year = {2012}
}
@misc{Singhal2012,
author = {Singhal, Amit},
booktitle = {Official Blog of Google},
title = {{Introducing the Knowledge Graph: things, not strings}},
year = {2012}
}
@article{Wang2013a,
author = {Wang, Ling and Sahbi, Hichem},
doi = {10.1109/ICCV.2013.393},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Sahbi - 2013 - Directed Acyclic Graph Kernels for Action Recognition.pdf:pdf},
isbn = {978-1-4799-2840-8},
journal = {2013 IEEE International Conference on Computer Vision},
month = {dec},
pages = {3168--3175},
publisher = {Ieee},
title = {{Directed Acyclic Graph Kernels for Action Recognition}},
year = {2013}
}
@article{Sakakibara2007,
abstract = {Several computational methods based on stochastic context-free grammars have been developed for modeling and analyzing functional RNA sequences. These grammatical methods have succeeded in modeling typical secondary structures of RNA, and are used for structural alignment of RNA sequences. However, such stochastic models cannot sufficiently discriminate member sequences of an RNA family from nonmembers and hence detect noncoding RNA regions from genome sequences. A novel kernel function, stem kernel, for the discrimination and detection of functional RNA sequences using support vector machines (SVMs) is proposed. The stem kernel is a natural extension of the string kernel, specifically the all-subsequences kernel, and is tailored to measure the similarity of two RNA sequences from the viewpoint of secondary structures. The stem kernel examines all possible common base pairs and stem structures of arbitrary lengths, including pseudoknots between two RNA sequences, and calculates the inner product of common stem structure counts. An efficient algorithm is developed to calculate the stem kernels based on dynamic programming. The stem kernels are then applied to discriminate members of an RNA family from nonmembers using SVMs. The study indicates that the discrimination ability of the stem kernel is strong compared with conventional methods. Furthermore, the potential application of the stem kernel is demonstrated by the detection of remotely homologous RNA families in terms of secondary structures. This is because the string kernel is proven to work for the remote homology detection of protein sequences. These experimental results have convinced us to apply the stem kernel in order to find novel RNA families from genome sequences.},
author = {Sakakibara, Yasubumi and Popendorf, Kris and Ogawa, Nana and Asai, Kiyoshi and Sato, Kengo},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sakakibara et al. - 2007 - Stem kernels for RNA sequence analyses.pdf:pdf},
issn = {0219-7200},
journal = {Journal of bioinformatics and computational biology},
keywords = {Base Sequence,Chromosome Mapping,Chromosome Mapping: statistics {\&} numerical data,Computational Biology,Databases, Nucleic Acid,Databases, Nucleic Acid: statistics {\&} numerical da,Nucleic Acid Conformation,RNA,RNA, Transfer,RNA, Transfer: chemistry,RNA, Transfer: genetics,RNA, Viral,RNA, Viral: chemistry,RNA, Viral: genetics,RNA: chemistry,RNA: genetics,Sequence Analysis, RNA,Sequence Analysis, RNA: statistics {\&} numerical dat,Stochastic Processes,Tymovirus,Tymovirus: genetics},
month = {oct},
number = {5},
pages = {1103--22},
pmid = {17933013},
title = {{Stem kernels for RNA sequence analyses.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17933013},
volume = {5},
year = {2007}
}
@article{Weisfeiler1976,
author = {Weisfeiler, Boris},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Weisfeiler - 1976 - On construction and identification of graphs.pdf:pdf},
isbn = {3540080511},
journal = {Lecture Notes in Mathematics},
title = {{On construction and identification of graphs}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.135.8587},
year = {1976}
}
@article{Roverato2006,
annote = {Learning of large-scale networks of interactions from microarray
data is an important and challenging problem in bioinformatics. A
widely used approach is to assume that the available data constitute
a random sample from a multivariate distribution belonging to a Gaussian
graphical model. As a consequence, the prime objects of inference
are full-order partial correlations which are partial correlations
between two variables given the remaining ones. In the context of
microarray data the number of variables exceeds the sample size and
this precludes the application of traditional structure learning
procedures because a sampling version of full-order partial correlations
does not exist. In this paper we introduce a structure learning procedure,
that we call the qp-procedure, based on limited-order partial correlations.
The procedure is implemented in a freely available package for the
statistical software R.},
author = {{Roverato A. Castelo}, R},
howpublished = {http://rivista-statistica.cib.unibo.it/article/view/1212},
issn = {1973-2201},
journal = {STATISTICA -BOLOGNA-},
month = {jun},
number = {4},
pages = {343--365},
title = {{Structural learning of Gaussian graphical models from microarray data with p larger than n}},
type = {Text.Serial.Journal},
volume = {66},
year = {2006}
}
@article{Chi2013,
author = {Chi, Lianhua and Li, Bin and Zhu, Xingquan},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chi, Li, Zhu - 2013 - LNAI 7818 - Fast Graph Stream Classification Using Discriminative Clique Hashing.pdf:pdf},
keywords = {cliques,graph classification,graph streams,hashing},
pages = {225--236},
title = {{LNAI 7818 - Fast Graph Stream Classification Using Discriminative Clique Hashing}},
year = {2013}
}
@article{Lenzerini2011,
author = {Lenzerini},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Lenzerini - Unknown - Information Integration slides.pdf:pdf},
issn = {1090-2104},
journal = {BISS},
title = {{Information Integration slides}},
volume = {2011}
}
@article{Galitsky2013,
author = {Galitsky, Boris A and Kuznetsov, Sergei O and Usikov, Daniel},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Galitsky, Kuznetsov, Usikov - 2013 - Parse Thicket Representation for Multi-sentence Search.pdf:pdf},
keywords = {generalization,learning syntactic parse tree,learning taxonomy,search relevance,syntactic},
pages = {153--172},
title = {{Parse Thicket Representation for Multi-sentence Search}},
year = {2013}
}
@inproceedings{Crammer2003,
author = {Crammer, Koby and Kandola, Jaz S and Singer, Yoram},
booktitle = {NIPS},
editor = {Thrun, Sebastian and Saul, Lawrence K and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Crammer, Kandola, Singer - 2003 - Online Classification on a Budget(2).pdf:pdf},
isbn = {0-262-20152-6},
publisher = {MIT Press},
title = {{Online Classification on a Budget}},
year = {2003}
}
@inproceedings{Costa2010,
annote = {From Duplicate 1 ( 







Fast Neighborhood Subgraph Pairwise Distance Kernel







- Costa, Fabrizio; Grave, Kurt De )



},
author = {Costa, Fabrizio and {De Grave}, Kurt},
booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
editor = {Joachims, Johannes F{\"{u}}rnkranz and Thorsten},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Costa, De Grave - 2010 - Fast neighborhood subgraph pairwise distance kernel.pdf:pdf},
pages = {255--262},
publisher = {Omnipress},
title = {{Fast neighborhood subgraph pairwise distance kernel}},
year = {2010}
}
@inproceedings{DBLP:conf/nips/ViswanathanS02,
author = {Vishwanathan, S V N and Smola, Alexander J},
booktitle = {NIPS},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vishwanathan, Smola - 2002 - Fast Kernels for String and Tree Matching.pdf:pdf},
isbn = {0-262-02550-7},
pages = {569--576},
title = {{Fast Kernels for String and Tree Matching}},
year = {2002}
}
@article{Kundu2013,
abstract = {MOTIVATION: State-of-the-art experimental data for determining binding specificities of peptide recognition modules (PRMs) is obtained by high-throughput approaches like peptide arrays. Most prediction tools applicable to this kind of data are based on an initial multiple alignment of the peptide ligands. Building an initial alignment can be error-prone, especially in the case of the proline-rich peptides bound by the SH3 domains. RESULTS: Here, we present a machine-learning approach based on an efficient graph-kernel technique to predict the specificity of a large set of 70 human SH3 domains, which are an important class of PRMs. The graph-kernel strategy allows us to (i) integrate several types of physico-chemical information for each amino acid, (ii) consider high-order correlations between these features and (iii) eliminate the need for an initial peptide alignment. We build specialized models for each human SH3 domain and achieve competitive predictive performance of 0.73 area under precision-recall curve, compared with 0.27 area under precision-recall curve for state-of-the-art methods based on position weight matrices. We show that better models can be obtained when we use information on the noninteracting peptides (negative examples), which is currently not used by the state-of-the art approaches based on position weight matrices. To this end, we analyze two strategies to identify subsets of high confidence negative data. The techniques introduced here are more general and hence can also be used for any other protein domains, which interact with short peptides (i.e. other PRMs). AVAILABILITY: The program with the predictive models can be found at http://www.bioinf.uni-freiburg.de/Software/SH3PepInt/SH3PepInt.tar.gz. We also provide a genome-wide prediction for all 70 human SH3 domains, which can be found under http://www.bioinf.uni-freiburg.de/Software/SH3PepInt/Genome-Wide-Predictions.tar.gz. SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Kundu, Kousik and Costa, Fabrizio and Backofen, Rolf},
doi = {10.1093/bioinformatics/btt220},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kundu, Costa, Backofen - 2013 - A graph kernel approach for alignment-free domain-peptide interaction prediction with an application to.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Artificial Intelligence,Genome,Human,Humans,Ligands,Models,Molecular,Peptides,Peptides: chemistry,Peptides: metabolism,Protein,Protein: methods,Sequence Analysis,Software,src Homology Domains},
month = {jul},
number = {13},
pages = {i335--43},
pmid = {23813002},
title = {{A graph kernel approach for alignment-free domain-peptide interaction prediction with an application to human SH3 domains.}},
volume = {29},
year = {2013}
}
@book{Michalski1983,
author = {Michalski, R.S. and Carbonell, J.G. and Mitchell, T.M.},
keywords = {and information science,artificial intelligence,classification,computerized simulation,computing,data acquisition,data analysis,decision making,document types,education,efficiency,engineering,expert systems,game theory,general and miscellaneous//mathematics,human factors engineering,information retrieval,knowledge base,learning,man-machine systems,mathematical models,mathematics,planning,psychology,reviews,simulation,statistics,training},
language = {English},
month = {jan},
publisher = {Kaufman Publishers Inc.,Los Altos, CA},
title = {{Machine learning: An artificial intelligence approach}},
url = {http://www.osti.gov/energycitations/product.biblio.jsp?osti{\_}id=7054175},
year = {1983}
}
@article{Tipping2001,
author = {Tipping, Michael E},
doi = {http://dx.doi.org/10.1162/15324430152748236},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {211--244},
publisher = {JMLR.org},
title = {{Sparse bayesian learning and the relevance vector machine}},
volume = {1},
year = {2001}
}
@article{Wang2011a,
abstract = {Despite the considerable progress in disease gene discovery, we are far from uncovering the underlying cellular mechanisms of diseases since complex traits, even many Mendelian diseases, cannot be explained by simple genotype-phenotype relationships. More recently, an increasingly accepted view is that human diseases result from perturbations of cellular systems, especially molecular networks. Genes associated with the same or similar diseases commonly reside in the same neighborhood of molecular networks. Such observations have built the basis for a large collection of computational approaches to find previously unknown genes associated with certain diseases. The majority of the methods are based on protein interactome networks, with integration of other large-scale genomic data or disease phenotype information, to infer how likely it is that a gene is associated with a disease. Here, we review recent, state of the art, network-based methods used for prioritizing disease genes as well as unraveling the molecular basis of human diseases.},
author = {Wang, Xiujuan and Gulbahce, Natali and Yu, Haiyuan},
doi = {10.1093/bfgp/elr024},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Gulbahce, Yu - 2011 - Network-based methods for human disease gene prediction.pdf:pdf},
issn = {2041-2657},
journal = {Briefings in functional genomics},
keywords = {Disease,Disease: genetics,Gene Regulatory Networks,Gene Regulatory Networks: genetics,Genetic Association Studies,Genomics,Genomics: methods,Humans,Protein Interaction Mapping},
month = {sep},
number = {5},
pages = {280--93},
pmid = {21764832},
title = {{Network-based methods for human disease gene prediction.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21764832},
volume = {10},
year = {2011}
}
@article{Campbell2002,
abstract = {Kernel methods have become an increasingly popular tool for machine
learning tasks such as classification, regression or novelty detection.
They exhibit good generalization performance on many real-life datasets,
there are few free parameters to adjust and the architecture of the
learning machine does not need to be found by experimentation. In
this tutorial, we survey this subject with a principal focus on the
most well-known models based on kernel substitution, namely, support
vector machines.},
author = {Campbell, Colin},
journal = {Neurocomputing},
month = {oct},
number = {1},
pages = {63--84},
title = {{Kernel methods: a survey of current techniques}},
volume = {48},
year = {2002}
}
@book{Shawe-Taylor2004,
abstract = {Kernel methods provide a powerful and unified framework for pattern discovery, motivating algorithms that can act on general types of data (e.g. strings, vectors or text) and look for general types of relations (e.g. rankings, classifications, regressions, clusters). The application areas range from neural networks and pattern recognition to machine learning and data mining. This book, developed from lectures and tutorials, fulfils two major roles: firstly it provides practitioners with a large toolkit of algorithms, kernels and solutions ready to use for standard pattern discovery problems in fields such as bioinformatics, text analysis, image analysis. Secondly it provides an easy introduction for students and researchers to the growing field of kernel-based pattern analysis, demonstrating with examples how to handcraft an algorithm or a kernel for a new specific application, and covering all the necessary conceptual and mathematical tools to do so.},
address = {New York, NY, USA},
author = {Shawe-Taylor, J. and Cristianini, N.},
booktitle = {Cambridge University Press},
doi = {10.2277},
isbn = {0521813972},
issn = {00401706},
pages = {476},
publisher = {Cambridge University Press},
title = {{Kernel Methods for Pattern Analysis}},
url = {http://eprints.ecs.soton.ac.uk/9580/},
year = {2004}
}
@inproceedings{Ohkura2005,
author = {Ohkura, Nobuhito and Hirata, Kouichi and Kuboyama, Tetsuji and Harao, Masateru},
booktitle = {Discovery Science},
editor = {Hoffmann, Achim G and Motoda, Hiroshi and Scheffer, Tobias},
isbn = {3-540-29230-6},
pages = {189--202},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{The {\{}$\backslash$it {\}}-Gram Distance for Ordered Unlabeled Trees}},
volume = {3735},
year = {2005}
}
@article{10.1109/ICDM.2009.30,
address = {Los Alamitos, CA, USA},
author = {Hido, Shohei and Kashima, Hisashi},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICDM.2009.30},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hido, Kashima - 2009 - A Linear-Time Graph Kernel.pdf:pdf},
issn = {1550-4786},
journal = {Data Mining, IEEE International Conference on},
pages = {179--188},
publisher = {IEEE Computer Society},
title = {{A Linear-Time Graph Kernel}},
volume = {0},
year = {2009}
}
@article{Bunke2012,
author = {Bunke, Horst and Riesen, Kaspar},
doi = {10.1016/j.patrec.2011.04.017},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bunke, Riesen - 2012 - Towards the unification of structural and statistical pattern recognition.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {graph classification,statistical pattern recognition,structural pattern recognition},
month = {may},
number = {7},
pages = {811--825},
publisher = {Elsevier B.V.},
title = {{Towards the unification of structural and statistical pattern recognition}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865511001309},
volume = {33},
year = {2012}
}
@inproceedings{Fouss2006b,
abstract = {Thispaperpresentsasurveyaswellasanempiricalcomparisonandevaluationofsevenkernelsongraphs andtworelatedsimilaritymatrices,thatwegloballyrefertoas‘‘kernelsongraphs''forsimplicity.Theyare theexponentialdiffusionkernel,theLaplacianexponentialdiffusionkernel,thevonNeumanndiffusion kernel,theregularizedLaplaciankernel,thecommute-time(orresistance-distance)kernel,therandom- walk-with-restartsimilaritymatrix,andfinally,akernelfirstintroducedinthispaper(theregularized commute-timekernel)andtwokernelsdefinedinsomeofourpreviousworkandfurtherinvestigated in this paper (the Markov diffusion kernel and the relative-entropy diffusion matrix). The kernel-on- graphsapproachissimpleandintuitive.Itisillustratedbyapplyingtheninekernelstoacollaborative- recommendationtask,viewedasalinkpredictionproblem,andtoasemisupervisedclassificationtask, both on several databases. The methods compute proximity measures between nodes that help study thestructureofthegraph.Ourcomparisonssuggestthattheregularizedcommute-timeandtheMarkov diffusionkernelsperformbestontheinvestigatedtasks,closelyfollowedbytheregularizedLaplacian kernel},
author = {Fouss, Fran{\c{c}}ois and Fran{\c{c}}oisse, Kevin and Yen, Luh and Pirotte, Alain and Saerens, Marco},
booktitle = {ICDM},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fouss et al. - 2006 - An Experimental Investigation of Graph Kernels on Collaborative Recommendation and Semisupervised Classification.pdf:pdf},
isbn = {0769527019},
title = {{An Experimental Investigation of Graph Kernels on Collaborative Recommendation and Semisupervised Classification}},
year = {2006}
}
@article{Suzuki2000,
author = {Suzuki, Jun and Hirao, Tsutomu and Sasaki, Yutaka and Maeda, Eisaku},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Suzuki et al. - 2003 - Hierarchical directed acyclic graph kernel Methods for structured natural language data.pdf:pdf},
journal = {{\ldots} of the 41st Annual Meeting on {\ldots}},
pages = {2--4},
title = {{Hierarchical directed acyclic graph kernel: Methods for structured natural language data}},
url = {http://dl.acm.org/citation.cfm?id=1075101},
year = {2003}
}
@article{Tibshirani,
author = {Tibshirani, Ryan J. and Tibshirani, Robert},
doi = {10.1214/08-AOAS224},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Tibshirani, Tibshirani - 2009 - A bias correction for the minimum error rate in cross-validation.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {cross-validation,optimism es-,prediction error estimation},
month = {jun},
number = {2},
pages = {822--829},
title = {{A bias correction for the minimum error rate in cross-validation}},
url = {http://projecteuclid.org/euclid.aoas/1245676196},
volume = {3},
year = {2009}
}
@article{Vinh2010,
author = {Vinh, Nguyen Duy and Inokuchi, Akihiro and Washio, Takashi},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vinh, Inokuchi, Washio - 2010 - Graph Classification Based on Optimizing Graph Spectra.pdf:pdf},
keywords = {graph kernel,graph spectra,interlace theorem},
pages = {205--220},
title = {{Graph Classification Based on Optimizing Graph Spectra}},
year = {2010}
}
@misc{Chervonenkis1971,
abstract = {Abstract not available. {\textcopyright} Society for Industrial and Applied Mathematics},
author = {Chervonenkis, A. Ya. and Vapnik, V. N.},
booktitle = {Theory of Probability {\&} Its Applications},
doi = {10.1137/1116025},
issn = {0040-585X},
keywords = {VC dimension,machine{\_}learning,ml-theory,statistical analysis,statistical-learning-theory,statistics,stl,structural risk minimization,theoretical-analysis,uniform convergence},
mendeley-tags = {VC dimension,machine{\_}learning,ml-theory,statistical analysis,statistical-learning-theory,statistics,stl,structural risk minimization,theoretical-analysis,uniform convergence},
pages = {264--280},
title = {{On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities}},
volume = {16},
year = {1971}
}
@article{Vogelstein2011,
archivePrefix = {arXiv},
arxivId = {1108.1427},
author = {Vogelstein, Joshua T. and Gray, William R. and Vogelstein, R. Jacob and Priebe, Carey E.},
eprint = {1108.1427},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vogelstein et al. - 2011 - Graph Classification using Signal-Subgraphs Applications in Statistical Connectomics.pdf:pdf},
month = {aug},
pages = {1--15},
title = {{Graph Classification using Signal-Subgraphs: Applications in Statistical Connectomics}},
url = {http://arxiv.org/abs/1108.1427v2},
year = {2011}
}
@inproceedings{Aiolli2007,
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro and Hagenbuchner, Markus},
booktitle = {Proceedings of the 2007 ESANN Conference},
title = {{''Kernelized'' Self Organizing Maps for Structured Data}},
year = {2007}
}
@article{Kimura2011,
author = {Kimura, Daisuke and Kuboyama, Tetsuji and Shibuya, Tetsuo and Kashima, Hisashi},
doi = {10.1527/tjsai.26.473},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kimura et al. - 2011 - A Subpath Kernel for Rooted Unordered Trees.pdf:pdf},
issn = {1346-8030},
journal = {Transactions of the Japanese Society for Artificial Intelligence},
keywords = {convolution kernels,kernel methods,tree kernels},
pages = {473--482},
title = {{A Subpath Kernel for Rooted Unordered Trees}},
url = {http://joi.jlc.jst.go.jp/JST.JSTAGE/tjsai/26.473?from=CrossRef},
volume = {26},
year = {2011}
}
@article{Alon2007,
author = {Alon, Uri},
doi = {10.1038/446497a},
issn = {1476-4687},
journal = {Nature},
keywords = {Animals,Escherichia coli,Escherichia coli: metabolism,Gene Expression Regulation,Metabolic Networks and Pathways,Models, Biological},
month = {mar},
number = {7135},
pages = {497},
pmid = {17392770},
title = {{Simplicity in biology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22717554},
volume = {446},
year = {2007}
}
@incollection{year,
title = {{No Title}}
}
@article{Luss2008,
author = {Luss, Ronny and D'Aspremont, Alexandre},
journal = {CoRR},
title = {{Support Vector Machine Classification with Indefinite Kernels}},
volume = {abs/0804.0},
year = {2008}
}
@inproceedings{Vishwanathan2006,
author = {Vishwanathan, S V N and Borgwardt, Karsten M and Schraudolph, Nicol N},
booktitle = {NIPS},
pages = {1449--1456},
title = {{Fast Computation of Graph Kernels}},
year = {2006}
}
@article{Datar02,
author = {Datar, M and Gionis, A and Indyk, P and Motwani, R},
journal = {SIAM Journ. on Comp.},
number = {6},
pages = {1794--1813},
title = {{Maintaining stream statistics over sliding windows}},
volume = {31},
year = {2002}
}
@article{DietterichThomasG.;LathropRichardH.;Lozano-Perez1997,
author = {{Dietterich, Thomas G.; Lathrop, Richard H.; Lozano-P{\'{e}}rez}, Tom{\'{a}}s},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Dietterich, Thomas G. Lathrop, Richard H. Lozano-P{\'{e}}rez - 1997 - Solving the multiple instance problem with axis-parallel rectangles.pdf:pdf},
keywords = {machine},
title = {{Solving the multiple instance problem with axis-parallel rectangles}},
volume = {89},
year = {1997}
}
@article{Gaber2005,
address = {New York, NY, USA},
author = {Gaber, M M and Zaslavsky, A and Krishnaswamy, S},
doi = {http://doi.acm.org/10.1145/1083784.1083789},
issn = {0163-5808},
journal = {ACM SIGMOD Records},
number = {2},
pages = {18--26},
publisher = {ACM Press},
title = {{Mining data streams: a review}},
volume = {34},
year = {2005}
}
@misc{TheMendeleySupportTeam2011a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/The Mendeley Support Team - 2010 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Tamburini2011,
author = {Tamburini, Gianpiero},
doi = {10.1007/s00381-010-1360-2},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Reyes - 2011 - Introduction to the special section more than measurement error discovering meaning behind informant discrepancies in cli.pdf:pdf},
issn = {1433-0350},
journal = {Child's nervous system : ChNS : official journal of the International Society for Pediatric Neurosurgery},
month = {jan},
pages = {1--38},
pmid = {21264671},
title = {{Introduction the paper: Posterior fossa decompression and the cerebellum in Chiari type II malformation: A preliminary MRI study, by Salman M.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21267564},
year = {2011}
}
@inproceedings{1273615,
address = {New York, NY, USA},
author = {Wachman, Gabriel and Khardon, Roni},
booktitle = {ICML '07: Proceedings of the 24th international conference on Machine learning},
doi = {http://doi.acm.org/10.1145/1273496.1273615},
isbn = {978-1-59593-793-3},
pages = {943--950},
publisher = {ACM},
title = {{Learning from interpretations: a rooted kernel for ordered hypergraphs}},
url = {http://portal.acm.org/ft{\_}gateway.cfm?id=1273615{\&}type=pdf{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=82529914{\&}CFTOKEN=45845436},
year = {2007}
}
@article{Barabasi2004,
author = {Barabasi, Albert-Laszlo and Oltvai, Zoltan N},
doi = {10.1038/nrg1272},
issn = {1471-0056},
journal = {Nat Rev Genet},
month = {feb},
number = {2},
pages = {101--113},
shorttitle = {Network biology},
title = {{Network biology: understanding the cell's functional organization}},
volume = {5},
year = {2004}
}
@article{Xiong2005,
author = {Xiong, Huilin and Swamy, M N S and Ahmad, M O},
journal = {IEEE TRANSACTIONS ON NEURAL NETWORKS},
month = {mar},
number = {2},
pages = {460--474},
title = {{Optimizing the kernel in the empirical feature space}},
volume = {16},
year = {2005}
}
@article{Cortes2010b,
abstract = {Kernel approximation is commonly used to scale kernel-based algorithms to applications contain- ing as many as several million instances. This paper analyzes the effect of such approximations in the kernel matrix on the hypothesis generated by several widely used learning algorithms. We give stability bounds based on the norm of the kernel approximation for these algorithms, in- cluding SVMs, KRR, and graph Laplacian-based regularization algorithms. These bounds help de- termine the degree of approximation that can be tolerated in the estimation of the kernel matrix. Our analysis is general and applies to arbitrary approximations of the kernel matrix. However, we also give a specific analysis of the Nystr¨ om low-rank approximation in this context and re- port the results of experiments evaluating the quality of the Nystr¨ om low-rank kernel approx- imation when used with ridge regression.},
author = {Cortes, Corinna and Mohri, M and Talwalkar, a},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Mohri, Talwalkar - 2010 - On the impact of kernel approximation on learning accuracy.pdf:pdf},
issn = {15324435},
journal = {{\ldots} Conference on {\ldots}},
pages = {113--120},
title = {{On the impact of kernel approximation on learning accuracy}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}CortesMT10.pdf},
volume = {9},
year = {2010}
}
@article{Slater2005,
abstract = {BACKGROUND: Exhaustive methods of sequence alignment are accurate but slow, whereas heuristic approaches run quickly, but their complexity makes them more difficult to implement. We introduce bounded sparse dynamic programming (BSDP) to allow rapid approximation to exhaustive alignment. This is used within a framework whereby the alignment algorithms are described in terms of their underlying model, to allow automated development of efficient heuristic implementations which may be applied to a general set of sequence comparison problems.

RESULTS: The speed and accuracy of this approach compares favourably with existing methods. Examples of its use in the context of genome annotation are given.

CONCLUSIONS: This system allows rapid implementation of heuristics approximating to many complex alignment models, and has been incorporated into the freely available sequence alignment program, exonerate.},
author = {Slater, Guy St C and Birney, Ewan},
doi = {10.1186/1471-2105-6-31},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Slater, Birney - 2005 - Automated generation of heuristics for biological sequence comparison.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Automation,Computational Biology,Computational Biology: methods,Computer Simulation,DNA,DNA, Complementary,DNA, Complementary: metabolism,Databases, Factual,Genome,Humans,Information Storage and Retrieval,Mathematical Computing,Models, Biological,Models, Theoretical,RNA, Messenger,RNA, Messenger: metabolism,Sequence Alignment,Sequence Analysis, DNA,Software},
month = {jan},
pages = {31},
pmid = {15713233},
title = {{Automated generation of heuristics for biological sequence comparison.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=553969{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {6},
year = {2005}
}
@article{Duchi:2009:EOB:1577069.1755882,
author = {Duchi, John and Singer, Yoram},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {2899­­2934},
title = {{Efficient Online and Batch Learning Using Forward Backward Splitting}},
volume = {10},
year = {2009}
}
@article{Weibull,
author = {Weibull, J{\"{o}}rgen W.},
title = {{Evolutionary Game Theory}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.147.8709}
}
@article{Jaakkola2000,
author = {Jaakkola, Tommi and Diekhans, Mark and Haussler, David},
journal = {Journal of Computational Biology},
number = {1-2},
pages = {95--114},
title = {{A Discriminative Framework for Detecting Remote Protein Homologies.}},
volume = {7},
year = {2000}
}
@article{caelli,
author = {Caelli, T and Caetano, T S},
journal = {Pattern Recognition Letters},
number = {3},
pages = {339--346},
title = {{Graphical models for graph matching: Approximate models and optimal algorithms.}},
volume = {26},
year = {2005}
}
@article{Fukumizu,
author = {Fukumizu, Kenji and Gretton, Arthur},
pages = {1--9},
title = {{Kernel Bayes ' Rule}}
}
@article{Burge2013,
abstract = {The Rfam database (available via the website at http://rfam.sanger.ac.uk and through our mirror at http://rfam.janelia.org) is a collection of non-coding RNA families, primarily RNAs with a conserved RNA secondary structure, including both RNA genes and mRNA cis-regulatory elements. Each family is represented by a multiple sequence alignment, predicted secondary structure and covariance model. Here we discuss updates to the database in the latest release, Rfam 11.0, including the introduction of genome-based alignments for large families, the introduction of the Rfam Biomart as well as other user interface improvements. Rfam is available under the Creative Commons Zero license.},
author = {Burge, Sarah W and Daub, Jennifer and Eberhardt, Ruth and Tate, John and Barquist, Lars and Nawrocki, Eric P and Eddy, Sean R and Gardner, Paul P and Bateman, Alex},
doi = {10.1093/nar/gks1005},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Burge et al. - 2013 - Rfam 11.0 10 years of RNA families.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Base Sequence,Databases, Nucleic Acid,Genomics,Internet,Molecular Sequence Annotation,Nucleic Acid Conformation,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: classification,RNA, Untranslated: genetics,Sequence Alignment,User-Computer Interface},
month = {jan},
number = {Database issue},
pages = {D226--32},
pmid = {23125362},
title = {{Rfam 11.0: 10 years of RNA families.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3531072{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {41},
year = {2013}
}
@inproceedings{Gartner2003a,
abstract = {As most 'real-world' data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data. One of the most widely used tools for modeling structured data are graphs. An interesting and important challenge is thus to investigate kernels on instances that are represented by graphs. So far, only very specific graphs such as trees and strings have been considered. This paper investigates kernels on labeled directed graphs with general structure. It is shown that computing a strictly positive definite graph kernel is at least as hard as solving the graph isomorphism problem. It is also shown that computing an inner product in a feature space indexed by all possible graphs, where each feature counts the number of subgraphs isomorphic to that graph, is {\{}NP-hard.{\}} On the other hand, inner products in an alternative feature space, based on walks in the graph, can be computed in polynomial time. Such kernels are defined in this paper.},
address = {Berlin, Heidelberg},
annote = {From Duplicate 1 ( 





On Graph Kernels : Hardness Results and Efficient Alternatives





- G{\"{a}}rtner, Thomas; Flach, Peter; Wrobel, Stefan )










From Duplicate 1 ( 























On Graph Kernels : Hardness Results and Efficient Alternatives























- Gartner, Thomas; Flach, P; Wrobel, S )







},
author = {Gartner, Thomas and Flach, Peter and Wrobel, Stefan and G{\"{a}}rtner, Thomas},
booktitle = {Proceedings of the 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop},
doi = {10.1007/b12006},
editor = {Sch{\"{o}}lkopf, Bernhard and Warmuth, Manfred K.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gartner, Flach, Wrobel - 2003 - On graph kernels Hardness results and efficient alternatives.pdf:pdf},
isbn = {978-3-540-40720-1},
keywords = {graph,kernels},
pages = {129--143},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{On Graph Kernels: Hardness Results and Efficient Alternatives}},
url = {http://link.springer.com/10.1007/b12006},
volume = {2777},
year = {2003}
}
@inproceedings{Aiolli2006,
address = {Los Alamitos, CA, USA},
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro and Moschitti, Alessandro},
booktitle = {Proceedings of the 2006 IEEE Conference on Data Mining},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICDM.2006.69},
issn = {1550-4786},
pages = {787--791},
publisher = {IEEE Computer Society},
title = {{Fast On-line Kernel Learning for Trees}},
year = {2006}
}
@article{Schuster,
author = {Schuster, Peter},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Schuster - Unknown - Inverse Folding and Sequence-Structure Maps of Ribonucleic Acids.pdf:pdf},
title = {{Inverse Folding and Sequence-Structure Maps of Ribonucleic Acids}}
}
@inproceedings{Orsini,
author = {Orsini, Francesco and Frasconi, Paolo and Raedt, Luc De},
booktitle = {Proceedings of the 24th International Joint Conference on Artificial Intelligence},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Orsini, Frasconi, Raedt - 2015 - Graph invariant kernels.pdf:pdf},
title = {{Graph invariant kernels}},
url = {http://ijcai.org/papers15/Papers/IJCAI15-528.pdf},
year = {2015}
}
@book{abelson-et-al:scheme,
address = {Cambridge, Massachusetts},
author = {Abelson, Harold and Sussman, Gerald{\~{}}Jay and Sussman, Julie},
publisher = {MIT Press},
title = {{Structure and Interpretation of Computer Programs}},
year = {1985}
}
@phdthesis{Martino2009,
annote = {From Duplicate 1 ( 


Kernel Methods for Tree Structured Data Giovanni Da San Martino Technical Report UBLCS-2009-04 February 2009 Department of Computer Science University of Bologna


- )

},
author = {Martino, Giovanni Da San},
booktitle = {Science},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Martino - 2009 - Kernel Methods for Tree Structured Data.pdf:pdf},
number = {February},
school = {University of Bologna},
title = {{Kernel Methods for Tree Structured Data}},
year = {2009}
}
@article{Wang2012,
author = {Wang, Zhuang and Crammer, Koby and Vucetic, Slobodan},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Crammer, Vucetic - 2012 - Breaking the Curse of Kernelization Budgeted Stochastic Gradient Descent for Large-Scale SVM Training.pdf:pdf},
journal = {Journal of Machine Learning Research},
number = {10},
pages = {3103--3131},
title = {{Breaking the Curse of Kernelization : Budgeted Stochastic Gradient Descent for Large-Scale SVM Training}},
volume = {13},
year = {2012}
}
@article{Fera2004,
abstract = {BACKGROUND: The proliferation of structural and functional studies of RNA has revealed an increasing range of RNA's structural repertoire. Toward the objective of systematic cataloguing of RNA's structural repertoire, we have recently described the basis of a graphical approach for organizing RNA secondary structures, including existing and hypothetical motifs.

DESCRIPTION: We now present an RNA motif database based on graph theory, termed RAG for RNA-As-Graphs, to catalogue and rank all theoretically possible, including existing, candidate and hypothetical, RNA secondary motifs. The candidate motifs are predicted using a clustering algorithm that classifies RNA graphs into RNA-like and non-RNA groups. All RNA motifs are filed according to their graph vertex number (RNA length) and ranked by topological complexity.

CONCLUSIONS: RAG's quantitative cataloguing allows facile retrieval of all classes of RNA secondary motifs, assists identification of structural and functional properties of user-supplied RNA sequences, and helps stimulate the search for novel RNAs based on predicted candidate motifs.},
author = {Fera, Daniela and Kim, Namhee and Shiffeldrim, Nahum and Zorn, Julie and Laserson, Uri and Gan, Hin Hark and Schlick, Tamar},
doi = {10.1186/1471-2105-5-88},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fera et al. - 2004 - RAG RNA-As-Graphs web resource.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Computational Biology,Computational Biology: methods,Computer Graphics,Computer Graphics: trends,Databases, Nucleic Acid,Databases, Nucleic Acid: trends,Internet,Internet: trends,RNA,RNA: chemistry,Software,Software Design},
month = {jul},
pages = {88},
pmid = {15238163},
title = {{RAG: RNA-As-Graphs web resource.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=471545{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {5},
year = {2004}
}
@article{Bai2011,
author = {Bai, Lu and Hancock, Edwin R},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bai, Hancock - 2011 - Graph Clustering Using the Jensen-Shannon Kernel.pdf:pdf},
keywords = {jensen-shannon},
pages = {394--401},
title = {{Graph Clustering Using the Jensen-Shannon Kernel}},
year = {2011}
}
@article{Duch2005,
author = {Duch, W},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Duch - 2005 - Support vector neural training.pdf:pdf},
journal = {Lecture Notes in Computer Science},
pages = {67--72},
title = {{Support vector neural training}},
volume = {3697},
year = {2005}
}
@inproceedings{Graf2004,
author = {Graf, Hans Peter and Cosatto, Eric and Bottou, L{\'{e}}on and Durdanovic, Igor and Vapnik, Vladimir},
booktitle = {NIPS},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Graf et al. - 2004 - Parallel Support Vector Machines The Cascade SVM.pdf:pdf},
title = {{Parallel Support Vector Machines: The Cascade SVM}},
year = {2004}
}
@article{Qureshi2012,
author = {Qureshi, Irfan a. and Mehler, Mark F.},
doi = {10.1038/nrn3234},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Qureshi, Mehler - 2012 - Emerging roles of non-coding RNAs in brain evolution, development, plasticity and disease.pdf:pdf},
issn = {1471-003X},
journal = {Nature Reviews Neuroscience},
month = {jul},
number = {8},
pages = {528--541},
publisher = {Nature Publishing Group},
title = {{Emerging roles of non-coding RNAs in brain evolution, development, plasticity and disease}},
url = {http://www.nature.com/doifinder/10.1038/nrn3234},
volume = {13},
year = {2012}
}
@inproceedings{Moschitti2006a,
author = {Moschitti, Alessandro},
booktitle = {ECML},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Moschitti - 2006 - Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.pdf:pdf},
isbn = {3-540-45375-X},
month = {sep},
pages = {318--329},
series = {Lecture Notes in Computer Science},
title = {{Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees}},
volume = {4212},
year = {2006}
}
@inproceedings{WEKA,
address = {$\backslash$mbox{\{}$\backslash$tt http://www.cs.waikato.ac.nz/ml/weka{\}}},
author = {{The University of Waikato}},
title = {{Weka 3: Data Mining Software in Java, Version 3.6}},
year = {2009}
}
@inproceedings{Kashima2002,
author = {Kashima, Hisashi and Koyanagi, Teruo},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
isbn = {1-55860-873-7},
pages = {291--298},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Kernels for Semi-Structured Data}},
year = {2002}
}
@article{Karbalaie2012,
author = {Karbalaie, Fatemeh and Sami, Ashkan and Ahmadi, Mansour and Club, Young Researchers and Branch, Shiraz},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Karbalaie et al. - 2012 - Semantic Malware Detection by Deploying Graph Mining.pdf:pdf},
keywords = {frequent sub graph,labeled graph,malware detection,semantic,subgraph isomorphism,system call},
number = {1},
pages = {373--379},
title = {{Semantic Malware Detection by Deploying Graph Mining}},
volume = {9},
year = {2012}
}
@article{Sackmann2006,
abstract = {BACKGROUND:Signal transduction pathways are usually modelled using
classical quantitative methods, which are based on ordinary differential
equations (ODEs). However, some difficulties are inherent in this
approach. On the one hand, the kinetic parameters involved are often
unknown and have to be estimated. With increasing size and complexity
of signal transduction pathways, the estimation of missing kinetic
data is not possible. On the other hand, ODEs based models do not
support any explicit insights into possible (signal-) flows within
the network. Moreover, a huge amount of qualitative data is available
due to high-throughput techniques. In order to get information on
the systems behaviour, qualitative analysis techniques have been
developed. Applications of the known qualitative analysis methods
concern mainly metabolic networks. Petri net theory provides a variety
of established analysis techniques, which are also applicable to
signal transduction models. In this context special properties have
to be considered and new dedicated techniques have to be designed.METHODS:We
apply Petri net theory to model and analyse signal transduction pathways
first qualitatively before continuing with quantitative analyses.
This paper demonstrates how to build systematically a discrete model,
which reflects provably the qualitative biological behaviour without
any knowledge of kinetic parameters. The mating pheromone response
pathway in Saccharomyces cerevisiae serves as case study.RESULTS:We
propose an approach for model validation of signal transduction pathways
based on the network structure only. For this purpose, we introduce
the new notion of feasible t-invariants, which represent minimal
self-contained subnets being active under a given input situation.
Each of these subnets stands for a signal flow in the system. We
define maximal common transition sets (MCT-sets), which can be used
for t-invariant examination and net decomposition into smallest biologically
meaningful functional units.CONCLUSION:The paper demonstrates how
Petri net analysis techniques can promote a deeper understanding
of signal transduction pathways. The new concepts of feasible t-invariants
and MCT-sets have been proven to be useful for model validation and
the interpretation of the biological system behaviour. Whereas MCT-sets
provide a decomposition of the net into disjunctive subnets, feasible
t-invariants describe subnets, which generally overlap. This work
contributes to qualitative modelling and to the analysis of large
biological networks by their fully automatic decomposition into biologically
meaningful modules.},
author = {Sackmann, Andrea and Heiner, Monika and Koch, Ina},
doi = {10.1186/1471-2105-7-482},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {482},
title = {{Application of Petri net based analysis techniques to signal transduction pathways}},
volume = {7},
year = {2006}
}
@article{Yan2008,
author = {Yan, Xifeng and Han, Jiawei and Yu, Philip S},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Yan, Han, Yu - 2008 - Mining Significant Graph Patterns by Leap Search ∗ Categories and Subject Descriptors.pdf:pdf},
isbn = {9781605581026},
journal = {Search},
title = {{Mining Significant Graph Patterns by Leap Search ∗ Categories and Subject Descriptors}},
year = {2008}
}
@article{Friedman1937,
author = {Friedman, M},
journal = {Journ. of Am. Stat. Ass.},
pages = {675--701},
title = {{The use of ranks to avoid the assumption of normality implicit in the analysis of variance}},
volume = {32},
year = {1937}
}
@article{Halevy,
author = {Halevy, Alon Y and Ives, Zachary G and Mork, Peter},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Halevy, Ives, Mork - Unknown - Piazza Data Management Infrastructure for Semantic Web Applications.pdf:pdf},
journal = {System},
keywords = {peer data management systems,semantic web,xml},
title = {{Piazza : Data Management Infrastructure for Semantic Web Applications}}
}
@article{Demsar2006,
author = {Demsar, Janez},
journal = {Journ. of Mach. Learn. Res.},
month = {jan},
pages = {1--30},
title = {{Statistical Comparisons of Classifiers over Multiple Data Sets}},
volume = {7},
year = {2006}
}
@incollection{Woznica2010,
annote = {10.1007/978-3-642-13672-6{\_}37},
author = {Woznica, Adam and Kalousis, Alexandros and Hilario, Melanie},
booktitle = {Advances in Knowledge Discovery and Data Mining},
editor = {Zaki, Mohammed and Yu, Jeffrey and Ravindran, B and Pudi, Vikram},
pages = {374--385},
publisher = {Springer Berlin / Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Adaptive Matching Based Kernels for Labelled Graphs}},
volume = {6119},
year = {2010}
}
@article{Mathews2006,
abstract = {RNA secondary structure is often predicted from sequence by free energy minimization. Over the past two years, advances have been made in the estimation of folding free energy change, the mapping of secondary structure and the implementation of computer programs for structure prediction. The trends in computer program development are: efficient use of experimental mapping of structures to constrain structure prediction; use of statistical mechanics to improve the fidelity of structure prediction; inclusion of pseudoknots in secondary structure prediction; and use of two or more homologous sequences to find a common structure.},
author = {Mathews, David H and Turner, Douglas H},
doi = {10.1016/j.sbi.2006.05.010},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mathews, Turner - 2006 - Prediction of RNA secondary structure by free energy minimization.pdf:pdf},
issn = {0959-440X},
journal = {Current opinion in structural biology},
keywords = {Base Sequence,Computational Biology,Computational Biology: methods,Mathematical Computing,Models, Molecular,Molecular Sequence Data,Nucleic Acid Conformation,RNA,RNA: chemistry,Thermodynamics},
month = {jun},
number = {3},
pages = {270--8},
pmid = {16713706},
title = {{Prediction of RNA secondary structure by free energy minimization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16713706},
volume = {16},
year = {2006}
}
@article{Badanidiyuru2012,
author = {Badanidiyuru, Ashwinkumar and Kleinberg, Robert and Singer, Yaron},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Badanidiyuru, Kleinberg, Singer - 2012 - Learning on a Budget Posted Price Mechanisms for Online Procurement.pdf:pdf},
isbn = {9781450314152},
number = {212},
pages = {128--145},
title = {{Learning on a Budget: Posted Price Mechanisms for Online Procurement}},
volume = {1},
year = {2012}
}
@article{Saito2011,
abstract = {Clustering of unannotated transcripts is an important task to identify novel families of noncoding RNAs (ncRNAs). Several hierarchical clustering methods have been developed using similarity measures based on the scores of structural alignment. However, the high computational cost of exact structural alignment requires these methods to employ approximate algorithms. Such heuristics degrade the quality of clustering results, especially when the similarity among family members is not detectable at the primary sequence level.},
author = {Saito, Yutaka and Sato, Kengo and Sakakibara, Yasubumi},
doi = {10.1186/1471-2105-12-S1-S48},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Saito, Sato, Sakakibara - 2011 - Fast and accurate clustering of noncoding RNAs using ensembles of sequence alignments and secondary str.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Base Sequence,Cluster Analysis,Computational Biology,Computational Biology: methods,Nucleic Acid Conformation,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: classification,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Software},
month = {jan},
number = {Suppl 1},
pages = {S48},
pmid = {21342580},
title = {{Fast and accurate clustering of noncoding RNAs using ensembles of sequence alignments and secondary structures.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3044305{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {12 Suppl 1},
year = {2011}
}
@article{Lodhi2002,
abstract = {We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets},
author = {Lodhi, H and Saunders, C and Shawe-Taylor, J and Cristianini, N and Watkins, C},
doi = {10.1162/153244302760200687},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Lodhi et al. - 2002 - Text Classification using String Kernels.pdf:pdf},
isbn = {1532-4435},
issn = {0003-6951},
journal = {Journal of Machine Learning Research},
keywords = {String Kernels,text classification},
pages = {419--444},
title = {{Text Classification using String Kernels}},
url = {http://discovery.ucl.ac.uk/13443/},
volume = {2},
year = {2002}
}
@inproceedings{Domongos2001a,
address = {San Francisco, CA},
annote = {HoeffdingTree},
author = {Hulten, G and Spencer, L and Domingos, P},
booktitle = {Proc. of the 7th Internl. Conf. on Knowl. Disc. and Data Mining (KDD'01)},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hulten, Spencer, Domingos - 2001 - Mining Time Changing Data Streams.pdf:pdf},
pages = {97--106},
title = {{Mining Time Changing Data Streams}},
year = {2001}
}
@article{Wang2006,
abstract = {Correct splice site recognition is critical in pre-mRNA splicing. We find that almost all of a diverse panel of exonic splicing silencer (ESS) elements alter splice site choice when placed between competing sites, consistently inhibiting use of intron-proximal 5' and 3' splice sites. Supporting a general role for ESSs in splice site definition, we found that ESSs are both abundant and highly conserved between alternative splice site pairs and that mutation of ESSs located between natural alternative splice site pairs consistently shifted splicing toward the intron-proximal site. Some exonic splicing enhancers (ESEs) promoted use of intron-proximal 5' splice sites, and tethering of hnRNP A1 and SF2/ASF proteins between competing splice sites mimicked the effects of ESS and ESE elements, respectively. Further, we observed that specific subsets of ESSs had distinct effects on a multifunctional intron retention reporter and that one of these subsets is likely preferred for regulation of endogenous intron retention events. Together, our findings provide a comprehensive picture of the functions of ESSs in the control of diverse types of splicing decisions.},
author = {Wang, Zefeng and Xiao, Xinshu and {Van Nostrand}, Eric and Burge, Christopher B},
doi = {10.1016/j.molcel.2006.05.018},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2006 - General and specific functions of exonic splicing silencers in splicing control.pdf:pdf},
issn = {1097-2765},
journal = {Molecular cell},
keywords = {3' Untranslated Regions,3' Untranslated Regions: physiology,Alternative Splicing,Alternative Splicing: physiology,Animals,Cells, Cultured,Computational Biology,Exons,Exons: genetics,Gene Silencing,Gene Silencing: physiology,Humans,Mice,Models, Biological,RNA Splicing,RNA Splicing: genetics,RNA Splicing: physiology,Silencer Elements, Transcriptional,Silencer Elements, Transcriptional: physiology},
month = {jul},
number = {1},
pages = {61--70},
pmid = {16797197},
title = {{General and specific functions of exonic splicing silencers in splicing control.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1839040{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {23},
year = {2006}
}
@inproceedings{Ng1998,
author = {Ng, Andrew Y.},
booktitle = {Proceedings of the Fifteenth International Conference on Machine Learning},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ng - 1998 - On Feature Selection Learning with Exponentially Many Irrelevant Features As Training Examples.pdf:pdf},
pages = {404----412},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{On Feature Selection: Learning with Exponentially Many Irrelevant Features As Training Examples}},
year = {1998}
}
@article{Shi2009,
abstract = {We propose hashing to facilitate efficient kernels. This generalizes
previous work using sampling and we show a principled way to compute
the kernel matrix for data streams and sparse feature spaces. Moreover,
we give deviation bounds from the exact kernel matrix. This has applications
to estimation on strings and graphs.},
author = {Shi, Qinfeng and Petterson, James and Dror, Gideon and Langford, John and Smola, Alex and Vishwanathan, S V N},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shi et al. - 2009 - Hash Kernels for Structured Data.pdf:pdf},
journal = {Journal of Machine Learning Research},
month = {nov},
pages = {2615--2637},
title = {{Hash Kernels for Structured Data}},
volume = {10},
year = {2009}
}
@article{Rudi2015,
abstract = {We study Nystr$\backslash$"om type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystr$\backslash$"om Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.},
archivePrefix = {arXiv},
arxivId = {1507.04717},
author = {Rudi, Alessandro and Camoriano, Raffaello and Rosasco, Lorenzo},
eprint = {1507.04717},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rudi, Camoriano, Rosasco - 2015 - Less is More Nystrom Computational Regularization.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Less is More: Nystr$\backslash$"om Computational Regularization}},
url = {http://arxiv.org/abs/1507.04717},
year = {2015}
}
@book{Vapnik1998,
author = {Vapnik, Vladimir N},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vapnik - 1998 - Statistical Learning Theory.pdf:pdf},
isbn = {0471030031},
publisher = {Wiley-Interscience},
title = {{Statistical Learning Theory}},
year = {1998}
}
@article{Eddy2001a,
abstract = {Non-coding RNA (ncRNA) genes produce functional RNA molecules rather than encoding proteins. However, almost all means of gene identification assume that genes encode proteins, so even in the era of complete genome sequences, ncRNA genes have been effectively invisible. Recently, several different systematic screens have identified a surprisingly large number of new ncRNA genes. Non-coding RNAs seem to be particularly abundant in roles that require highly specific nucleic acid recognition without complex catalysis, such as in directing post-transcriptional regulation of gene expression or in guiding RNA modifications.},
author = {Eddy, S R},
doi = {10.1038/35103511},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Eddy - 2001 - Non-coding RNA genes and the modern RNA world.pdf:pdf},
issn = {1471-0056},
journal = {Nature reviews. Genetics},
keywords = {Base Sequence,Escherichia coli,Escherichia coli: genetics,Humans,Molecular Sequence Data,Nucleic Acid Conformation,RNA, Bacterial,RNA, Bacterial: genetics,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: genetics},
month = {dec},
number = {12},
pages = {919--29},
pmid = {11733745},
title = {{Non-coding RNA genes and the modern RNA world.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11733745},
volume = {2},
year = {2001}
}
@inproceedings{Horv'ath2004,
abstract = {With applications in biology, the world-wide web, and several other
areas, mining of graph-structured objects has received significant
interest recently. One of the major research directions in this field
is concerned with predictive data mining in graph databases where
each instance is represented by a graph. Some of the proposed approaches
for this task rely on the excellent classification performance of
support vector machines. To control the computational cost of these
approaches, the underlying kernel functions are based on frequent
patterns. In contrast to these approaches, we propose a kernel function
based on a natural set of cyclic and tree patterns independent of
their frequency, and discuss its computational aspects. To practically
demonstrate the effectiveness of our approach, we use the popular
{\{}NCI-HIV{\}} molecule dataset. Our experimental results show that cyclic
pattern kernels can be computed quickly and offer predictive performance
superior to recent graph kernels based on frequent patterns.},
address = {Seattle, {\{}WA,{\}} {\{}USA{\}}},
author = {Horv{\'{a}}th, Tam{\'{a}}s and G{\"{a}}rtner, Thomas and Wrobel, Stefan},
booktitle = {Proceedings of the tenth {\{}ACM{\}} {\{}SIGKDD{\}} international conference on Knowledge discovery and data mining},
doi = {10.1145/1014052.1014072},
isbn = {1-58113-888-1},
pages = {158--167},
publisher = {ACM},
title = {{Cyclic pattern kernels for predictive graph mining}},
year = {2004}
}
@article{Fan2004,
author = {Fan, Wei},
isbn = {1581138881},
journal = {Most},
keywords = {concept-drift,data streams,decision trees},
pages = {128--137},
title = {{Systematic Data Selection to Mine Concept-Drifting Data Streams}},
year = {2004}
}
@article{Hopcroft1973,
author = {Hopcroft, JE and Tarjan, RE},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hopcroft, Tarjan - 1973 - Dividing a graph into triconnected components.pdf:pdf},
journal = {SIAM Journal on Computing},
title = {{Dividing a graph into triconnected components}},
url = {http://epubs.siam.org/doi/abs/10.1137/0202012},
year = {1973}
}
@inproceedings{Wang2007,
abstract = {The choice of the kernel function which determines the mapping between
the input space and the feature space is of crucial importance to
kernel methods. The past few years have seen many efforts in learning
either the kernel function or the kernel matrix. In this paper, we
address this model selection issue by learning the hyperparameter
of the kernel function for a support vector machine {\{}(SVM).{\}} We trace
the solution path with respect to the kernel hyperparameter without
having to train the model multiple times. Given a kernel hyperparameter
value and the optimal solution obtained for that value, we find that
the solutions of the neighborhood hyperparameters can be calculated
exactly. However, the solution path does not exhibit piecewise linearity
and extends nonlinearly. As a result, the breakpoints cannot be computed
in advance. We propose a method to approximate the breakpoints. Our
method is both efficient and general in the sense that it can be
applied to many kernel functions in common use.},
address = {Corvalis, Oregon},
author = {Wang, Gang and Yeung, Dit-Yan and Lochovsky, Frederick H},
booktitle = {Proceedings of the 24th international conference on Machine learning},
doi = {10.1145/1273496.1273616},
isbn = {978-1-59593-793-3},
pages = {951--958},
publisher = {ACM},
title = {{A kernel path algorithm for support vector machines}},
year = {2007}
}
@article{gottlob:nonmon,
author = {Gottlob, Georg},
journal = {Journal of Logic and Computation},
month = {jun},
number = {3},
pages = {397--425},
title = {{Complexity results for nonmonotonic logics}},
volume = {2},
year = {1992}
}
@article{Lim2009,
abstract = {Computational identification of prognostic biomarkers capable of withstanding
follow-up validation efforts is still an open challenge in cancer
research. For instance, several gene expression profiles analysis
methods have been developed to identify gene signatures that can
classify cancer sub-phenotypes associated with poor prognosis. However,
signatures originating from independent studies show only minimal
overlap and perform poorly when classifying datasets other than the
ones they were generated from. In this paper, we propose a computational
systems biology approach that can infer robust prognostic markers
by identifying upstream Master Regulators, causally related to the
presentation of the phenotype of interest. Such a strategy effectively
extends and complements other existing methods and may help further
elucidate the molecular mechanisms of the observed pathophysiological
phenotype. Results show that inferred regulators substantially outperform
canonical gene signatures both on the original dataset and across
distinct datasets.},
annote = {{\{}PMID:{\}} 19209726},
author = {Lim, Wei Keat and Lyashenko, Eugenia and Califano, Andrea},
issn = {1793-5091},
journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
keywords = {Algorithms,Biometry,Breast{\_}Neoplasms,Databases,Female,Gene Expression Profiling,Genes,Genetic,Humans,Oligonucleotide Array Sequence Analysis,Phenotype,Prognosis,Regulator,Regulatory Elements,Systems Biology,Transcriptional,biomarcatori},
pages = {504--515},
title = {{Master regulators used as breast cancer metastasis classifier}},
year = {2009}
}
@inproceedings{Su1996,
address = {New York, New York, USA},
author = {Su, Peter and Drysdale, Robert L Scot},
booktitle = {Proceedings of the eleventh annual symposium on Computational geometry - SCG '95},
doi = {10.1145/220279.220286},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Su, Drysdale - 1995 - A comparison of sequential Delaunay triangulation algorithms.pdf:pdf},
isbn = {0897917243},
pages = {61--70},
publisher = {ACM Press},
title = {{A comparison of sequential Delaunay triangulation algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=220279.220286},
year = {1995}
}
@article{emailFiltering,
author = {Katakis, I and Tsoumakas, G and Vlahavas, I},
journal = {Knowl. and Inform. Sys.},
number = {3},
pages = {371--391},
title = {{Tracking recurring contexts using ensemble classifiers: an application to email filtering}},
volume = {22},
year = {2010}
}
@article{Neumann2009,
author = {Neumann, Marion and Kersting, Kristian and Xu, Zhao and Schulz, Daniel},
doi = {10.1109/ICDM.2009.56},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neumann et al. - 2009 - Stacked Gaussian Process Learning.pdf:pdf},
isbn = {978-1-4244-5242-2},
journal = {2009 Ninth IEEE International Conference on Data Mining},
keywords = {-statistical relational learning,bayesian regression,black boxes,cesses,figure 1,flow predictions,for a bus line,gaussian pro-,in the german city,of ulm using gaussian,processes,stacked learning,the larger the boxes,the more predicted},
month = {dec},
pages = {387--396},
publisher = {Ieee},
title = {{Stacked Gaussian Process Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5360264},
year = {2009}
}
@article{kramer,
author = {{Kramer S.}, Raedt L and C., Helma},
journal = {Proc. of KDD},
pages = {136--143},
title = {{Molecular feature mining in hiv data.}},
year = {2001}
}
@article{Ben-David2003,
author = {Ben-David, Shai and Eiron, Nadav and Simon, Hans Ulrich},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {441--461},
publisher = {JMLR.org},
title = {{Limitations of learning via embeddings in euclidean half spaces}},
volume = {3},
year = {2003}
}
@inproceedings{Jun2010,
address = {Haifa, Israel},
author = {Jun, Wang and Sanjiv, Kumar and Shih-Fu, Chang},
booktitle = {Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
editor = {F{\"{u}}rnkranz, Johannes and Joachims, Thorsten},
month = {jun},
pages = {1127--1134},
publisher = {Omnipress},
title = {{Sequential Projection Learning for Hashing with Compact Codes}},
year = {2010}
}
@article{Compagna2005,
author = {Compagna, Luca},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Compagna - 2005 - SAT-based Model-Checking of Security Protocols.pdf:pdf},
journal = {Gene},
number = {August},
title = {{SAT-based Model-Checking of Security Protocols}},
year = {2005}
}
@inproceedings{Horvath2004,
address = {New York, New York, USA},
author = {Horv{\'{a}}th, Tam{\'{a}}s and G{\"{a}}rtner, Thomas and Wrobel, Stefan},
booktitle = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '04},
doi = {10.1145/1014052.1014072},
isbn = {1581138889},
pages = {158},
publisher = {ACM Press},
title = {{Cyclic pattern kernels for predictive graph mining}},
url = {http://portal.acm.org/citation.cfm?doid=1014052.1014072},
year = {2004}
}
@inproceedings{Taira1999,
abstract = {Investigates the effect of prior feature selection in support vector machine (SVM) text categorization. The input space was gradually increased by using mutual information (MI) filtering and part-of-speech (POS) filtering, which determine the portion of words that are appropriate for learning from the information-theoretic and the linguistic perspectives, respectively. We tested the two filtering methods on SVMs as well as a decision tree algorithm, C4.5. The SVMs' results common to both filtering are that 1) the optimal number of features differed completely across categories, and 2) the average performance for all categories was best when all of the words were used. In addition, a comparison of the two filtering methods clarified that POS filtering on SVMs consistently outperformed MI filtering, which indicates that SVMs cannot find irrelevant parts of speech. These results suggest a simple strategy for the SVM text categorization: use a full number of words found through a rough filtering technique like part-of-speech tagging.},
author = {Taira, Hirotoshi and Haruno, Masahiko},
booktitle = {Proceedings of AAAI99 16th Conference of the American Association for Artificial Intelligence},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Taira, Haruno - 1999 - Feature Selection in SVM Text Categorization.pdf:pdf},
number = {4},
pages = {480--486},
publisher = {AAAI Press, Menlo Park, US},
title = {{Feature Selection in SVM Text Categorization}},
url = {http://www.springerlink.com/index/9rkk15dfy3rrcx41.pdf},
volume = {41},
year = {1999}
}
@inproceedings{bach2008,
address = {Pisa, Italy},
author = {Bach, Stephen H and Maloof, Marcus A},
booktitle = {Proc. of the 2008 Internl. Conf. on Data Mining (ICDM'08)},
pages = {23--32},
title = {{Paired Learners for Concept Drift}},
year = {2008}
}
@article{Altschul1990,
author = {Altschul, SF and Gish, W and Miller, W},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Altschul, Gish, Miller - 1990 - Basic local alignment search tool.pdf:pdf},
journal = {Journal of Molecular Biology},
number = {3},
pages = {403--410},
title = {{Basic local alignment search tool}},
url = {http://www.sciencedirect.com/science/article/pii/S0022283605803602},
volume = {215},
year = {1990}
}
@article{Mcmahan2014,
author = {Mcmahan, H Brendan and Orabona, Francesco},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mcmahan, Orabona - 2014 - Unconstrained Online Linear Learning in Hilbert Spaces Minimax Algorithms and Normal Approximations.pdf:pdf},
keywords = {minimax analysis,online convex optimization,online learning},
number = {2013},
pages = {1--20},
title = {{Unconstrained Online Linear Learning in Hilbert Spaces: Minimax Algorithms and Normal Approximations}},
volume = {35},
year = {2014}
}
@article{Zhang2012a,
author = {Zhang, Lijun and Jin, Rong and Chen, Chun and Bu, Jiajun and He, Xiaofei},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2012 - Efficient Online Learning for Large-Scale Sparse Kernel Logistic Regression.pdf:pdf},
journal = {AAAI},
keywords = {Machine Learning (Main Track)},
pages = {1219--1225},
title = {{Efficient Online Learning for Large-Scale Sparse Kernel Logistic Regression.}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/viewPDFInterstitial/5003/5544},
year = {2012}
}
@inproceedings{Zhang2009,
address = {Morristown, NJ, USA},
author = {Zhang, Lidan and Chan, Kwok-Ping},
booktitle = {GEMS '09: Proceedings of the Workshop on Geometrical Models of Natural Language Semantics},
pages = {25--32},
publisher = {Association for Computational Linguistics},
title = {{A study of convolution tree kernel with local alignment}},
year = {2009}
}
@article{Reyes2011,
abstract = {Discrepancies often arise among multiple informants' reports of child and adolescent psychopathology and related constructs (e.g., parenting, family relationship quality and functioning, parental monitoring). Recently, studies using various designs (laboratory, longitudinal, randomized controlled trial, meta-analysis) have revealed that discrepancies among informants' reports (a) yield important information regarding where children express behaviors (time course, features of the context[s] of behavioral expression) and about the informants who observe their expression, (b) demonstrate stability over time in both community and clinic settings, (c) predict poor child and adolescent outcomes in ways that the individual informants' reports do not, and (d) can be used to identify meaningful treatment outcomes patterns within randomized controlled trials. Using existing data sources, the articles in this special section expand upon this emerging body of research. In particular, the articles illustrate how clinical science and practice can use informant discrepancies to increase understanding of the causes and consequences of, as well as treatments for, child and adolescent psychopathology.},
author = {Reyes, Andres De Los},
doi = {10.1080/15374416.2011.533405},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Reyes - 2011 - Introduction to the special section more than measurement error discovering meaning behind informant discrepancies in cli.pdf:pdf},
issn = {1537-4424},
journal = {Journal of clinical child and adolescent psychology : the official journal for the Society of Clinical Child and Adolescent Psychology, American Psychological Association, Division 53},
month = {jan},
number = {1},
pages = {1--9},
pmid = {21229439},
title = {{Introduction to the special section: more than measurement error: discovering meaning behind informant discrepancies in clinical assessments of children and adolescents.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21229439},
volume = {40},
year = {2011}
}
@article{Helma2003,
author = {Helma, Christoph and Kramer, Stefan},
journal = {Bioinformatics},
number = {10},
pages = {1179--1182},
title = {{A Survey of the Predictive Toxicology Challenge 2000-2001}},
volume = {19},
year = {2003}
}
@article{Lemaire1991,
author = {Lemaire, J},
journal = {Astin Bullettin},
month = {apr},
number = {1},
pages = {17--40},
title = {{Cooperative game theory and its insurance applications}},
volume = {21},
year = {1991}
}
@article{Platt1999,
author = {Platt, JC},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Platt - 1999 - Fast Training of Support Vector Machines using Sequential Minimal Optimization.pdf:pdf},
title = {{Fast Training of Support Vector Machines using Sequential Minimal Optimization}},
url = {http://www.cs.utsa.edu/{~}bylander/cs6243/smo-book.pdf},
year = {1999}
}
@article{Schietgat2010,
author = {Schietgat, Leander},
doi = {10.3233/AIC-2010-0482},
journal = {Ai Communications},
keywords = {activity learning,graph mining,hierarchical multi-label classification,structure,structured prediction},
number = {May},
pages = {95--96},
title = {{Graph-based data mining for biological applications Leander Schietgat}},
volume = {24},
year = {2010}
}
@article{doi:10.1137/S0097539703432542,
author = {Cesa-Bianchi, N and Conconi, A and Gentile, C},
journal = {SIAM Journal on Computing},
number = {3},
pages = {640--668},
title = {{A Second-Order Perceptron Algorithm}},
volume = {34},
year = {2005}
}
@article{Tai2007a,
abstract = {Motivation: In the context of sample (e.g. tumor) classifications
with microarray gene expression data, many methods have been proposed.
However, almost all the methods ignore existing biological knowledge
and treat all the genes equally a priori. On the other hand, because
some genes have been identified by previous studies to have biological
functions or to be involved in pathways related to the outcome (e.g.
cancer), incorporating this type of prior knowledge into a classifier
can potentially improve both the predictive performance and interpretability
of the resulting model. Results: We propose a simple and general
framework to incorporate such prior knowledge into building a penalized
classifier. As two concrete examples, we apply the idea to two penalized
classifiers, nearest shrunken centroids (also called PAM) and penalized
partial least squares (PPLS). Instead of treating all the genes equally
a priori as in standard penalized methods, we group the genes according
to their functional associations based on existing biological knowledge
or data, and adopt group-specific penalty terms and penalization
parameters. Simulated and real data examples demonstrate that, if
prior knowledge on gene grouping is indeed informative, our new methods
perform better than the two standard penalized methods, yielding
higher predictive accuracy and screening out more irrelevant genes.
Contact: weip@biostat.umn.edu},
author = {Tai, Feng and Pan, Wei},
doi = {10.1093/bioinformatics/btm234},
journal = {Bioinformatics},
number = {14},
pages = {1775--1782},
title = {{Incorporating prior knowledge of predictors into penalized classifiers with multiple penalty terms}},
volume = {23},
year = {2007}
}
@article{Aggarwal2008,
author = {Aggarwal, C C},
journal = {Knowl. and Inform. Sys.},
number = {2},
pages = {137--156},
title = {{On classification and segmentation of massive audio data streams}},
volume = {20},
year = {2009}
}
@article{Gauzere2015,
abstract = {Abstract Chemoinformatics is a research field concerned with the study of physical or biological molecular properties through computer science׳s research fields such as machine learning and graph theory. From this point of view, graph kernels provide a nice framework which allows to naturally combine machine learning and graph theory techniques. Graph kernels based on bags of patterns have proven their efficiency on several problems both in terms of accuracy and computational time. Treelet kernel is a graph kernel based on a bag of small subtrees. We propose in this paper several extensions of this kernel devoted to chemoinformatics problems. These extensions aim to weight each pattern according to its influence, to include the comparison of non-isomorphic patterns, to include stereo information and finally to explicitly encode cyclic information into kernel computation.},
author = {Ga{\"{u}}z{\`{e}}re, Benoit and Grenier, Pierre-Anthony and Brun, Luc and Villemin, Didier},
doi = {http://dx.doi.org/10.1016/j.patcog.2014.07.029},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ga{\"{u}}z{\`{e}}re et al. - 2015 - Treelet kernel incorporating cyclic, stereo and inter pattern information in chemoinformatics.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Chemoinformatics,Graph kernel,Machine learning},
number = {2},
pages = {356--367},
title = {{Treelet kernel incorporating cyclic, stereo and inter pattern information in chemoinformatics}},
url = {http://www.sciencedirect.com/science/article/pii/S003132031400288X},
volume = {48},
year = {2015}
}
@inproceedings{Feragen2013,
author = {Feragen, Aasa and Kasenburg, Niklas and Petersen, Jens and de Bruijne, Marleen and Borgwardt, Karsten M.},
booktitle = {Neural Information Processing Systems (NIPS) 2013},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Feragen et al. - 2013 - Scalable kernels for graphs with continuous attributes.pdf:pdf},
keywords = {Forschungsgruppe Borgwardt,real labels continuous},
mendeley-tags = {real labels continuous},
pages = {216--224},
title = {{Scalable kernels for graphs with continuous attributes}},
url = {http://papers.nips.cc/paper/5155-scalable-kernels-for-graphs-with-continuous-attributes.pdf},
year = {2013}
}
@inproceedings{Shalev-Shwartz2004,
address = {New York, NY, USA},
author = {Shalev-Shwartz, Shai and Singer, Yoram and Ng, Andrew Y},
booktitle = {ICML '04: Proceedings of the twenty-first international conference on Machine learning},
doi = {http://doi.acm.org/10.1145/1015330.1015376},
isbn = {1-58113-828-5},
pages = {94},
publisher = {ACM},
title = {{Online and batch learning of pseudo-metrics}},
year = {2004}
}
@inproceedings{Kazama2006,
address = {Morristown, NJ, USA},
author = {Kazama, Jun'ichi and Torisawa, Kentaro},
booktitle = {CoNLL-X '06: Proceedings of the Tenth Conference on Computational Natural Language Learning},
pages = {53--60},
publisher = {Association for Computational Linguistics},
title = {{Semantic role recognition using kernels on weighted marked ordered labeled trees}},
year = {2006}
}
@inproceedings{Suzuki2004,
address = {Morristown, NJ, USA},
author = {Suzuki, Jun and Isozaki, Hideki and Maeda, Eisaku},
booktitle = {ACL '04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics},
doi = {http://dx.doi.org/10.3115/1218955.1218971},
pages = {119},
publisher = {Association for Computational Linguistics},
title = {{Convolution kernels with feature selection for natural language processing tasks}},
year = {2004}
}
@inproceedings{kdd99,
address = {$\backslash$mbox{\{}$\backslash$tt http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html{\}}},
author = {{The UCI KDD: University of California}},
title = {{KDD Cup 1999 Data}},
year = {1999}
}
@article{Hand2009,
author = {Hand, David J.},
doi = {10.1007/s10994-009-5119-5},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hand - 2009 - Measuring classifier performance a coherent alternative to the area under the ROC curve.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {auc,classification,cost,error rate,loss,misclassification,rate,roc curves,sensitivity,specificity},
month = {jun},
number = {1},
pages = {103--123},
title = {{Measuring classifier performance: a coherent alternative to the area under the ROC curve}},
url = {http://www.springerlink.com/index/10.1007/s10994-009-5119-5},
volume = {77},
year = {2009}
}
@article{Zhang2012b,
author = {Zhang, Kai and Lan, Liang and Wang, Zhuang and Moerchen, Fabian},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2012 - Scaling up Kernel SVM on Limited Resources A Low-rank Linearization Approach.pdf:pdf},
journal = {Proceedings of the 15th International Conference on Artificial Intelligence and Statistics},
pages = {1425--1434},
title = {{Scaling up Kernel SVM on Limited Resources: A Low-rank Linearization Approach}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2012{\_}ZhangLWM12.pdf},
volume = {XX},
year = {2012}
}
@article{Shin2011,
author = {Shin, Kilho},
journal = {Journal of Machine Learning Research},
keywords = {alignment,discrete structure,kernel},
pages = {367--382},
title = {{Mapping kernels defined over countably infinite mapping systems and their application}},
volume = {20},
year = {2011}
}
@article{Nejdl,
author = {Nejdl, Wolfgang and Wolf, Boris and Qu, Changtao and Decker, Stefan and Sintek, Michael and Naeve, Ambj{\"{o}}rn and Nilsson, Mikael and Palm{\'{e}}r, Matthias and Risch, Tore},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Nejdl et al. - Unknown - EDUTELLA A P2P Networking Infrastructure Based on RDF.pdf:pdf},
journal = {World Wide Web Internet And Web Information Systems},
title = {{EDUTELLA : A P2P Networking Infrastructure Based on RDF}}
}
@article{Gartner2003,
address = {New York, NY, USA},
author = {G{\"{a}}rtner, Thomas},
doi = {10.1145/959242.959248},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
keywords = {inductive logic programming,ing,kernel methods,multi-relational data min-,structured data},
month = {jul},
number = {1},
pages = {49},
publisher = {ACM},
title = {{A survey of kernels for structured data}},
url = {http://portal.acm.org/ft{\_}gateway.cfm?id=959248{\&}type=pdf{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=82529775{\&}CFTOKEN=62382615 http://portal.acm.org/citation.cfm?doid=959242.959248},
volume = {5},
year = {2003}
}
@article{li2007,
address = {Beijing, China},
author = {Hu, Xue-Gang and Li, Pei-Pei and Wu, Xin-Dong and Wu, Gong-Qing},
journal = {Journ. of Comp. Sci. and Tech.},
number = {5},
pages = {711--724},
publisher = {Institute of Computing Technology},
title = {{A semi-random multiple decision-tree algorithm for mining data streams}},
volume = {22},
year = {2007}
}
@article{Narayanan2016,
abstract = {In this paper, we propose a novel graph kernel specifically to address a challenging problem in the field of cyber-security, namely, malware detection. Previous research has revealed the following: (1) Graph representations of programs are ideally suited for malware detection as they are robust against several attacks, (2) Besides capturing topological neighbourhoods (i.e., structural information) from these graphs it is important to capture the context under which the neighbourhoods are reachable to accurately detect malicious neighbourhoods. We observe that state-of-the-art graph kernels, such as Weisfeiler-Lehman kernel (WLK) capture the structural information well but fail to capture contextual information. To address this, we develop the Contextual Weisfeiler-Lehman kernel (CWLK) which is capable of capturing both these types of information. We show that for the malware detection problem, CWLK is more expressive and hence more accurate than WLK while maintaining comparable efficiency. Through our large-scale experiments with more than 50,000 real-world Android apps, we demonstrate that CWLK outperforms two state-of-the-art graph kernels (including WLK) and three malware detection techniques by more than 5.27{\%} and 4.87{\%} F-measure, respectively, while maintaining high efficiency. This high accuracy and efficiency make CWLK suitable for large-scale real-world malware detection.},
archivePrefix = {arXiv},
arxivId = {1606.06369},
author = {Narayanan, Annamalai and Meng, Guozhu and Yang, Liu and Liu, Jinliang and Chen, Lihui},
eprint = {1606.06369},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Narayanan et al. - 2016 - Contextual Weisfeiler-Lehman Graph Kernel For Malware Detection.pdf:pdf},
keywords = {graph kernels,malware detection,program},
title = {{Contextual Weisfeiler-Lehman Graph Kernel For Malware Detection}},
url = {http://arxiv.org/abs/1606.06369},
year = {2016}
}
@inproceedings{Grossi2009,
author = {Grossi, Valerio and Sperduti, Alessandro},
booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Grossi, Sperduti - 2009 - Kernel-Based Selective Ensemble Learning for Streams of Trees.pdf:pdf},
keywords = {Machine Learning},
pages = {1281--1287},
title = {{Kernel-Based Selective Ensemble Learning for Streams of Trees}},
volume = {1},
year = {2009}
}
@techreport{Tsakalidis1988,
author = {Tsakalidis, A K and van Leeuwen, J},
institution = {Department of Computer Science, University of Utrecht, Utrecht},
number = {RUU-CS-88-17},
title = {{An Optimal Pointer Machine Algorithm for Finding Nearest Common Ancestors}}
}
@misc{Neuhaus_arandom,
author = {Neuhaus, Michel and Bunke, Horst},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neuhaus, Bunke - 2006 - A Random Walk Kernel Derived from Graph Edit Distance.pdf:pdf},
title = {{A Random Walk Kernel Derived from Graph Edit Distance}},
year = {2006}
}
@article{Driessens:kx,
author = {Driessens, K K and Driessens, K},
doi = {10.1007/s10994-006-8258-y},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Driessens, Driessens - 2003 - Graph kernels and Gaussian processes for relational reinforcement learning.pdf:pdf},
journal = {Machine Learning},
pages = {146--163},
title = {{Graph kernels and Gaussian processes for relational reinforcement learning}},
year = {2003}
}
@article{Narayanan2016a,
abstract = {In this paper, we present subgraph2vec, a novel approach for learning latent representations of rooted subgraphs from large graphs inspired by recent advancements in Deep Learning and Graph Kernels. These latent representations encode semantic substructure dependencies in a continuous vector space, which is easily exploited by statistical models for tasks such as graph classification, clustering, link prediction and community detection. subgraph2vec leverages on local information obtained from neighbourhoods of nodes to learn their latent representations in an unsupervised fashion. We demonstrate that subgraph vectors learnt by our approach could be used in conjunction with classifiers such as CNNs, SVMs and relational data clustering algorithms to achieve significantly superior accuracies. Also, we show that the subgraph vectors could be used for building a deep learning variant of Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and large-scale real-world datasets reveal that subgraph2vec achieves significant improvements in accuracies over existing graph kernels on both supervised and unsupervised learning tasks. Specifically, on two realworld program analysis tasks, namely, code clone and malware detection, subgraph2vec outperforms state-of-the-art kernels by more than 17{\%} and 4{\%}, respectively.},
archivePrefix = {arXiv},
arxivId = {1606.08928},
author = {Narayanan, Annamalai and Chandramohan, Mahinthan and Chen, Lihui and Liu, Yang and Saminathan, Santhoshkumar},
eprint = {1606.08928},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Narayanan et al. - 2016 - subgraph2vec Learning Distributed Representations of Rooted Sub-graphs from Large Graphs.pdf:pdf},
journal = {Acm},
keywords = {4d trajectory management,importance sampling,motion planning,separation assurance,tactical planning},
month = {jun},
number = {1},
pages = {4503},
title = {{subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs}},
url = {http://arxiv.org/abs/1606.08928},
year = {2016}
}
@article{Hagenbuchner2009,
abstract = {Self-organizing maps capable of processing graph structured information
are a relatively new concept. This paper describes a novel concept
on the processing of graph structured information using the self-organizing
map framework which allows the processing of much more general types
of graphs, e.g. cyclic graphs, directed graphs. Previous approaches
to this problem were limited to the processing of bounded graphs,
their computational complexity can grow rapidly with the level of
connectivity of the graphs concerned, and are restricted to the processing
of positional graphs. The novel concept proposed in this paper, namely,
by using the clusters formed in the state space of the self-organizing
map to represent the ''strengths'' of the activation of the neighboring
vertices, rather than as in previous approaches, using the state
space of the surrounding vertices to represent such ''strengths''
of activations. Such an approach resulted in reduced computational
demand, and in allowing the processing of non-positional graphs.},
author = {Hagenbuchner, Markus and Sperduti, Alessandro and Tsoi, Ah Chung},
journal = {Neurocomput.},
keywords = {graph{\_}processing,mapping{\_}and{\_}clustering{\_}of{\_}structured{\_}data,self-organizing maps unread neural{\_}networks},
number = {7-9},
pages = {1419--1430},
title = {{Graph self-organizing maps for cyclic and unbounded graphs}},
volume = {72},
year = {2009}
}
@article{Alippi,
author = {Alippi, Cesare and Ntalampiras, Stavros and Roveri, Manuel},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Alippi, Ntalampiras, Roveri - 2013 - A Cognitive Fault Diagnosis System for Distributed Sensor Networks.pdf:pdf},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
number = {8},
pages = {1--14},
title = {{A Cognitive Fault Diagnosis System for Distributed Sensor Networks}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6502725},
volume = {24},
year = {2013}
}
@article{Rietman2011,
abstract = {In this paper we provide a review of selected mathematical ideas that can help us better understand the boundary between living and non-living systems. We focus on group theory and abstract algebra applied to molecular systems biology. Throughout this paper we briefly describe possible open problems. In connection with the genetic code we propose that it may be possible to use perturbation theory to explore the adjacent possibilities in the 64-dimensional space-time manifold of the evolving genome. With regards to algebraic graph theory, there are several minor open problems we discuss. In relation to network dynamics and groupoid formalism we suggest that the network graph might not be the main focus for understanding the phenotype but rather the phase space of the network dynamics. We show a simple case of a C6 network and its phase space network. We envision that the molecular network of a cell is actually a complex network of hypercycles and feedback circuits that could be better represented in a higher-dimensional space. We conjecture that targeting nodes in the molecular network that have key roles in the phase space, as revealed by analysis of the automorphism decomposition, might be a better way to drug discovery and treatment of cancer.},
author = {Rietman, Edward a and Karp, Robert L and Tuszynski, Jack a},
doi = {10.1186/1742-4682-8-21},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rietman, Karp, Tuszynski - 2011 - Review and application of group theory to molecular systems biology.pdf:pdf},
issn = {1742-4682},
journal = {Theoretical biology {\&} medical modelling},
keywords = {Cell Cycle,Genetic Code,Models, Biological,Molecular Biology,Systems Biology},
month = {jan},
number = {1},
pages = {21},
pmid = {21696623},
publisher = {BioMed Central Ltd},
title = {{Review and application of group theory to molecular systems biology.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3149578{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {8},
year = {2011}
}
@article{Poelmans,
author = {Poelmans, Jonas and Elzinga, Paul and Viaene, Stijn and Dedene, Guido},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Poelmans et al. - Unknown - Formal Concept Analysis in Knowledge Discovery a Survey.pdf:pdf},
keywords = {exploratory data analysis,fca,formal concept analysis,knowledge discovery,mining,systematic literature overview,text},
number = {1999},
title = {{Formal Concept Analysis in Knowledge Discovery : a Survey}}
}
@misc{Mount2015,
author = {Mount, John},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mount - 2015 - How sure are you that large margin implies low VC dimension.pdf:pdf},
number = {1995},
pages = {1--26},
title = {{How sure are you that large margin implies low VC dimension?}},
volume = {20},
year = {2015}
}
@article{Gaber2005,
author = {Gaber, Mohamed Medhat and Zaslavsky, Arkady and Krishnaswamy, Shonali},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gaber, Zaslavsky, Krishnaswamy - 2005 - Mining Data Streams A Review.pdf:pdf},
journal = {ACM SIGMOD},
number = {2},
pages = {18--26},
title = {{Mining Data Streams : A Review}},
volume = {34},
year = {2005}
}
@misc{bgl,
title = {{The Boost Graph Library}},
url = {http://www.boost.org/doc/libs/1{\_}42{\_}0/libs/graph/doc/index.html}
}
@inproceedings{Manku2002,
author = {Manku, GS and Motwani, Rajeev},
booktitle = {VLDB},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Manku, Motwani - 2002 - Approximate frequency counts over data streams.pdf:pdf},
pages = {346--357},
title = {{Approximate frequency counts over data streams}},
url = {http://dl.acm.org/citation.cfm?id=1287400},
year = {2002}
}
@article{Cai1992,
author = {Cai, Jin-Yi and Furer, Martin and Immerman, Neil},
doi = {10.1007/BF01305232},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cai, Furer, Immerman - 1992 - An optimal lower bound on the number of variables for graph identification.pdf:pdf},
issn = {0209-9683},
journal = {Combinatorica},
month = {dec},
number = {4},
pages = {389--410},
title = {{An optimal lower bound on the number of variables for graph identification}},
url = {http://link.springer.com/10.1007/BF01305232},
volume = {12},
year = {1992}
}
@inproceedings{Anguita2004,
annote = {Perugia, Italy},
author = {Anguita, Davide and Ridella, Sandro and Rivieccio, Fabio},
booktitle = {Proceedings of the WIRN04 XV Italian Workshop on Neural Networks},
month = {sep},
title = {{An Algorithm for Reducing the Number of Support Vectors}},
year = {2004}
}
@inproceedings{Trentini2006,
abstract = {The number of XML documents produced and available on the Internet
is steadily increasing. It is thus important to devise automatic
procedures to extract useful information from them with little or
no intervention by a human operator. In this paper, we investigate
the efficacy of an unsupervised learning approach, namely Self-Organising
Maps (SOMs), for the automatic clustering of XML documents. Specifically,
we consider a relatively large corpus of XML formatted data from
the INEX initiative and evaluate it using two different self-organising
map models. The first model is the classical SOM model, and it requires
the XML documents to be represented by real-valued vectors, obtained
using a "bag of words" (or better a "bag of tags")
approach. The other model is the SOM for structured data (SOM-SD)
approach which is able to cluster structured data, and it is possible
to feed the model with tree structured representations of the XML
documents, thus explicitly preserving the structural information
in the documents. The experimental results show that the SOM model
exhibits quite a poor performance on this problem domain which requires
the ability to encode structural properties of the data. The SOM-SD
model, on the other hand, is able to produce a good clustering and
generalization performance.},
author = {Trentini, Francesca and Hagenbuchner, Markus and Sperduti, Alessandro and Scarselli, Franco},
booktitle = {Proceedings of the International Joint Conference on Neural Networks, IJCNN 2006, part of the IEEE World Congress on Computational Intelligence, WCCI 2006, Vancouver, BC, Canada, 16-21 July 2006},
pages = {1805--1812},
publisher = {IEEE Press},
title = {{A Self-Organising Map Approach for Clustering of XML Documents}},
year = {2006}
}
@phdthesis{Cardillo2007,
author = {Cardillo, Franco Alberto},
month = {oct},
school = {Dipartimento di Informatica, Universita' di Pisa},
title = {{Attention-based Object Detection}},
year = {2007}
}
@book{Scholkopf,
author = {and Scholkopf, Beranrd and Smola, Alexander},
isbn = {0262194759},
title = {{Learning with kernels}}
}
@article{Watanabe1960,
author = {Watanabe, S},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Watanabe - 1960 - Information theoretical analysis of multivariate correlation.pdf:pdf},
journal = {IBM Journal of Research and Development},
number = {1},
pages = {66--82},
title = {{Information theoretical analysis of multivariate correlation}},
volume = {4},
year = {1960}
}
@article{Anguita2004a,
author = {Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Ridella, Sandro},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Anguita et al. - 2004 - The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers.pdf:pdf},
pages = {1--9},
title = {{The Impact of Unlabeled Patterns in Rademacher Complexity Theory for Kernel Classifiers}},
year = {2004}
}
@phdthesis{Gaertner/diss,
annote = {[ buy it from {\{}$\backslash$}},
author = {G{\"{a}}rtner, Thomas},
school = {Universit{\{}{\&}{\}}auml;t Bonn},
title = {{Kernels for Structured Data}},
year = {2005}
}
@book{Aggarwal2010,
address = {Boston, MA},
author = {Aggarwal, Charu C},
booktitle = {Media},
doi = {10.1007/978-1-4419-6045-0},
editor = {Aggarwal, Charu C. and Wang, Haixun},
isbn = {978-1-4419-6044-3},
publisher = {Springer US},
series = {Advances in Database Systems},
title = {{Managing and Mining Graph Data}},
url = {http://www.springerlink.com/index/10.1007/978-1-4419-6045-0},
volume = {40},
year = {2010}
}
@article{Vidal2011,
abstract = {Complex biological systems and cellular networks may underlie most genotype to phenotype relationships. Here, we review basic concepts in network biology, discussing different types of interactome networks and the insights that can come from analyzing them. We elaborate on why interactome networks are important to consider in biology, how they can be mapped and integrated with each other, what global properties are starting to emerge from interactome network models, and how these properties may relate to human disease.},
author = {Vidal, Marc and Cusick, Michael E and Barab{\'{a}}si, Albert-L{\'{a}}szl{\'{o}}},
doi = {10.1016/j.cell.2011.02.016},
issn = {1097-4172},
journal = {Cell},
keywords = {Disease,Disease: genetics,Gene Regulatory Networks,Humans,Metabolic Networks and Pathways,Protein Interaction Mapping,Proteins,Proteins: metabolism,Systems Biology},
month = {mar},
number = {6},
pages = {986--98},
pmid = {21414488},
title = {{Interactome networks and human disease.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3102045{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {144},
year = {2011}
}
@article{smola_learning_????,
author = {Williamson smola},
journal = {learning the kernel with hyperkernels},
title = {{Learning Kernels with Hyperparameters}}
}
@inproceedings{655996,
address = {San Francisco, CA, USA},
annote = {From Duplicate 1 ( Diffusion Kernels on Graphs and Other Discrete Input Spaces - Kondor, Risi Imre; Lafferty, John D )



From Duplicate 2 ( Diffusion kernels on graphs and other discrete structures - Kondor, Risi Imre; Lafferty, John )

The application of kernel-based learning algorithms has, so far,
largely been confined to realvalued data and a few special data types,
such as strings. In this paper we propose a general method of constructing
natural families of kernels over discrete structures, based on the
matrix exponentiation idea. In particular, we focus on generating
kernels on graphs, for which we propose a special class of exponential
kernels, based on the heat equation, called diffusion kernels, and
show that these can be regarded as the discretization of the familiar
Gaussian kernel of Euclidean space.},
author = {Kondor, Risi Imre and Lafferty, John D},
booktitle = {ICML '02: Proceedings of the Nineteenth International Conference on Machine Learning},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kondor, Lafferty - 2002 - Diffusion Kernels on Graphs and Other Discrete Input Spaces.pdf:pdf},
howpublished = {http://eprints.kfupm.edu.sa/34988/},
isbn = {1-55860-873-7},
pages = {315--322},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Diffusion Kernels on Graphs and Other Discrete Input Spaces}},
type = {Article},
year = {2002}
}
@article{Will2007,
abstract = {The RFAM database defines families of ncRNAs by means of sequence similarities that are sufficient to establish homology. In some cases, such as microRNAs and box H/ACA snoRNAs, functional commonalities define classes of RNAs that are characterized by structural similarities, and typically consist of multiple RNA families. Recent advances in high-throughput transcriptomics and comparative genomics have produced very large sets of putative noncoding RNAs and regulatory RNA signals. For many of them, evidence for stabilizing selection acting on their secondary structures has been derived, and at least approximate models of their structures have been computed. The overwhelming majority of these hypothetical RNAs cannot be assigned to established families or classes. We present here a structure-based clustering approach that is capable of extracting putative RNA classes from genome-wide surveys for structured RNAs. The LocARNA (local alignment of RNA) tool implements a novel variant of the Sankoff algorithm that is sufficiently fast to deal with several thousand candidate sequences. The method is also robust against false positive predictions, i.e., a contamination of the input data with unstructured or nonconserved sequences. We have successfully tested the LocARNA-based clustering approach on the sequences of the RFAM-seed alignments. Furthermore, we have applied it to a previously published set of 3,332 predicted structured elements in the Ciona intestinalis genome (Missal K, Rose D, Stadler PF (2005) Noncoding RNAs in Ciona intestinalis. Bioinformatics 21 (Supplement 2): i77-i78). In addition to recovering, e.g., tRNAs as a structure-based class, the method identifies several RNA families, including microRNA and snoRNA candidates, and suggests several novel classes of ncRNAs for which to date no representative has been experimentally characterized.},
author = {Will, Sebastian and Reiche, Kristin and Hofacker, Ivo L and Stadler, Peter F and Backofen, Rolf},
doi = {10.1371/journal.pcbi.0030065},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Will et al. - 2007 - Inferring noncoding RNA families and classes by means of genome-scale structure-based clustering.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Animals,Chromosome Mapping,Chromosome Mapping: methods,Ciona intestinalis,Ciona intestinalis: genetics,Multigene Family,Multigene Family: genetics,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: classification,RNA, Untranslated: genetics,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Sequence Homology, Nucleic Acid},
month = {apr},
number = {4},
pages = {e65},
pmid = {17432929},
title = {{Inferring noncoding RNA families and classes by means of genome-scale structure-based clustering.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1851984{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {3},
year = {2007}
}
@article{Bartlett2005,
archivePrefix = {arXiv},
arxivId = {arXiv:math/0508275v1},
author = {Bartlett, Peter L. and Bousquet, Olivier and Mendelson, Shahar},
doi = {10.1214/009053605000000282},
eprint = {0508275v1},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bartlett, Bousquet, Mendelson - 2005 - Local Rademacher complexities.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {and phrases,article published by the,concentration inequalities,data-dependent complex-,error bounds,ity,rademacher averages,reprint of the original,this is an electronic},
month = {aug},
number = {4},
pages = {1497--1537},
primaryClass = {arXiv:math},
title = {{Local Rademacher complexities}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1123250221/},
volume = {33},
year = {2005}
}
@book{Gustafsson00,
author = {Gustafsson, Frederik},
booktitle = {Adaptive Filtering and Change Detection},
isbn = {ISBN: 978-0-470-85340-5},
month = {oct},
pages = {510},
publisher = {Wiley},
title = {{Adaptive Filtering and Change Detection}},
year = {2000}
}
@article{Yanardag2014,
abstract = {A commonly used paradigm for representing graphs is to use a vector that contains normalized frequencies of occurrence of certain motifs or sub-graphs. This vector representation can be used in a variety of applications, such as, for computing similarity between graphs. The graphlet kernel of Shervashidze et al. [32] uses induced sub-graphs of k nodes (christened as graphlets by Przulj [28]) as motifs in the vector representation, and computes the kernel via a dot product between these vectors. One can easily show that this is a valid kernel between graphs. However, such a vector representation suffers from a few drawbacks. As k becomes larger we encounter the sparsity problem; most higher order graphlets will not occur in a given graph. This leads to diagonal dominance, that is, a given graph is similar to itself but not to any other graph in the dataset. On the other hand, since lower order graphlets tend to be more numerous, using lower values of k does not provide enough discrimination ability. We propose a smoothing technique to tackle the above problems. Our method is based on a novel extension of Kneser-Ney and Pitman-Yor smoothing techniques from natural language processing to graphs. We use the relationships between lower order and higher order graphlets in order to derive our method. Consequently, our smoothing algorithm not only respects the dependency between sub-graphs but also tackles the diagonal dominance problem by distributing the probability mass across graphlets. In our experiments, the smoothed graphlet kernel outperforms graph kernels based on raw frequency counts.},
archivePrefix = {arXiv},
arxivId = {1403.0598},
author = {Yanardag, Pinar and Vishwanathan, S. V. N.},
eprint = {1403.0598},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Yanardag, Vishwanathan - 2014 - The Structurally Smoothed Graphlet Kernel.pdf:pdf},
isbn = {9781450314626},
journal = {arXiv},
title = {{The Structurally Smoothed Graphlet Kernel}},
url = {http://arxiv.org/abs/1403.0598},
year = {2014}
}
@article{Paris2008,
author = {Paris, Informatique De},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Paris - 2008 - Logistic regression for graph classification.pdf:pdf},
keywords = {graph mining,logistic regression},
number = {Siso},
title = {{Logistic regression for graph classification}},
volume = {2},
year = {2008}
}
@article{Edgar2004,
abstract = {We describe MUSCLE, a new computer program for creating multiple alignments of protein sequences. Elements of the algorithm include fast distance estimation using kmer counting, progressive alignment using a new profile function we call the log-expectation score, and refinement using tree-dependent restricted partitioning. The speed and accuracy of MUSCLE are compared with T-Coffee, MAFFT and CLUSTALW on four test sets of reference alignments: BAliBASE, SABmark, SMART and a new benchmark, PREFAB. MUSCLE achieves the highest, or joint highest, rank in accuracy on each of these sets. Without refinement, MUSCLE achieves average accuracy statistically indistinguishable from T-Coffee and MAFFT, and is the fastest of the tested methods for large numbers of sequences, aligning 5000 sequences of average length 350 in 7 min on a current desktop computer. The MUSCLE program, source code and PREFAB test data are freely available at http://www.drive5. com/muscle.},
author = {Edgar, Robert C.},
doi = {10.1093/nar/gkh340},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Edgar - 2004 - MUSCLE Multiple sequence alignment with high accuracy and high throughput.pdf:pdf},
isbn = {0305-1048},
issn = {03051048},
journal = {Nucleic Acids Research},
pages = {1792--1797},
pmid = {15034147},
title = {{MUSCLE: Multiple sequence alignment with high accuracy and high throughput}},
volume = {32},
year = {2004}
}
@article{Rapaport2007,
abstract = {{\{}BACKGROUND:Microarrays{\}} have become extremely useful for analysing
genetic phenomena, but establishing a relation between microarray
analysis results (typically a list of genes) and their biological
significance is often difficult. Currently, the standard approach
is to map a posteriori the results onto gene networks in order to
elucidate the functions perturbed at the level of pathways. However,
integrating a priori knowledge of the gene networks could help in
the statistical analysis of gene expression data and in their biological
{\{}interpretation.RESULTS:We{\}} propose a method to integrate a priori
the knowledge of a gene network in the analysis of gene expression
data. The approach is based on the spectral decomposition of gene
expression profiles with respect to the eigenfunctions of the graph,
resulting in an attenuation of the high-frequency components of the
expression profiles with respect to the topology of the graph. We
show how to derive unsupervised and supervised classification algorithms
of expression profiles, resulting in classifiers with biological
relevance. We illustrate the method with the analysis of a set of
expression profiles from irradiated and non-irradiated yeast {\{}strains.CONCLUSION:Including{\}}
a priori knowledge of a gene network for the analysis of gene expression
data leads to good classification performance and improved interpretability
of the results.},
author = {Rapaport, Franck and Zinovyev, Andrei and Dutreix, Marie and Barillot, Emmanuel and Vert, Jean-Philippe},
doi = {10.1186/1471-2105-8-35},
issn = {1471-2105},
journal = {{\{}BMC{\}} Bioinformatics},
number = {1},
pages = {35},
title = {{Classification of microarray data using gene networks}},
volume = {8},
year = {2007}
}
@article{DeBie2007,
abstract = {Motivation: Hunting disease genes is a problem of primary importance
in biomedical research. Biologists usually approach this problem
in two steps: first a set of candidate genes is identified using
traditional positional cloning or high-throughput genomics techniques;
second, these genes are further investigated and validated in the
wet lab, one by one. To speed up discovery and limit the number of
costly wet lab experiments, biologists must test the candidate genes
starting with the most probable candidates. So far, biologists have
relied on literature studies, extensive queries to multiple databases
and hunches about expected properties of the disease gene to determine
such an ordering. Recently, we have introduced the data mining tool
ENDEAVOUR (Aerts et al., 2006), which performs this task automatically
by relying on different genome-wide data sources, such as Gene Ontology,
literature, microarray, sequence and more. Results: In this article,
we present a novel kernel method that operates in the same setting:
based on a number of different views on a set of training genes,
a prioritization of test genes is obtained. We furthermore provide
a thorough learning theoretical analysis of the method's guaranteed
performance. Finally, we apply the method to the disease data sets
on which ENDEAVOUR (Aerts et al., 2006) has been benchmarked, and
report a considerable improvement in empirical performance. Availability:
The MATLAB code used in the empirical results will be made publicly
available. Contact: tijl.debie@gmail.com or yves.moreau@esat.kuleuven.be},
author = {{De Bie}, Tijl and Tranchevent, Leon-Charles and van Oeffelen, Liesbeth M M and Moreau, Yves},
doi = {10.1093/bioinformatics/btm187},
journal = {Bioinformatics},
number = {13},
pages = {125--132},
title = {{Kernel-based data fusion for gene prioritization}},
volume = {23},
year = {2007}
}
@article{Rahman2010,
abstract = {Question answering systems use information retrieval (IR) and information
extraction (IE) methods to retrieve documents containing a valid
answer. Question classification plays an important role in the question
answer frame to reduce the gap between question and answer. This
paper presents our research work on automatic question classification
through machine learning approaches. We have experimented with machine
learning algorithms Support Vector Machines (SVM) using kernel methods.
An effective way to integrate syntactic structures for question classification
in machine learning algorithms is the use of tree kernel (TK) functions.
Here we use SubTree kernel, SubSet Tree kernel with Bag of words
and Partial Tree kernels. Trade-off between training error and margin,
Costfactor and the decay factor has significant impact when we use
SVM for the above mentioned kernel types. The experiments determined
the individual impact for Trade-off between training error and margin,
Cost-factor and the decay factor and later the combined effect for
Trade-off between training error and margin, Cost-factor. For each
kernel types depending on these result we also figure out some hyper
planes which can maximize the performance. Based on some standard
data set outcomes of our experiment for question classification is
promising.},
author = {Rahman, Muhammad},
journal = {Journal of Computers},
keywords = {tree{\_}kernel unread SubSetTree SubTree PartialTree},
number = {1},
pages = {32--39},
title = {{Performance Evaluation for Question Classification by Tree Kernels using Support Vector Machines}},
volume = {5},
year = {2010}
}
@article{Kuksa2012,
author = {Kuksa, PP and Khan, Imdadullah and Pavlovic, Vladimir},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kuksa, Khan, Pavlovic - 2012 - Generalized Similarity Kernels for Efficient Sequence Classification.pdf:pdf},
journal = {SDM},
number = {February},
title = {{Generalized Similarity Kernels for Efficient Sequence Classification.}},
url = {http://epubs.siam.org/doi/abs/10.1137/1.9781611972825.75},
year = {2012}
}
@inproceedings{Kuboyama2006,
author = {Kuboyama, Tetsuji and Shin, Kilho and Kashima, Hisashi},
booktitle = {{\{}ECML/PKDD{\}} Workshop on Mining and Learning with Graphs},
title = {{Flexible Tree Kernels based on Counting the Number of Tree Mappings}},
year = {2006}
}
@inproceedings{Gama2004b,
author = {Gama, J and Medas, P and Castillo, G and Rodrigues, P},
booktitle = {SBIA Brazilian Symposium on Artificial Intelligence},
pages = {286--295},
title = {{Learning with drift detection}},
year = {2004}
}
@article{Vilcek,
author = {Vilcek, Alexandre},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vilcek - Unknown - Deep Learning with K-Means Applied to Community Detection in Networks.pdf:pdf},
pages = {1--9},
title = {{Deep Learning with K-Means Applied to Community Detection in Networks}}
}
@article{Yang2012,
abstract = {BACKGROUND: Identifying disease genes from human genome is an important but challenging task in biomedical research. Machine learning methods can be applied to discover new disease genes based on the known ones. Existing machine learning methods typically use the known disease genes as the positive training set P and the unknown genes as the negative training set N (non-disease gene set does not exist) to build classifiers to identify new disease genes from the unknown genes. However, such kind of classifiers is actually built from a noisy negative set N as there can be unknown disease genes in N itself. As a result, the classifiers do not perform as well as they could be.

RESULT: Instead of treating the unknown genes as negative examples in N, we treat them as an unlabeled set U. We design a novel positive-unlabeled (PU) learning algorithm PUDI (PU learning for disease gene identification) to build a classifier using P and U. We first partition U into four sets, namely, reliable negative set RN, likely positive set LP, likely negative set LN and weak negative set WN. The weighted support vector machines are then used to build a multi-level classifier based on the four training sets and positive training set P to identify disease genes. Our experimental results demonstrate that our proposed PUDI algorithm outperformed the existing methods significantly.

CONCLUSION: The proposed PUDI algorithm is able to identify disease genes more accurately by treating the unknown data more appropriately as unlabeled set U instead of negative set N. Given that many machine learning problems in biomedical research do involve positive and unlabeled data instead of negative data, it is possible that the machine learning methods for these problems can be further improved by adopting PU learning methods, as we have done here for disease gene identification.

AVAILABILITY AND IMPLEMENTATION: The executable program and data are available at http://www1.i2r.a-star.edu.sg/{\~{}}xlli/PUDI/PUDI.html.},
author = {Yang, Peng and Li, Xiao-Li and Mei, Jian-Ping and Kwoh, Chee-Keong and Ng, See-Kiong},
doi = {10.1093/bioinformatics/bts504},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Yang et al. - 2012 - Positive-unlabeled learning for disease gene identification.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Artificial Intelligence,Disease,Disease: genetics,Genes,Humans,Support Vector Machines},
month = {oct},
number = {20},
pages = {2640--7},
pmid = {22923290},
title = {{Positive-unlabeled learning for disease gene identification.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3467748{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {28},
year = {2012}
}
@inproceedings{Frohloch,
address = {New York, NY, USA},
author = {Fr{\"{o}}hlich, Holger and Wegner, J{\"{o}}rg K. and Sieker, Florian and Zell, Andreas},
booktitle = {ICML '05: Proceedings of the 22nd international conference on Machine learning},
doi = {http://doi.acm.org/10.1145/1102351.1102380},
isbn = {1-59593-180-5},
pages = {225--232},
publisher = {ACM},
title = {{Optimal assignment kernels for attributed molecular graphs}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102380},
year = {2005}
}
@article{Eskandari2012,
author = {Eskandari, Mojtaba and Hashemi, Sattar},
doi = {10.1016/j.jvlc.2012.02.002},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Eskandari, Hashemi - 2012 - A graph mining approach for detecting unknown malwares.pdf:pdf},
issn = {1045926X},
journal = {Journal of Visual Languages {\&} Computing},
keywords = {API,CFG,Detection,Malware,PE-file,Unknown malwares},
month = {jun},
number = {3},
pages = {154--162},
publisher = {Elsevier},
title = {{A graph mining approach for detecting unknown malwares}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1045926X12000146},
volume = {23},
year = {2012}
}
@article{Albert,
author = {Albert, I},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Albert - Unknown - Predicting Protein-Protein Interactions with Collaborative Filtering Algorithms.pdf:pdf},
journal = {cmgm.stanford.edu},
title = {{Predicting Protein-Protein Interactions with Collaborative Filtering Algorithms}},
url = {http://cmgm.stanford.edu/biochem218/Projects 2004/albert.pdf}
}
@article{Lib,
author = {Li, Li-jia and Su, Hao and Lim, Yongwhan and Fei-fei, Li},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - Unknown - Objects as Attributes for Scene Classification.pdf:pdf},
title = {{Objects as Attributes for Scene Classification}}
}
@article{Bach2009,
abstract = {For supervised and unsupervised learning, positive definite kernels allow to use
large and potentially infinite dimensional feature spaces with a computational cost
that only depends on the number of observations. This is usually done through
the penalization of predictor functions by Euclidean or Hilbertian norms. In this
paper, we explore penalizing by sparsity-inducing norms such as the ℓ1-norm or
the block ℓ1-norm. We assume that the kernel decomposes into a large sum of
individual basis kernels which can be embedded in a directed acyclic graph; we
show that it is then possible to perform kernel selection through a hierarchical
multiple kernel learning framework, in polynomial time in the number of selected
kernels. This framework is naturally applied to non linear variable selection; our
extensive simulations on synthetic datasets and datasets from the UCI repository
show that efficiently exploring the large feature space through sparsity-inducing
norms leads to state-of-the-art predictive performance.},
archivePrefix = {arXiv},
arxivId = {0809.1493},
author = {Bach, Francis},
eprint = {0809.1493},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bach - 2009 - Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning.pdf:pdf},
isbn = {9781605609492},
keywords = {Learning/Statistics {\&} Optimisation},
number = {2},
title = {{Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning}},
url = {http://eprints.pascal-network.org/archive/00004524/},
year = {2009}
}
@article{Richard2004,
author = {{Meraz, Richard F. {\&} Holbrook}, Stephen R.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Meraz, Richard F. {\&} Holbrook - 2004 - Classification of non-coding RNA using graph representations of secondary structure.pdf:pdf},
journal = {Science},
title = {{Classification of non-coding RNA using graph representations of secondary structure}},
year = {2004}
}
@article{Vishwanathan2010,
abstract = {We present a unified framework to study graph kernels, special cases
of which include the random walk (Gartner et al., 2003; Borgwardt
et al., 2005) and marginalized (Kashima et al., 2003, 2004; Mah et
al., 2004) graph kernels. Through reduction to a Sylvester equation
we improve the time complexity of kernel computation between unlabeled
graphs with n vertices from O(n6) to O(n3). We find a spectral decomposition
approach even more efficient when computing entire kernel matrices.
For labeled graphs we develop conjugate gradient and fixed-point
methods that take O(dn3) time per iteration, where d is the size
of the label set. By extending the necessary linear algebra to Reproducing
Kernel Hilbert Spaces (RKHS) we obtain the same result for d-dimensional
edge kernels, and O(n4) in the infinite-dimensional case; on sparse
graphs these algorithms only take O(n2) time per iteration in all
cases. Experiments on graphs from bioinformatics and other application
domains show that these techniques can speed up computation of the
kernel by an order of magnitude or more. We also show that certain
rational kernels (Cortes et al., 2002, 2003, 2004) when specialized
to graphs reduce to our random walk graph kernel. Finally, we relate
our framework to R-convolution kernels (Haussler, 1999) and provide
a kernel that is close to the optimal assignment kernel of kernel
of Froehlich et al. (2006) yet provably positive semi-definite.},
author = {Vishwanathan, S V N and Schraudolph, Nicol N and Kondor, Risi and Borgwardt, Karsten M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vishwanathan et al. - 2010 - Graph Kernels.pdf:pdf},
journal = {Journal of Machine Learning Research},
month = {apr},
pages = {1201--1242},
title = {{Graph Kernels}},
volume = {11},
year = {2010}
}
@article{Bul,
author = {Bul, Samuel Rota and Pelillo, Marcello},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bul, Pelillo - Unknown - A Game-Theoretic Approach to Hypergraph Clustering.pdf:pdf},
pages = {1--9},
title = {{A Game-Theoretic Approach to Hypergraph Clustering}}
}
@article{Herbrich2001,
author = {Herbrich, Ralf and Graepel, Thore and Campbell, Colin},
doi = {http://dx.doi.org/10.1162/153244301753683717},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {sep},
pages = {245--279},
publisher = {JMLR.org},
title = {{Bayes point machines}},
volume = {1},
year = {2001}
}
@article{Chapelle2009,
abstract = {This book addresses some theoretical aspects of semisupervised learning (SSL). The book is organized as a collection of different contributions of authors who are experts on this topic. The objectives of this book are to present a large overview of the SSL methods and to classify these methods into four classes that correspond to the first four main parts of the book (this would include generative models},
author = {Chapelle, O and Scholkopf, B and {Zien  A.}, Eds.},
doi = {10.1109/TNN.2009.2015974},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
pages = {542},
title = {{Semi-Supervised Learning (Chapelle, O. et al., Eds.}},
volume = {20},
year = {2009}
}
@article{Lin1997,
author = {Lin, I-jong and Kung, Sun-yuan},
doi = {10.1109/78.650096},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Lin, Kung - 1997 - Coding and comparison of DAG's as a novel neural structure with applications to on-line handwriting recognition.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {11},
pages = {2701--2708},
title = {{Coding and comparison of DAG's as a novel neural structure with applications to on-line handwriting recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=650096},
volume = {45},
year = {1997}
}
@article{Metwally2006,
author = {Metwally, A and Agrawal, D and Abbadi, AE},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Metwally, Agrawal, Abbadi - 2006 - An integrated efficient solution for computing frequent and top-k elements in data streams.pdf:pdf},
journal = {ACM Transactions on Database {\ldots}},
number = {3},
pages = {1095--1133},
title = {{An integrated efficient solution for computing frequent and top-k elements in data streams}},
url = {http://dl.acm.org/citation.cfm?id=1166084},
volume = {31},
year = {2006}
}
@article{Science2006,
author = {Science, Computer and Knopper, J W},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Science, Knopper - 2006 - Automatic Proofs of Graph Nonisomorphism.pdf:pdf},
number = {June},
title = {{Automatic Proofs of Graph Nonisomorphism}},
year = {2006}
}
@article{Rieck2010,
abstract = {Convolution kernels for trees provide simple means for learning with
tree-structured data. The computation time of tree kernels is quadratic
in the size of the trees, since all pairs of nodes need to be compared.
Thus, large parse trees, obtained from HTML documents or structured
network data, render convolution kernels inapplicable. In this article,
we propose an effective approximation technique for parse tree kernels.
The approximate tree kernels (ATKs) limit kernel computation to a
sparse subset of relevant subtrees and discard redundant structures,
such that training and testing of kernel-based learning methods are
significantly accelerated. We devise linear programming approaches
for identifying such subsets for supervised and unsupervised learning
tasks, respectively. Empirically, the approximate tree kernels attain
run-time improvements up to three orders of magnitude while preserving
the predictive accuracy of regular tree kernels. For unsupervised
tasks, the approximate tree kernels even lead to more accurate predictions
by identifying relevant dimensions in feature space.},
author = {Rieck, Konrad and Krueger, Tammo and Brefeld, Ulf and Mueller, Klaus-Robert},
journal = {Journal of Machine Learning Research},
month = {feb},
pages = {555--580},
title = {{Approximate Tree Kernels}},
volume = {11},
year = {2010}
}
@article{Bengio2012,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
eprint = {1206.5538},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bengio, Courville, Vincent - 2012 - Representation Learning A Review and New Perspectives.pdf:pdf},
month = {jun},
number = {1993},
pages = {1--30},
title = {{Representation Learning: A Review and New Perspectives}},
url = {http://arxiv.org/abs/1206.5538},
year = {2012}
}
@article{Kohavi1995,
author = {Kohavi, Ron},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kohavi - 1995 - A study of cross-validation and bootstrap for accuracy estimation and model selection.pdf:pdf},
journal = {IJCAI},
number = {2},
pages = {1137--1145},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection}},
url = {http://frostiebek.free.fr/docs/Machine Learning/validation-1.pdf},
volume = {14},
year = {1995}
}
@book{559923,
address = {Cambridge, MA, USA},
author = {Scholkopf, Bernhard and Smola, Alexander J},
isbn = {0262194759},
publisher = {MIT Press},
title = {{Learning with Kernels}},
year = {2001}
}
@article{Cortes1995a,
abstract = {The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high- dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demon- strated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/BF00994018},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Vapnik - 1995 - Support-vector networks.pdf:pdf},
isbn = {0885-6125},
issn = {08856125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {9052598814225336358},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@inproceedings{Fung2001,
address = {New York},
annote = {ftp://ftp.cs.wisc.edu/pub/dmi/tech-reports/01-02.ps},
author = {Fung, Glenn and Mangasarian, Olvi L},
booktitle = {Proceedings KDD-2001: Knowledge Discovery and Data Mining, August 26-29, 2001, San Francisco, CA},
editor = {Provost, F and Srikant, R},
pages = {77--86},
publisher = {Asscociation for Computing Machinery},
title = {{Proximal Support Vector Machine Classifiers}},
year = {2001}
}
@inproceedings{DBLP:conf/nips/2002,
booktitle = {NIPS},
editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
isbn = {0-262-02550-7},
publisher = {MIT Press},
title = {{Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]}},
year = {2003}
}
@article{Cawley2010,
author = {Cawley, Gavin C and Talbot, Nicola L C},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cawley, Talbot - 2010 - On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation.pdf:pdf},
keywords = {bias-variance trade-off,model selection,over-,performance evaluation,selection bias},
pages = {2079--2107},
title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
volume = {11},
year = {2010}
}
@misc{shogun,
title = {{Shogun Machine learning toolbox}},
url = {http://www.shogun-toolbox.org/}
}
@inproceedings{Kolter05,
address = {Bonn, Germany},
author = {Kolter, J Z and Maloof, M A},
booktitle = {Proc. of the 22nd Internl. Conf. on Machine learning (ICML'05)},
pages = {449--456},
title = {{Using additive expert ensembles to cope with concept drift}},
year = {2005}
}
@inproceedings{Zhang2013,
author = {Zhang, Lijun and Yi, Jinfeng and Jin, Rong and Lin, Ming and He, Xiaofei},
booktitle = {ICML},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2013 - Online kernel learning with a near optimal sparsity bound.pdf:pdf},
keywords = {Online learning,kernel method,regret bound,sparsity bound},
title = {{Online kernel learning with a near optimal sparsity bound}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013{\_}zhang13c},
volume = {28},
year = {2013}
}
@inproceedings{Birlinghoven,
address = {Berlin, Heidelberg},
author = {Neumann, Marion and Patricia, Novi and Garnett, Roman and Kersting, Kristian},
booktitle = {ECML PKDD},
doi = {10.1007/978-3-642-33460-3},
editor = {Flach, Peter A. and {De Bie}, Tijl and Cristianini, Nello},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neumann et al. - 2012 - Efficient Graph Kernels by Randomization.pdf:pdf},
isbn = {978-3-642-33459-7},
pages = {378--393},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Efficient Graph Kernels by Randomization}},
url = {http://link.springer.com/10.1007/978-3-642-33460-3},
volume = {7523},
year = {2012}
}
@article{King1995,
author = {King, Ross D and Sternberg, Michael J E and Srinivasan, Ashwin},
doi = {10.1007/BF03037232},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/King, Sternberg, Srinivasan - 1995 - Relating chemical activity to structure An examination of ILP successes.pdf:pdf},
issn = {0288-3635},
journal = {New Generation Computing},
month = {dec},
number = {3-4},
pages = {411--433},
title = {{Relating chemical activity to structure: An examination of ILP successes}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.1260 http://link.springer.com/10.1007/BF03037232},
volume = {13},
year = {1995}
}
@article{Fuglede,
author = {Fuglede, B. and Topsoe, F.},
doi = {10.1109/ISIT.2004.1365067},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fuglede, Topsoe - Unknown - Jensen-Shannon divergence and Hilbert space embedding.pdf:pdf},
isbn = {0-7803-8280-3},
journal = {International Symposium onInformation Theory, 2004. ISIT 2004. Proceedings.},
pages = {30--30},
publisher = {Ieee},
title = {{Jensen-Shannon divergence and Hilbert space embedding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1365067}
}
@article{Furer1987,
author = {Furer, M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Furer - 1987 - A counterexample in graph isomorphism testing.pdf:pdf},
number = {December},
title = {{A counterexample in graph isomorphism testing}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+counterexample+in+graph+isomorphism+testing.{\#}1},
year = {1987}
}
@article{Pelckmans2008,
author = {Pelckmans, Kristiaan and Suykens, JA},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Pelckmans, Suykens - 2008 - An online algorithm for learning a labeling of a graph.pdf:pdf},
journal = {Mining and Learning with Graphs},
number = {1},
pages = {1--3},
title = {{An online algorithm for learning a labeling of a graph}},
url = {ftp://www.interspeech2007.org/sista/kpelckma/MLG08{\_}graphtron.pdf},
volume = {4},
year = {2008}
}
@book{Poranen2004,
author = {Poranen, Timo},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Poranen - 2004 - Approximation Algorithms for Some Topological Invariants of Graphs.pdf:pdf},
isbn = {9514460987},
title = {{Approximation Algorithms for Some Topological Invariants of Graphs}},
year = {2004}
}
@article{Mattei2014a,
abstract = {Structural information is crucial in ribonucleic acid (RNA) analysis and functional annotation; nevertheless, how to include such structural data is still a debated problem. Dot-bracket notation is the most common and simple representation for RNA secondary structures but its simplicity leads also to ambiguity requiring further processing steps to dissolve. Here we present BEAR (Brand nEw Alphabet for RNA), a new context-aware structural encoding represented by a string of characters. Each character in BEAR encodes for a specific secondary structure element (loop, stem, bulge and internal loop) with specific length. Furthermore, exploiting this informative and yet simple encoding in multiple alignments of related RNAs, we captured how much structural variation is tolerated in RNA families and convert it into transition rates among secondary structure elements. This allowed us to compute a substitution matrix for secondary structure elements called MBR (Matrix of BEAR-encoded RNA secondary structures), of which we tested the ability in aligning RNA secondary structures. We propose BEAR and the MBR as powerful resources for the RNA secondary structure analysis, comparison and classification, motif finding and phylogeny.},
author = {Mattei, Eugenio and Ausiello, Gabriele and Ferr{\`{e}}, Fabrizio and Helmer-Citterich, Manuela},
doi = {10.1093/nar/gku283},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mattei et al. - 2014 - A novel approach to represent and compare RNA secondary structures(2).pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Algorithms,Computational Biology,Computational Biology: methods,Nucleic Acid Conformation,RNA,RNA: chemistry,Sequence Analysis, RNA},
month = {jan},
number = {10},
pages = {6146--57},
pmid = {24753415},
title = {{A novel approach to represent and compare RNA secondary structures.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4041456{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {42},
year = {2014}
}
@article{Rosenblatt1958,
author = {Rosenblatt, F},
journal = {Psychological review},
number = {6},
pages = {386--408},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@article{Ben-david2002,
author = {Ben-david, Shai and Eiron, Nadav and Simon, Hans Ulrich},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ben-david, Eiron, Simon - 2002 - Limitations of Learning Via Embeddings in Euclidean Half Spaces.pdf:pdf},
pages = {441--461},
title = {{Limitations of Learning Via Embeddings in Euclidean Half Spaces}},
volume = {3},
year = {2002}
}
@article{Pas2008,
abstract = {Pathway information provides insight into the biological processes
underlying microarray data. Pathway information is widely available
for humans and laboratory animals in databases through the internet,
but less for other species, for example, livestock. Many software
packages use species-specific gene IDs that cannot handle genomics
data from other species. We developed a species-independent method
to search pathways databases to analyse microarray data. Three PERL
scripts were developed that use the names of the genes on the microarray.
(1) Add synonyms of gene names by searching the Gene Ontology (GO)
database. (2) Search the Kyoto Encyclopaedia of Genes and Genomes
(KEGG) database for pathway information using this GO-enriched gene
list. (3) Combine the pathway data with the microarray data and visualize
the results using color codes indicating regulation. To demonstrate
the power of the method, we used a previously reported chicken microarray
experiment investigating line-specific reactions to Salmonella infection
as an example.},
author = {te Pas, M F W and van Hemert, S and Hulsegge, B and Hoekman, A J W and Pool, M H and Rebel, J M J and Smits, M A},
doi = {10.1155/2008/719468},
journal = {Advances in Bioinformatics},
month = {dec},
pages = {7},
title = {{A Pathway Analysis Tool for Analyzing Microarray Data of Species with Low Physiological Information}},
volume = {2008},
year = {2008}
}
@article{Babai1983,
author = {Babai, L and Luks, EM},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Babai, Luks - 1983 - Canonical labeling of graphs.pdf:pdf},
journal = {Proceedings of the fifteenth annual ACM symposium {\ldots}},
pages = {171--183},
title = {{Canonical labeling of graphs}},
url = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf http://dl.acm.org/citation.cfm?id=808746},
year = {1983}
}
@article{Lenzerini,
author = {Lenzerini, Maurizio},
title = {{Data Integration: A Theoretical Perspective}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.9907}
}
@article{Zanzotto2010,
author = {Zanzotto, Fabio Massimo and Dell'Arciprete, Lorenzo and Moschitti, Alessandro},
journal = {Fundamenta Informaticae Journal},
title = {{Efficient Graph Kernels for Textual Entailment Recognition}},
year = {2010}
}
@article{Ramon2003,
abstract = {Recently, kernel methods have become a popular tool for machine learning
and data mining. As most "real-world" data is structured,

research in kernel methods has begun investigating kernels for various
kinds of structured data. One of the most widely used tools for modeling

structured data are graphs. In this paper we study the trade-off between
expressivity and efficiency of graph kernels. First, we motivate
the need

for this discussion by showing that fully general graph kernels can
not even be approximated efficiently.We also discuss generalizations
of graph

kernels defined in literature and show that they are either not positive
definite or not very useful. Finally, we propose a new graph kernel
based

on subtree patterns. We argue that while a little more computationally
expensive, this kernel is more expressive than kernels based on walks.},
author = {Ramon, J and G{\"{a}}rtner, Thomas},
journal = {Proceedings of the First International Workshop on Mining Graphs, Trees and Sequences {\{}(MGTS-2003){\}}},
pages = {65--74},
title = {{Expressivity versus Efficiency of Graph Kernels}},
year = {2003}
}
@article{Foggia2007,
author = {Foggia, Pasquale and Vento, Mario and Elettrica, Ingegneria},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Foggia, Vento, Elettrica - 2007 - Challenging Complexity of Maximum Common Subgraph Detection Algorithms A Performance Analysis of Thre.pdf:pdf},
journal = {Journal of Graph Algorithms and Applications},
number = {1},
pages = {99--143},
title = {{Challenging Complexity of Maximum Common Subgraph Detection Algorithms : A Performance Analysis of Three Algorithms on a Wide Database of Graphs Donatello Conte}},
volume = {11},
year = {2007}
}
@article{Kunegis2008,
author = {Kunegis, J{\'{e}}r{\^{o}}me and Lommatzsch, Andreas and Albayrak, Sahin and Bauckhage, Christian},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kunegis et al. - 2008 - On the Scalability of Graph Kernels Applied to Collaborative Recommenders.pdf:pdf},
journal = {The 18th European Conference on Artificial Intelligence Proceedings - Workshop on Recommender Systems Tuesday July 22, 2008 Patras, Greece},
pages = {35--38},
title = {{On the Scalability of Graph Kernels Applied to Collaborative Recommenders}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:On+the+Scalability+of+Graph+Kernels+Applied+to+Collaborative+Recommenders{\#}0},
year = {2008}
}
@article{Ong2005,
abstract = {This paper addresses the problem of choosing a kernel suitable for
estimation with a support vector machine, hence further automating
machine learning. This goal is achieved by defining a reproducing
kernel Hilbert space on the space of kernels itself. Such a formulation
leads to a statistical estimation problem similar to the problem
of minimizing a regularized risk functional. 

We state the equivalent representer theorem for the choice of kernels
and present a semidefinite programming formulation of the resulting
optimization problem. Several recipes for constructing hyperkernels
are provided, as well as the details of common machine learning problems.
Experimental results for classification, regression and novelty detection
on {\{}UCI{\}} data show the feasibility of our approach.},
author = {Ong, Cheng Soon and Smola, Alexander J and Williamson, Robert C},
journal = {Journal of Machine Learning Research},
month = {jul},
pages = {1043--1071},
title = {{Learning the Kernel with Hyperkernels}},
volume = {6},
year = {2005}
}
@article{Wachman2003,
author = {Wachman, Gabriel},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wachman - 2003 - Learning from Interpretations A Rooted Kernel for Ordered Hypergraphs.pdf:pdf},
journal = {Computer},
pages = {943--950},
title = {{Learning from Interpretations : A Rooted Kernel for Ordered Hypergraphs}},
year = {2003}
}
@article{Downs2002,
author = {Downs, Tom and Gates, Kevin E and Masters, Annette},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {293--297},
publisher = {JMLR.org},
title = {{Exact simplification of support vector solutions}},
volume = {2},
year = {2002}
}
@article{Zhang2002,
author = {Zhang, Tong},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang - 2002 - Effective dimension and generalization of kernel learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
title = {{Effective dimension and generalization of kernel learning}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/LT18.pdf},
year = {2002}
}
@techreport{Platt1998,
abstract = {This paper proposes a new algorithm for training support vector machines:
Sequential

Minimal Optimization, or SMO. Training a support vector machine requires
the solution of

a very large quadratic programming (QP) optimization problem. SMO
breaks this large

QP problem into a series of smallest possible QP problems. These small
QP problems are

solved analytically, which avoids using a time-consuming numerical
QP optimization as an

inner loop. The amount of memory required for SMO is linear in the
training set size,

which allows SMO to handle very large training sets. Because matrix
computation is

avoided, SMO scales somewhere between linear and quadratic in the
training set size for

various test problems, while the standard chunking SVM algorithm scales
somewhere

between linear and cubic in the training set size. SMO's computation
time is dominated by

SVM evaluation, hence SMO is fastest for linear SVMs and sparse data
sets. 

On real-world sparse data sets, SMO can be more than 1000 times faster
than the chunking

algorithm.},
author = {Platt, John C},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Platt - 1998 - Sequential minimal optimization A fast algorithm for training support vector machines.pdf:pdf},
institution = {Microsoft Research},
number = {MSR-TR-98-14},
title = {{Sequential minimal optimization: A fast algorithm for training support vector machines}},
year = {1998}
}
@article{Widmer1996,
author = {Widmer, G and Kubat, M},
journal = {Mach. Learn.},
number = {1},
pages = {69--101},
title = {{Learning in the presence of concept drift and hidden contexts}},
volume = {23},
year = {1996}
}
@article{Kudo,
author = {Kudo, Taku and Maeda, E and Matsumoto, Y},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kudo, Maeda, Matsumoto - 2004 - An application of boosting to graph classification.pdf:pdf},
journal = {Advances in neural {\ldots}},
title = {{An application of boosting to graph classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2005{\_}369.pdf},
year = {2004}
}
@book{Kostakis2014,
abstract = {An abstraction resilient to common malware obfuscation techniques is the$\backslash$ncall-graph. A call-graph is the representation of an executable file as$\backslash$na directed graph with labeled vertices, where the vertices correspond to$\backslash$nfunctions and the edges to function calls. Unfortunately, most of the$\backslash$ninteresting graph comparison problems, including full-graph comparison$\backslash$nand computing the largest common subgraph, belong to the -hard class.$\backslash$nThis makes the study and use of graphs in large scale systems difficult.$\backslash$nExisting work has focused only on offline clustering and has not$\backslash$naddressed the issue of clustering streams of graphs. In this paper we$\backslash$npresent Classy, a scalable distributed system that clusters streams of$\backslash$nlarge call-graphs for purposes including automated malware$\backslash$nclassification and facilitating malware analysts. Since algorithms aimed$\backslash$nat clustering sets are not suitable for clustering streams of objects,$\backslash$nwe propose the use of a clustering algorithm that relies on the notion$\backslash$nof candidate clusters and reference samples therein. We demonstrate via$\backslash$nthorough experimentation that this approach yields results very close to$\backslash$nthe offline optimal. Graph similarity is determined by computing a graph$\backslash$nedit distance (GED) of pairs of graphs using an adapted version of$\backslash$nsimulated annealing. Furthermore, we present a novel lower bound for the$\backslash$nGED. We also study the problem of approximating statistics of clusters$\backslash$nof graphs when the distances of only a fraction of all possible pairs$\backslash$nhave been computed. Finally, we present results and statistics from a$\backslash$nreal production-side system that has clustered and contains more than$\backslash$n0.8 million graphs.},
author = {Kostakis, Orestis},
booktitle = {Data Mining and Knowledge Discovery},
doi = {10.1007/s10618-014-0367-9},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kostakis - 2014 - Classy Fast clustering streams of call-graphs.pdf:pdf},
isbn = {1061801403},
issn = {13845810},
keywords = {Call-graphs,Clustering,Graph edit distance,Malware,Streams},
number = {5-6},
pages = {1554--1585},
title = {{Classy: Fast clustering streams of call-graphs}},
volume = {28},
year = {2014}
}
@inproceedings{pfahringer2008,
address = {Osaka, Japan},
author = {Pfahringer, B and Holmes, G and Kirkby, R},
booktitle = {PAKDD'08},
pages = {296--307},
title = {{Handling Numeric Attributes in Hoeffding Trees}},
year = {2008}
}
@inproceedings{Grossi2010,
address = {Innsbruck, Austria},
author = {Grossi, Valerio and Turini, Franco},
booktitle = {Proc. of the 10th Internl. Artificial Intelligence and Applications (AIA2010)},
pages = {339--346},
title = {{A New Selective Ensemble Approach for Data Streams Classification}},
year = {2010}
}
@inproceedings{Salakhutdinov2007,
abstract = {We show how to learn a deep graphical model of the word-count vectors
obtained from a large set of documents. The values of the latent
variables in the deepest layer are easy to infer and give a much
better representation of each document than Latent Semantic Analysis.
When the deepest layer is forced to use a small number of binary
variables (e.g. ), the graphical model performs "semantic hashing":
Documents are mapped to memory addresses in such a way that semantically
similar documents are located at nearby addresses. Documents similar
to a query document can then be found by simply accessing all the
addresses that differ by only a few bits from the address of the
query document. This way of extending the efficiency of hash-coding
to approximate matching is much faster than locality sensitive hashing,
which is the fastest current method. By using semantic hashing to
filter the documents given to TF-IDF, we achieve higher accuracy
than applying TF-IDF to the entire document set.},
address = {Amsterdam},
author = {Salakhutdinov, Ruslan R and Hinton, Geoffrey E},
booktitle = {Proceedings of the SIGIR Workshop on Information Retrieval and Applications of Graphical Models},
publisher = {Elsevier},
title = {{Semantic Hashing}},
year = {2007}
}
@article{Gauzere2012,
author = {Ga{\"{u}}z{\`{e}}re, Benoit and Brun, Luc and Villemin, Didier},
doi = {10.1016/j.patrec.2012.03.020},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ga{\"{u}}z{\`{e}}re, Brun, Villemin - 2012 - Two new graphs kernels in chemoinformatics.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
month = {nov},
number = {15},
pages = {2038--2047},
title = {{Two new graphs kernels in chemoinformatics}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S016786551200102X},
volume = {33},
year = {2012}
}
@article{Wei2006,
abstract = {High-throughout genomic data provide an opportunity for identifying
pathways and genes that are related to various clinical phenotypes.
Besides these genomic data, another valuable source of data is the
biological knowledge about genes and pathways that might be related
to the phenotypes of many complex diseases. Databases of such knowledge
are often called the metadata. In microarray data analysis, such
metadata are currently explored in post hoc ways by gene set enrichment
analysis but have hardly been utilized in the modeling step. We propose
to develop and evaluate a pathway-based gradient descent boosting
procedure for nonparametric pathways-based regression {\{}(NPR){\}} analysis
to efficiently integrate genomic data and metadata. Such {\{}NPR{\}} models
consider multiple pathways simultaneously and allow complex interactions
among genes within the pathways and can be applied to identify pathways
and genes that are related to variations of the phenotypes. These
methods also provide an alternative to mediating the problem of a
large number of potential interactions by limiting analysis to biologically
plausible interactions between genes in related pathways. Our simulation
studies indicate that the proposed boosting procedure can indeed
identify relevant pathways. Application to a gene expression data
set on breast cancer distant metastasis identified that Wnt, apoptosis
and cell cycle regulated pathways are more likely related to the
risk of distant metastasis among lymph-node-negative breast cancer
patients. Results from analysis of other two breast cancer gene expression
data sets indicate that the pathways of Metalloendopeptidases {\{}(MMPs){\}}
and {\{}MMP{\}} inhibitors, as well as cell proliferation, cell growth
and maintenance are important to breast cancer relapse and survival.
We also observed that by incorporating the pathway information, we
achieved better prediction for cancer recurrence.},
author = {Wei, Zhi and Li, Hongzhe},
doi = {10.1093/biostatistics/kxl007},
journal = {Biostatistics},
month = {jun},
number = {2},
pages = {262--284},
title = {{Nonparametric pathway-based regression models for analysis of genomic data}},
volume = {8},
year = {2006}
}
@inproceedings{Johansson2014,
abstract = {Applications of machine learning methods increasingly deal with graph structured data through kernels. Most existing graph kernels compare graphs in terms of features defined on small subgraphs such as walks, paths or graphlets, adopting an inherently local perspective. However, several interesting properties such as girth or chromatic number are global properties of the graph, and are not captured in local substructures. This paper presents two graph kernels defined on unlabeled graphs which capture global properties of graphs using the celebrated Lov{\'{a}}sz number and its associated orthonormal representation. We make progress towards theoretical results aiding kernel choice, proving a result about the separation margin of our kernel for classes of graphs. We give empirical results on classification of synthesized graphs with important global properties as well as established benchmark graph datasets, showing that the accuracy of our kernels is better than or competitive to existing graph kernels.},
author = {{Fredrik Johansson and Vinay Jethava and Devdatt Dubhashi and Chiranjib Bhattacharyya}},
booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fredrik Johansson and Vinay Jethava and Devdatt Dubhashi and Chiranjib Bhattacharyya - 2014 - Global graph kernels using geometric embed.pdf:pdf},
pages = {694--702},
title = {{Global graph kernels using geometric embeddings}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2014c2{\_}johansson14},
volume = {32},
year = {2014}
}
@article{Adomavicius2005,
author = {Adomavicius, G. and Tuzhilin, a.},
doi = {10.1109/TKDE.2005.99},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Adomavicius, Tuzhilin - 2005 - Toward the next generation of recommender systems a survey of the state-of-the-art and possible extension.pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
month = {jun},
number = {6},
pages = {734--749},
title = {{Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1423975},
volume = {17},
year = {2005}
}
@book{Mitchell:1997:ML,
address = {New York},
author = {Mitchell, Tom M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mitchell - 1997 - Machine Learning.pdf:pdf},
keywords = {machine learning},
publisher = {McGraw-Hill},
title = {{Machine Learning}},
year = {1997}
}
@article{Zhang2009,
author = {Zhang, Heping and Yang, Yujun and Li, Chuanwen},
doi = {10.1016/j.dam.2009.03.007},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Yang, Li - 2009 - Kirchhoff index of composite graphs☆.pdf:pdf},
issn = {0166218X},
journal = {Discrete Applied Mathematics},
month = {jul},
number = {13},
pages = {2918--2927},
publisher = {Elsevier B.V.},
title = {{Kirchhoff index of composite graphs☆}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0166218X09001048},
volume = {157},
year = {2009}
}
@inproceedings{Masud2008,
address = {Pisa, Italy},
author = {Masud, M M and Gao, J and Khan, L and Han, J},
booktitle = {Proc. of the 8th IEEE Internl. Conf. of Data Mining (ICDM'08)},
pages = {929--934},
title = {{A practical approach to classify evolving data streams: training with limited amount of labeled data}},
year = {2008}
}
@article{gls:hypertrees,
author = {Gottlob, Georg and Leone, Nicola and Scarcello, Francesco},
journal = {Journal of Computer and System Sciences},
month = {may},
number = {3},
pages = {579--627},
title = {{Hypertree Decompositions and Tractable Queries}},
volume = {64},
year = {2002}
}
@inproceedings{Agrawal,
author = {Agrawal, Rakesh},
booktitle = {Proc 20th Int Conf Very Large Data Bases VLDB},
pages = {487--499},
title = {{Fast Algorithms for Mining Association Rules}},
year = {1994}
}
@inproceedings{Borgwardt2007a,
abstract = {As new graph structured data is being generated, graph comparison
has become an important and challenging problem in application areas
such as molecular biology, telecommunications, chemoinformatics,
and social networks. Graph kernels have recently been proposed as
a theoretically sound approach to this problem, and have been shown
to achieve high accuracies on benchmark datasets. Different graph
kernels compare different types of subgraphs in the input graphs.
So far, the choice of subgraphs to compare is rather ad hoc and is
often motivated by runtime considerations.There is no clear indication
that certain types of subgraphs are better than the others. On the
other hand, comparing all possible subgraphs has been shown to be
NP-hard, thus making it practically infeasible. These difficulties
seriously limit the practical applicability of graph kernels. In
this article, we attempt to rectify the situation, and make graph
kernels applicable for data mining on large graphs and large datasets.
Our starting point is the matrix reconstruction theorem, which states
that any matrix of size 5 or above can be reconstructed given all
its principal minors. By applying this to the adjacency matrix of
a graph, we recursively define a graph kernels and show that it can
be efficiently computed by using the distribution of all size 4 subgraphs
of a graph. This distribution, we argue, is similar to a sufficient
statistic of the graph, especially when the graph is large. Exhaustive
enumeration of these subgraphs is prohibitively expensive, scaling
as O(n4). But, by bounding the deviation of the empirical estimates
of the distribution from the true distribution, it suffices to sample
a fixed number of subgraphs. Incidentally, our bounds are stronger
than those found in the bio-informatics literature for similar techniques.In
our experimental evaluation, our graph kernel outperforms state-of-the-art
graph kernels both in times of time and classification accuracy.},
author = {Borgwardt, Karsten M and Petri, Tobias and Vishwanathan, S V N and Kriegel, Hans-Peter},
booktitle = {Proceedings of Mining and Learning with Graphs, MLG 2007, Firence, Italy, August 1-3, 2007},
editor = {Frasconi, Paolo and Kersting, Kristian and Tsuda, Koji},
month = {aug},
title = {{An efficient sampling scheme for comparison of large graphs}},
year = {2007}
}
@inproceedings{DBLP:conf/ecml/2006,
booktitle = {ECML},
editor = {F{\"{u}}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
isbn = {3-540-45375-X},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin, Germany, September 18-22, 2006, Proceedings}},
volume = {4212},
year = {2006}
}
@article{Luko,
author = {Lukoˇ, Mantas},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Lukoˇ - Unknown - A Practical Guide to Applying Echo State Networks.pdf:pdf},
title = {{A Practical Guide to Applying Echo State Networks}}
}
@inproceedings{Nicotra2004,
abstract = {We introduce a kernel for structured data, which is an extension of
the Fisher kernel used for sequences. In our approach, we extract
the Fisher score vectors from a Bayesian network, specifically a
hidden tree Markov model, which can be constructed starting from
the training data. Experiments on a {\{}QSPR{\}} (quantitative structure-property
relationship) analysis, where instances are naturally represented
as trees, allow a first test of the approach.},
author = {Nicotra, Luca and Micheli, Alessio and Starita, Antonina},
booktitle = {Proceedings of the 2004 {\{}IEEE{\}} International Joint Conference on Neural Networks},
isbn = {1098-7576},
keywords = {Bayesian{\_}network,Fisher score vectors,Fisher{\_}kernel{\_}function,belief{\_}networks,fisher,hidden Markov models,hidden tree Markov model,learning (artificial intelligence),quantitative structure-property relationship anal,support vector machine,support vector machines,training data,tree,tree fisher kernel,tree kernel,tree structured data,trees (mathematics)},
pages = {1917----1922 vol.3},
title = {{Fisher kernel for tree structured data}},
volume = {3},
year = {2004}
}
@article{Kobler2006,
author = {K{\"{o}}bler, J},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{o}}bler - 2006 - On graph isomorphism for restricted graph classes.pdf:pdf},
journal = {Logical Approaches to Computational Barriers},
title = {{On graph isomorphism for restricted graph classes}},
url = {http://link.springer.com/chapter/10.1007/11780342{\_}26},
year = {2006}
}
@article{Scarselli2009,
abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) is an element of IR(m) that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
doi = {10.1109/TNN.2008.2005605},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Scarselli et al. - 2009 - The graph neural network model.pdf:pdf},
issn = {1941-0093},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {Algorithms,Artificial Intelligence,Databases, Factual,Internet,Linear Models,Neural Networks (Computer),Nonlinear Dynamics,Pattern Recognition, Automated,Regression Analysis,Reproducibility of Results},
month = {jan},
number = {1},
pages = {61--80},
pmid = {19068426},
title = {{The graph neural network model.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19068426},
volume = {20},
year = {2009}
}
@article{Palla2005,
abstract = {Many complex systems in nature and society can be described in terms of networks capturing the intricate web of connections among the units they are made of. A key question is how to interpret the global organization of such networks as the coexistence of their structural subunits (communities) associated with more highly interconnected parts. Identifying these a priori unknown building blocks (such as functionally related proteins, industrial sectors and groups of people) is crucial to the understanding of the structural and functional properties of networks. The existing deterministic methods used for large networks find separated communities, whereas most of the actual networks are made of highly overlapping cohesive groups of nodes. Here we introduce an approach to analysing the main statistical features of the interwoven sets of overlapping communities that makes a step towards uncovering the modular structure of complex systems. After defining a set of new characteristic quantities for the statistics of communities, we apply an efficient technique for exploring overlapping communities on a large scale. We find that overlaps are significant, and the distributions we introduce reveal universal features of networks. Our studies of collaboration, word-association and protein interaction graphs show that the web of communities has non-trivial correlations and specific scaling properties.},
archivePrefix = {arXiv},
arxivId = {physics/0506133},
author = {Palla, Gergely and Derenyi, Imre and Farkas, Illes and Vicsek, Tamas},
doi = {10.1038/nature03607},
eprint = {0506133},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Palla et al. - 2005 - Uncovering the overlapping community structure of complex networks in nature and society - Supplementary.pdf:pdf},
isbn = {1476-4687 (Electronic)$\backslash$n0028-0836 (Linking)},
issn = {1476-4687},
journal = {Nature},
keywords = {Molecular Networks,Physics and Society,Statistical Mechanics},
number = {7043},
pages = {814--818},
pmid = {15944704},
primaryClass = {physics},
title = {{Uncovering the overlapping community structure of complex networks in nature and society - Supplementary}},
url = {http://arxiv.org/abs/physics/0506133},
volume = {435},
year = {2005}
}
@article{Kolter7,
address = {Cambridge, MA, USA},
author = {Kolter, J Z and Maloof, M A},
journal = {Journ. of Mach. Learn. Res.},
pages = {2755--2790},
publisher = {MIT Press},
title = {{Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts}},
volume = {8},
year = {2007}
}
@inproceedings{Maurer2008,
author = {Maurer, Andreas and Pontil, Massimiliano},
booktitle = {ALT},
editor = {Freund, Yoav and Gy{\"{o}}rfi, L{\'{a}}szl{\'{o}} and Tur{\'{a}}n, Gy{\"{o}}rgy and Zeugmann, Thomas},
isbn = {978-3-540-87986-2},
pages = {79--91},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Generalization Bounds for K-Dimensional Coding Schemes in Hilbert Spaces}},
volume = {5254},
year = {2008}
}
@article{Field1968,
author = {Field, Randolph and Solomon, Herbert and Cover, T M and Hart, P E},
journal = {Electronics},
title = {{On the Mean Accuracy of Statistical Pattern Recognizers}},
volume = {I},
year = {1968}
}
@article{Kuramochi2001,
address = {Los Alamitos, CA, USA},
author = {Kuramochi, Michihiro and Karypis, George},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICDM.2001.989534},
isbn = {0-7695-1119-8},
journal = {IEEE International Conference on Data Mining},
pages = {313},
publisher = {IEEE Computer Society},
title = {{Frequent Subgraph Discovery}},
volume = {0},
year = {2001}
}
@inproceedings{DBLP:conf/latin/2000,
booktitle = {LATIN},
editor = {Gonnet, Gaston H and Panario, Daniel and Viola, Alfredo},
isbn = {3-540-67306-7},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{LATIN 2000: Theoretical Informatics, 4th Latin American Symposium, Punta del Este, Uruguay, April 10-14, 2000, Proceedings}},
volume = {1776},
year = {2000}
}
@article{Cao2003,
author = {Cao, L J and Tay, Francis E H},
journal = {{\{}IEEE{\}} transactions on neural networks},
month = {nov},
number = {6},
title = {support vector machines with adaptive parameters in financial time series forecasting},
volume = {14},
year = {2003}
}
@article{Yoon2009,
abstract = {Hidden Markov models (HMMs) have been extensively used in biological sequence analysis. In this paper, we give a tutorial review of HMMs and their applications in a variety of problems in molecular biology. We especially focus on three types of HMMs: the profile-HMMs, pair-HMMs, and context-sensitive HMMs. We show how these HMMs can be used to solve various sequence analysis problems, such as pairwise and multiple sequence alignments, gene annotation, classification, similarity search, and many others.},
author = {Yoon, Byung-Jun},
doi = {10.2174/138920209789177575},
isbn = {1389202097},
issn = {13892029},
journal = {Current genomics},
pages = {402--415},
pmid = {20190955},
title = {{Hidden Markov Models and their Applications in Biological Sequence Analysis.}},
volume = {10},
year = {2009}
}
@inproceedings{Kashima03marginalizedkernels,
author = {Kashima, Hisashi and Tsuda, Koji and Inokuchi, Akihiro},
booktitle = {ICML},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kashima, Tsuda, Inokuchi - 2003 - Marginalized kernels between labeled graphs.pdf:pdf},
pages = {321--328},
publisher = {AAAI Press},
title = {{Marginalized kernels between labeled graphs}},
year = {2003}
}
@article{Arauzo-Azofra2007,
author = {Arauzo-Azofra, Antonio and Benitez, Jose Manuel and Castro, Juan Luis},
doi = {10.1007/s10844-007-0037-0},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Arauzo-Azofra, Benitez, Castro - 2007 - Consistency measures for feature selection.pdf:pdf},
isbn = {1084400700370},
issn = {0925-9902},
journal = {Journal of Intelligent Information Systems},
keywords = {a,arauzo-azofra,attribute evaluation,b,consistency,feature selection,measures},
month = {feb},
number = {3},
pages = {273--292},
title = {{Consistency measures for feature selection}},
url = {http://www.springerlink.com/index/10.1007/s10844-007-0037-0},
volume = {30},
year = {2007}
}
@article{Garofalakis08,
author = {Cormode, G and Garofalakis, M N},
journal = {ACM Trans. on Database Sys.},
number = {2},
pages = {1--39},
title = {{Approximate continuous querying over distributed streams}},
volume = {33},
year = {2008}
}
@article{Freund1999,
author = {Freund, Yoav and Schapire, Robert E},
journal = {Machine Learning},
number = {3},
pages = {277--296},
title = {{Large Margin Classification Using the Perceptron Algorithm}},
volume = {37},
year = {1999}
}
@article{Mendelson2003,
author = {Mendelson, S},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mendelson - 2003 - A few notes on statistical learning theory.pdf:pdf},
journal = {Advanced Lectures on Machine Learning},
pages = {1--43},
title = {{A few notes on statistical learning theory}},
url = {http://www.springerlink.com/index/958j5g0v2l93qlcw.pdf},
year = {2003}
}
@article{Philipp-Foliguet2009,
author = {Philipp-Foliguet, Sylvie and Gony, Julien and Gosselin, Philippe-Henri},
doi = {10.1016/j.cviu.2008.11.002},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Philipp-Foliguet, Gony, Gosselin - 2009 - FReBIR An image retrieval system based on fuzzy region matching.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
month = {jun},
number = {6},
pages = {693--707},
publisher = {Elsevier Inc.},
title = {{FReBIR: An image retrieval system based on fuzzy region matching}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314208001859},
volume = {113},
year = {2009}
}
@article{Bishop,
author = {Bishop, Christopher M.},
title = {{Neural Networks for Pattern Recognition}}
}
@article{Vert2009,
author = {Mah{\'{e}}, Pierre and Vert, Jean-Philippe},
doi = {10.1007/s10994-008-5086-2},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {chemoinformatics,graph kernels,support vector machines},
month = {oct},
number = {1},
pages = {3--35},
title = {{Graph kernels based on tree patterns for molecules}},
url = {http://www.springerlink.com/index/10.1007/s10994-008-5086-2},
volume = {75},
year = {2008}
}
@inproceedings{Kulis2009,
author = {Kulis, Brian and Darrell, Trevor},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C K I and Culotta, A},
pages = {1042--1050},
title = {{Learning to Hash with Binary Reconstructive Embeddings}},
year = {2009}
}
@article{Gardner2011,
abstract = {The Rfam database aims to catalogue non-coding RNAs through the use of sequence alignments and statistical profile models known as covariance models. In this contribution, we discuss the pros and cons of using the online encyclopedia, Wikipedia, as a source of community-derived annotation. We discuss the addition of groupings of related RNA families into clans and new developments to the website. Rfam is available on the Web at http://rfam.sanger.ac.uk.},
author = {Gardner, Paul P and Daub, Jennifer and Tate, John and Moore, Benjamin L and Osuch, Isabelle H and Griffiths-Jones, Sam and Finn, Robert D and Nawrocki, Eric P and Kolbe, Diana L and Eddy, Sean R and Bateman, Alex},
doi = {10.1093/nar/gkq1129},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gardner et al. - 2011 - Rfam Wikipedia, clans and the decimal release.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Databases, Nucleic Acid,Encyclopedias as Topic,Models, Statistical,Nucleic Acid Conformation,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: classification,Sequence Alignment,Sequence Analysis, RNA},
month = {jan},
number = {Database issue},
pages = {D141--5},
pmid = {21062808},
title = {{Rfam: Wikipedia, clans and the "decimal" release.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3013711{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {39},
year = {2011}
}
@article{Aiolli2005,
abstract = {Winner-take-all multiclass classifiers are built on the top of a set
of prototypes each representing one of the available classes. A pattern
is then classified with the label associated to the most 'similar'
prototype. Recent proposal of SVM extensions to multiclass can be
considered instances of the same strategy with one prototype per
class.


The multi-prototype SVM proposed in this paper extends multiclass
SVM to multiple prototypes per class. It allows to combine several
vectors in a principled way to obtain large margin decision functions.
For this problem, we give a compact constrained quadratic formulation
and we propose a greedy optimization algorithm able to find locally
optimal solutions for the non convex objective function.


This algorithm proceeds by reducing the overall problem into a series
of simpler convex problems. For the solution of these reduced problems
an efficient optimization algorithm is proposed. A number of pattern
selection strategies are then discussed to speed-up the optimization
process. In addition, given the combinatorial nature of the overall
problem, stochastic search strategies are suggested to escape from
local minima which are not globally optimal.


Finally, we report experiments on a number of datasets. The performance
obtained using few simple linear prototypes is comparable to that
obtained by state-of-the-art kernel-based methods but with a significant
reduction (of one or two orders) in response time.},
author = {Aiolli, Fabio and Sperduti, Alessandro},
journal = {Journal of Machine Learning Research},
pages = {817--850},
title = {{Multiclass Classification with Multi-Prototype Support Vector Machines}},
volume = {6},
year = {2005}
}
@inproceedings{Gilbert2002,
address = {Montreal, Quebec, Canada},
author = {Gilbert, A and Guha, S and Indyk, P and Kotidis, Y and Muthukrishnan, S and Strauss, M},
booktitle = {Proc. of the 2002 Annual ACM Symposium on Theory of Computing (STOC'02)},
pages = {389--398},
title = {{Fast, Small-Space Algorithms for Approximate Histogram Maintenance}},
year = {2002}
}
@article{Vanunu2010,
abstract = {A fundamental challenge in human health is the identification of disease-causing genes. Recently, several studies have tackled this challenge via a network-based approach, motivated by the observation that genes causing the same or similar diseases tend to lie close to one another in a network of protein-protein or functional interactions. However, most of these approaches use only local network information in the inference process and are restricted to inferring single gene associations. Here, we provide a global, network-based method for prioritizing disease genes and inferring protein complex associations, which we call PRINCE. The method is based on formulating constraints on the prioritization function that relate to its smoothness over the network and usage of prior information. We exploit this function to predict not only genes but also protein complex associations with a disease of interest. We test our method on gene-disease association data, evaluating both the prioritization achieved and the protein complexes inferred. We show that our method outperforms extant approaches in both tasks. Using data on 1,369 diseases from the OMIM knowledgebase, our method is able (in a cross validation setting) to rank the true causal gene first for 34{\%} of the diseases, and infer 139 disease-related complexes that are highly coherent in terms of the function, expression and conservation of their member proteins. Importantly, we apply our method to study three multi-factorial diseases for which some causal genes have been found already: prostate cancer, alzheimer and type 2 diabetes mellitus. PRINCE's predictions for these diseases highly match the known literature, suggesting several novel causal genes and protein complexes for further investigation.},
author = {Vanunu, Oron and Magger, Oded and Ruppin, Eytan and Shlomi, Tomer and Sharan, Roded},
doi = {10.1371/journal.pcbi.1000641},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vanunu et al. - 2010 - Associating genes and protein complexes with disease via network propagation.pdf:pdf},
issn = {1553-7358},
journal = {PLoS computational biology},
keywords = {Algorithms,Alzheimer Disease,Alzheimer Disease: genetics,Alzheimer Disease: metabolism,Databases, Genetic,Diabetes Mellitus,Diabetes Mellitus: genetics,Diabetes Mellitus: metabolism,Disease,Disease: genetics,Genes,Humans,Male,Multiprotein Complexes,Prostatic Neoplasms,Prostatic Neoplasms: genetics,Prostatic Neoplasms: metabolism,Protein Interaction Mapping,Protein Interaction Mapping: methods,Proteins,Proteins: genetics,Proteins: metabolism,Reproducibility of Results},
month = {jan},
number = {1},
pages = {e1000641},
pmid = {20090828},
title = {{Associating genes and protein complexes with disease via network propagation.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2797085{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {6},
year = {2010}
}
@incollection{Kashima2004,
author = {Kashima, Hisashi and Tsuda, Koji and Inokuchi, Akihiro},
chapter = {Kernels fo},
editor = {{Bernhard Sch{\"{o}}lkopf Koji Tsuda}, Jean-Philippe Vert},
pages = {155--170},
publisher = {MIT Press},
title = {{Kernel Methods in Computational Biology}},
year = {2004}
}
@article{Ben-david2001,
author = {Ben-david, Shai},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ben-david - 2001 - A Priori Generalization Bounds for Kernel Based Learning – Can Sparsity Save the Day.pdf:pdf},
pages = {0--3},
title = {{A Priori Generalization Bounds for Kernel Based Learning – Can Sparsity Save the Day?}},
year = {2001}
}
@inproceedings{FabioAiolliMicheleDonini2015,
address = {Cape Town},
author = {Aiolli, Fabio and Donini, Michele and Navarin, Nicol{\`{o}} and Sperduti, Alessandro},
booktitle = {IEEE Symposium on Computational Intelligence and Data Mining},
pages = {1607 -- 1614},
publisher = {IEEE},
title = {{Multiple Graph-Kernel Learning}},
year = {2015}
}
@article{Vert2009,
author = {Vert, Pierre Mah{\'{e}} Jean-philippe},
doi = {10.1007/s10994-008-5086-2},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vert - 2009 - Graph kernels based on tree patterns for molecules.pdf:pdf},
journal = {Machine Learning},
keywords = {chemoinformatics,graph kernels,support vector machines},
number = {September 2008},
pages = {3--35},
title = {{Graph kernels based on tree patterns for molecules}},
year = {2009}
}
@inproceedings{Thomas2002,
author = {G{\"{a}}rtner, Thomas and Flach, Peter A. and Kowalczyk, Adam and Smola, Alex J.},
booktitle = {In Proc. 19th International Conf. on Machine Learning},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/G{\"{a}}rtner et al. - 2002 - Multi-Instance Kernels.pdf:pdf},
number = {Mi},
pages = {179----186},
title = {{Multi-Instance Kernels}},
year = {2002}
}
@article{Clark2014,
archivePrefix = {arXiv},
arxivId = {1412.3409},
author = {Clark, Christopher and Storkey, Amos},
eprint = {1412.3409},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Clark, Storkey - 2014 - Teaching Deep Convolutional Neural Networks to Play Go.pdf:pdf},
month = {dec},
title = {{Teaching Deep Convolutional Neural Networks to Play Go}},
url = {http://arxiv.org/abs/1412.3409v1},
year = {2014}
}
@inproceedings{LPG08,
author = {Lebrun, J and Philipp-Foliguet, S and Gosselin, P.-H.},
booktitle = {19th ICPR International Conference on Pattern Recognition},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Lebrun, Philipp-Foliguet, Gosselin - 2008 - Image retrieval with graph kernel on regions.pdf:pdf},
organization = {IEEE, USF},
publisher = {IEEE},
title = {{Image retrieval with graph kernel on regions}},
url = {http://publi-etis.ensea.fr/2008/LPG08},
year = {2008}
}
@article{Li2008,
author = {Li, Xuehua and Shu, Lan},
doi = {10.3390/s8074186},
issn = {1424-8220},
journal = {Sensors},
number = {7},
pages = {4186--4200},
title = {{Kernel Based Nonlinear Dimensionality Reduction and Classification for Genomic Microarray}},
volume = {8},
year = {2008}
}
@article{Camillo2009,
abstract = {In the context of reverse engineering of biological networks, simulators
are helpful to test and compare the accuracy of different reverse-engineering
approaches in a variety of experimental conditions. A novel gene-network
simulator is presented that resembles some of the main features of
transcriptional regulatory networks related to topology, interaction
among regulators of transcription, and expression dynamics. The simulator
generates network topology according to the current knowledge of
biological network organization, including scale-free distribution
of the connectivity and clustering coefficient independent of the
number of nodes in the network. It uses fuzzy logic to represent
interactions among the regulators of each gene, integrated with differential
equations to generate continuous data, comparable to real data for
variety and dynamic complexity. Finally, the simulator accounts for
saturation in the response to regulation and transcription activation
thresholds and shows robustness to perturbations. It therefore provides
a reliable and versatile test bed for reverse engineering algorithms
applied to microarray data. Since the simulator describes regulatory
interactions and expression dynamics as two distinct, although interconnected
aspects of regulation, it can also be used to test reverse engineering
approaches that use both microarray and protein-protein interaction
data in the process of learning. A first software release is available
at http://www.dei.unipd.it/{\{}$\backslash$textasciitilde{\}}dicamill/software/netsim
as an R programming language package.},
annote = {{\{}PMID:{\}} 19348638},
author = {{Di Camillo}, Barbara and Toffolo, Gianna and Cobelli, Claudio},
doi = {10.1111/j.1749-6632.2008.03756.x},
issn = {1749-6632},
journal = {Annals of the New York Academy of Sciences},
month = {mar},
pages = {125--142},
title = {{A gene network simulator to assess reverse engineering algorithms}},
volume = {1158},
year = {2009}
}
@article{Gallicchio2013,
abstract = {In this paper we present the Tree Echo State Network (TreeESN) model, generalizing the paradigm of Reservoir Computing to tree structured data. TreeESNs exploit an untrained generalized recursive reservoir, exhibiting extreme efficiency for learning in structured domains. In addition, we highlight through the paper other characteristics of the approach: First, we discuss the Markovian characterization of reservoir dynamics, extended to the case of tree domains, that is implied by the contractive setting of the TreeESN state transition function. Second, we study two types of state mapping functions to map the tree structured state of TreeESN into a fixed-size feature representation for classification or regression tasks. The critical role of the relation between the choice of the state mapping function and the Markovian characterization of the task is analyzed and experimentally investigated on both artificial and real-world tasks. Finally, experimental results on benchmark and real-world tasks show that the TreeESN approach, in spite of its efficiency, can achieve comparable results with state-of-the-art, although more complex, neural and kernel based models for tree structured data. ?? 2012 Elsevier B.V.},
author = {Gallicchio, Claudio and Micheli, Alessio},
doi = {10.1016/j.neucom.2012.08.017},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gallicchio, Micheli - 2013 - Tree Echo State Networks.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Echo state networks,Learning in tree structured domains,Reservoir computing},
pages = {319--337},
publisher = {Elsevier},
title = {{Tree Echo State Networks}},
url = {http://dx.doi.org/10.1016/j.neucom.2012.08.017},
volume = {101},
year = {2013}
}
@inproceedings{Kandola2002,
author = {Kandola, Jaz S and Shawe-Taylor, John and Cristianini, Nello},
booktitle = {NIPS},
editor = {Becker, Suzanna and Thrun, Sebastian and Obermayer, Klaus},
isbn = {0-262-02550-7},
pages = {657--664},
publisher = {MIT Press},
title = {{Learning Semantic Similarity}},
year = {2002}
}
@book{Breiman84,
author = {Breiman, L and Friedman, J and Olshen, R and Stone, C},
publisher = {Wadsworth Internl. Group, Belmont, CA},
title = {{Classification and Regression Trees}},
year = {1984}
}
@article{VanDeursen2007,
author = {van Deursen, Ruud and Reymond, Jean-Louis},
doi = {10.1002/cmdc.200700021},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/van Deursen, Reymond - 2007 - Chemical space travel.pdf:pdf},
issn = {1860-7187},
journal = {ChemMedChem},
keywords = {Drug Design,Molecular Structure,Mutation},
month = {may},
number = {5},
pages = {636--40},
pmid = {17366512},
title = {{Chemical space travel.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17366512},
volume = {2},
year = {2007}
}
@article{DBLP:journals/tnn/AiolliMHS09,
author = {Aiolli, Fabio and Martino, Giovanni Da San and Hagenbuchner, Markus and Sperduti, Alessandro},
doi = {10.1109/TNN.2009.2033473},
journal = {{\{}IEEE{\}} Transactions on Neural Networks},
number = {12},
pages = {1938--1949},
title = {{Learning Nonsparse Kernels by Self-Organizing Maps for Structured Data}},
url = {http://dx.doi.org/10.1109/TNN.2009.2033473},
volume = {20},
year = {2009}
}
@article{Lehtinen2015,
abstract = {With the growing availability of large-scale biological datasets, automated methods of extracting functionally meaningful information from this data are becoming increasingly important. Data relating to functional association between genes or proteins, such as co-expression or functional association, is often represented in terms of gene or protein networks. Several methods of predicting gene function from these networks have been proposed. However, evaluating the relative performance of these algorithms may not be trivial: concerns have been raised over biases in different benchmarking methods and datasets, particularly relating to non-independence of functional association data and test data. In this paper we propose a new network-based gene function prediction algorithm using a commute-time kernel and partial least squares regression (Compass). We compare Compass to GeneMANIA, a leading network-based prediction algorithm, using a number of different benchmarks, and find that Compass outperforms GeneMANIA on these benchmarks. We also explicitly explore problems associated with the non-independence of functional association data and test data. We find that a benchmark based on the Gene Ontology database, which, directly or indirectly, incorporates information from other databases, may considerably overestimate the performance of algorithms exploiting functional association data for prediction.},
author = {Lehtinen, Sonja and Lees, Jon and B{\"{a}}hler, J{\"{u}}rg and Shawe-Taylor, John and Orengo, Christine},
doi = {10.1371/journal.pone.0134668},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Lehtinen et al. - 2015 - Gene Function Prediction from Functional Association Networks Using Kernel Partial Least Squares Regression.pdf:pdf},
issn = {1932-6203},
journal = {PloS one},
month = {jan},
number = {8},
pages = {e0134668},
pmid = {26288239},
title = {{Gene Function Prediction from Functional Association Networks Using Kernel Partial Least Squares Regression.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/26288239},
volume = {10},
year = {2015}
}
@inproceedings{Kriege2012,
author = {Kriege, Nils and Mutzel, Petra},
booktitle = {ICML},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kriege, Mutzel - 2012 - Subgraph matching kernels for attributed graphs.pdf:pdf},
pages = {1015--1022},
title = {{Subgraph matching kernels for attributed graphs}},
url = {http://arxiv.org/abs/1206.6483},
year = {2012}
}
@book{Cristianini2007,
abstract = {Where did SARS come from? Have we inherited genes from Neanderthals?
How do plants use their internal clock? The genomic revolution in
biology enables us to answer such questions. But the revolution would
have been impossible without the support of powerful computational
and statistical methods that enable us to exploit genomic data. Many
universities are introducing courses to train the next generation
of bioinformaticians: biologists fluent in mathematics and computer
science, and data analysts familiar with biology. This readable and
entertaining book, based on successful taught courses, provides a
roadmap to navigate entry to this field. It guides the reader through
key achievements of bioinformatics, using a hands-on approach. Statistical
sequence analysis, sequence alignment, hidden Markov models, gene
and motif finding and more, are introduced in a rigorous yet accessible
way. A companion website provides the reader with Matlab-related
software tools for reproducing the steps demonstrated in the book.},
author = {Cristianini, Nello and Hahn, Matthew William},
isbn = {978-0521671910},
month = {jan},
pages = {200},
publisher = {Cambridge University Press},
title = {{Introduction to Computational Genomics A Case Studies Approach}},
year = {2007}
}
@article{Hosseini,
author = {Hosseini, Babak and Hammer, Barbara},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Hosseini, Hammer - Unknown - Efficient Metric Learning for the Analysis of Motion Data.pdf:pdf},
isbn = {9781467382731},
title = {{Efficient Metric Learning for the Analysis of Motion Data}}
}
@article{Aggarwal1999,
author = {Aggarwal, CC and Wolf, JL and Wu, KL and Yu, PS},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal et al. - 1999 - Horting hatches an egg A new graph-theoretic approach to collaborative filtering.pdf:pdf},
journal = {Proceedings of the fifth ACM {\ldots}},
title = {{Horting hatches an egg: A new graph-theoretic approach to collaborative filtering}},
url = {http://dl.acm.org/citation.cfm?id=312230},
year = {1999}
}
@article{DBLP:journals/jmlr/OrabonaKC09,
author = {Orabona, Francesco and Keshet, Joseph and Caputo, Barbara},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Orabona, Keshet, Caputo - 2009 - Bounded Kernel-Based Online Learning ∗.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2643­2666},
title = {{Bounded Kernel­Based Online Learning}},
volume = {10},
year = {2009}
}
@inproceedings{scholz2005,
address = {Porto, Portugal},
author = {Scholz, M and Klinkenberg, R},
booktitle = {Proceeding of 2nd Internl. Workshop on Knowl. Disc. from Data Streams, in conjunction with ECML-PKDD 2005},
pages = {53--64},
title = {{An Ensemble Classifier for Drifting Concepts}},
year = {2005}
}
@inproceedings{wang2003,
address = {Washington, DC},
author = {Wang, H and Fan, W and Yu, P S and Han, J},
booktitle = {Proc. of the 9th Internl. Conf. on Knowl. Disc. and Data Mining (KDD'03)},
pages = {226--235},
title = {{Mining concept-drifting data streams using ensemble classifiers}},
year = {2003}
}
@article{Brun2011,
author = {Brun, Luc and Villemin, Didier},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Brun, Villemin - 2011 - Two New Graph Kernels and Applications to Chemoinformatics.pdf:pdf},
keywords = {chemoinformatics,edit-distance,graph kernel},
pages = {112--121},
title = {{Two New Graph Kernels and Applications to Chemoinformatics}},
year = {2011}
}
@article{vishwanathan_graph_2008,
abstract = {We present a unified framework to study graph kernels, special cases of which include the random walk graph kernel {\{}{\{}$\backslash$textbackslash{\}}citep{\{}GaeFlaWro03,BorOngSchVisetal05{\}},{\}} marginalized graph kernel {\{}{\{}$\backslash$textbackslash{\}}citep{\{}KasTsuIno03,KasTsuIno04,MahUedAkuPeretal04{\}},{\}} and geometric kernel on graphs {\{}{\{}$\backslash$textbackslash{\}}citep{\{}Gaertner02{\}}.{\}} Through extensions of linear algebra to Reproducing Kernel Hilbert Spaces {\{}(RKHS){\}} and reduction to a Sylvester equation, we construct an algorithm that improves the time complexity of kernel computation from {\{}{\$}O(n{\{}\backslashtextasciicircum{\}}6){\$}{\}} to {\{}{\$}O(n{\{}\backslashtextasciicircum{\}}3){\$}.{\}} When the graphs are sparse, conjugate gradient solvers or fixed-point iterations bring our algorithm into the sub-cubic domain. Experiments on graphs from bioinformatics and other application domains show that it is often more than a thousand times faster than previous approaches. We then explore connections between diffusion kernels {\{}{\{}$\backslash$textbackslash{\}}citep{\{}KonLaf02{\}},{\}} regularization on graphs {\{}{\{}$\backslash$textbackslash{\}}citep{\{}SmoKon03{\}},{\}} and graph kernels, and use these connections to propose new graph kernels. Finally, we show that rational kernels {\{}{\{}$\backslash$textbackslash{\}}citep{\{}CorHafMoh02,CorHafMoh03,CorHafMoh04{\}}{\}} when specialized to graphs reduce to the random walk graph kernel.},
author = {Vishwanathan, S V N and Schraudolph, Nicol N and Kondor, Imre Risi and Borgwardt, Karsten M},
journal = {Journal of Machine Learning Research},
keywords = {Computer{\_}Science{\_}-{\_}Learning,biomarcatori,graph,graph kernels,graph{\_}kernel,kernel for graph},
month = {jul},
pages = {1201−1242},
title = {{Graph Kernels}},
volume = {11},
year = {2010}
}
@article{Kivinen2004,
abstract = {Kernel-based algorithms such as support vector machines have achieved
considerable success in various problems in batch setting, where
all of the training data is available in advance. Support vector
machines combine the so-called kernel trick with the large margin
idea. There has been little use of these methods in an online setting
suitable for real-time applications. In this paper, we consider online
learning in a reproducing kernel Hilbert space. By considering classical
stochastic gradient descent within a feature space and the use of
some straightforward tricks, we develop simple and computationally
efficient algorithms for a wide range of problems such as classification,
regression, and novelty detection. In addition to allowing the exploitation
of the kernel trick in an online setting, we examine the value of
large margins for classification in the online setting with a drifting
target. We derive worst-case loss bounds, and moreover, we show the
convergence of the hypothesis to the minimizer of the regularized
risk functional. We present some experimental results that support
the theory as well as illustrating the power of the new algorithms
for online novelty detection.},
author = {Kivinen, Jyrki and Smola, Alexander J and Williamson, Robert C},
issn = {1053-587X},
journal = {{\{}IEEE{\}} Transactions on Signal Processing},
number = {8},
pages = {2165--2176},
title = {{Online learning with kernels}},
volume = {52},
year = {2004}
}
@article{Heyne2012,
abstract = {Clustering according to sequence-structure similarity has now become a generally accepted scheme for ncRNA annotation. Its application to complete genomic sequences as well as whole transcriptomes is therefore desirable but hindered by extremely high computational costs.},
author = {Heyne, Steffen and Costa, Fabrizio and Rose, Dominic and Backofen, Rolf},
doi = {10.1093/bioinformatics/bts224},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Heyne et al. - 2012 - GraphClust alignment-free structural clustering of local RNA secondary structures.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
month = {jun},
number = {12},
pages = {i224--i232},
pmid = {22689765},
title = {{GraphClust: alignment-free structural clustering of local RNA secondary structures.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3371856{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {28},
year = {2012}
}
@misc{borgwardt_graph_2008,
abstract = {Social and biological networks have led to a huge interest in data
analysis on graphs. Various groups within the {\{}KDD{\}} community have
begun to study the task of data mining on graphs, including researchers
from database-oriented graph mining, and researchers from kernel
machine learning. Their approaches are often complementary, and we
feel that exciting research problems and techniques can be discovered
by exploring the link between these different approaches to graph
mining.

This tutorial presents a comprehensive overview of the techniques
developed in graph mining and graph kernels and examines the connection
between them. The goal of this tutorial is i) to introduce newcomers
to the field of graph mining, ii) to introduce people with database
background to graph mining using kernel machines, iii) to introduce
people with machine learning background to database-oriented graph
mining, and iv) to present exciting research problems at the interface
of both fields.},
author = {Borgwardt, Karsten M and Yan, Xifen},
title = {{Graph Mining and Graph Kernels Tutorial}},
year = {2008}
}
@article{LiuPP08,
author = {Liu, Weifeng and Pokharel, Puskal P and Principe, Jose C},
journal = {IEEE Transactions on Signal Processing},
number = {2},
pages = {543--554},
title = {{The Kernel Least-Mean-Square Algorithm}},
volume = {56},
year = {2008}
}
@article{Aiolli2011,
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro},
editor = {Honkela, Timo and Duch, Wlodzislaw and Girolami, Mark A and Kaski, Samuel},
isbn = {978-3-642-21734-0},
journal = {ICANN},
keywords = {kernel methods,machine learning,tree kernels},
pages = {142--149},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Extending Tree Kernels with Topological Information}},
volume = {6791},
year = {2011}
}
@inproceedings{DBLP:conf/nips/2003,
booktitle = {NIPS},
editor = {Thrun, Sebastian and Saul, Lawrence K and Sch{\"{o}}lkopf, Bernhard},
isbn = {0-262-20152-6},
publisher = {MIT Press},
title = {{Advances in Neural Information Processing Systems 16 [Neural Information Processing Systems, NIPS 2003, December 8-13, 2003, Vancouver and Whistler, British Columbia, Canada]}},
year = {2004}
}
@article{Tsuda2003,
author = {Tsuda, Koji and Akaho, Shotaro and Asai, Kiyoshi},
doi = {http://dx.doi.org/10.1162/153244304322765649},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
pages = {67--81},
publisher = {JMLR.org},
title = {{The em algorithm for kernel matrix completion with auxiliary data}},
volume = {4},
year = {2003}
}
@inproceedings{DBLP:conf/pkdd/2006,
booktitle = {PKDD},
editor = {F{\"{u}}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
isbn = {3-540-45374-1},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Knowledge Discovery in Databases: PKDD 2006, 10th European Conference on Principles and Practice of Knowledge Discovery in Databases, Berlin, Germany, September 18-22, 2006, Proceedings}},
volume = {4213},
year = {2006}
}
@inproceedings{Sugiyama2015,
author = {Sugiyama, Mahito and Eth, Z},
booktitle = {Neural Information Processing Systems (NIPS)},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sugiyama, Eth - 2015 - Halting in Random Walk Kernels.pdf:pdf},
title = {{Halting in Random Walk Kernels}},
year = {2015}
}
@article{Shin2009,
author = {Shin, Kilho and Kuboyama, Tetsuji},
doi = {http://dx.doi.org/10.1016/j.tcs.2009.01.014},
journal = {Theor. Comput. Sci.},
number = {19},
pages = {1847--1862},
title = {{Polynomial summaries of positive semidefinite kernels}},
volume = {410},
year = {2009}
}
@article{Nawrocki2009,
abstract = {SUMMARY: INFERNAL builds consensus RNA secondary structure profiles called covariance models (CMs), and uses them to search nucleic acid sequence databases for homologous RNAs, or to create new sequence- and structure-based multiple sequence alignments.

AVAILABILITY: Source code, documentation and benchmark downloadable from http://infernal.janelia.org. INFERNAL is freely licensed under the GNU GPLv3 and should be portable to any POSIX-compliant operating system, including Linux and Mac OS/X.},
author = {Nawrocki, Eric P and Kolbe, Diana L and Eddy, Sean R},
doi = {10.1093/bioinformatics/btp157},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Nawrocki, Kolbe, Eddy - 2009 - Infernal 1.0 inference of RNA alignments.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Databases, Nucleic Acid,Nucleic Acid Conformation,RNA,RNA: chemistry,Sequence Alignment,Sequence Alignment: methods,Sequence Analysis, RNA,Software},
month = {may},
number = {10},
pages = {1335--7},
pmid = {19307242},
title = {{Infernal 1.0: inference of RNA alignments.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2732312{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {25},
year = {2009}
}
@inproceedings{Kondor2002,
annote = {The application of kernel-based learning algorithms has, so far,
largely been confined to realvalued data and a few special data types,
such as strings. In this paper we propose a general method of constructing
natural families of kernels over discrete structures, based on the
matrix exponentiation idea. In particular, we focus on generating
kernels on graphs, for which we propose a special class of exponential
kernels, based on the heat equation, called diffusion kernels, and
show that these can be regarded as the discretization of the familiar
Gaussian kernel of Euclidean space.},
author = {Kondor, Risi Imre and Lafferty, John},
booktitle = {Proceedings of the 19th international conference on Machine learning},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kondor, Lafferty - 2002 - Diffusion kernels on graphs and other discrete structures.pdf:pdf},
howpublished = {http://eprints.kfupm.edu.sa/34988/},
title = {{Diffusion kernels on graphs and other discrete structures}},
type = {Article},
year = {2002}
}
@inproceedings{Collins2001,
abstract = {We describe the application of kernel methods to Natural Language
Pro- cessing (NLP) problems. In many NLP tasks the objects being
modeled are strings, trees, graphs or other discrete structures which
require some mechanism to convert them into feature vectors. We describe
kernels for various natural language structures, allowing rich, high
dimensional rep- resentations of these structures. We show how a
kernel over trees can be applied to parsing using the voted perceptron
algorithm, and we give experimental results on the ATIS corpus of
parse trees.},
author = {Collins, Michael and Duffy, Nigel},
booktitle = {NIPS},
editor = {Dietterich, Thomas G and Becker, Suzanna and Ghahramani, Zoubin},
pages = {625--632},
publisher = {MIT Press},
title = {{Convolution Kernels for Natural Language}},
year = {2001}
}
@article{DaSanMartino2016,
archivePrefix = {arXiv},
arxivId = {1507.03372},
author = {{Da San Martino}, Giovanni and Navarin, Nicol{\`{o}} and Sperduti, Alessandro},
eprint = {1507.03372},
journal = {Neurocomputing},
pages = {92--103},
title = {{Ordered Decompositional DAG Kernels Enhancements}},
url = {http://arxiv.org/pdf/1507.03372.pdf},
volume = {192},
year = {2016}
}
@article{Cesa-Bianchi2004,
author = {Cesa-Bianchi, N. and Conconi, a. and Gentile, C.},
doi = {10.1109/TIT.2004.833339},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cesa-Bianchi, Conconi, Gentile - 2004 - On the Generalization Ability of On-Line Learning Algorithms.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {sep},
number = {9},
pages = {2050--2057},
title = {{On the Generalization Ability of On-Line Learning Algorithms}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1327806},
volume = {50},
year = {2004}
}
@inproceedings{Grossi2007,
address = {Roskilde, Denmark},
author = {Bellandi, Andrea and Furletti, Barbara and Grossi, Valerio and Romei, Andrea},
booktitle = {Proc. of the Internl. Workshop on Contexts and Ontologies: Representation and Reasoning (C{\{}{\&}{\}}O:RR) Collocated with the 6th Internl. and Interdisciplinary Conf. on Modelling and Using Context (CONTEXT 2007)},
title = {{Ontology-driven Association Rules Extraction: a Case of Study}},
year = {2007}
}
@article{Schomburg2004,
abstract = {BRENDA (BRaunschweig ENzyme DAtabase) represents a comprehensive collection of enzyme and metabolic information, based on primary literature. The database contains data from at least 83,000 different enzymes from 9800 different organisms, classified in approximately 4200 EC numbers. BRENDA includes biochemical and molecular information on classification and nomenclature, reaction and specificity, functional parameters, occurrence, enzyme structure, application, engineering, stability, disease, isolation and preparation, links and literature references. The data are extracted and evaluated from approximately 46,000 references, which are linked to PubMed as long as the reference is cited in PubMed. In the past year BRENDA has undergone major changes including a large increase in updating speed with {\textgreater}50{\%} of all data updated in 2002 or in the first half of 2003, the development of a new EC-tree browser, a taxonomy-tree browser, a chemical substructure search engine for ligand structure, the development of controlled vocabulary, an ontology for some information fields and a thesaurus for ligand names. The database is accessible free of charge to the academic community at http://www.brenda. uni-koeln.de.},
author = {Schomburg, Ida and Chang, Antje and Ebeling, Christian and Gremse, Marion and Heldt, Christian and Huhn, Gregor and Schomburg, Dietmar},
doi = {10.1093/nar/gkh081},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Schomburg et al. - 2004 - BRENDA, the enzyme database updates and major new developments.pdf:pdf},
isbn = {1362-4962 (Linking)},
issn = {1362-4962},
journal = {Nucleic acids research},
pages = {D431--D433},
pmid = {14681450},
title = {{BRENDA, the enzyme database: updates and major new developments.}},
volume = {32},
year = {2004}
}
@inproceedings{Oza01,
address = {Key West, FL},
author = {Oza, N C and Russell, S},
booktitle = {Proc. of 8th Internl. Workshop on Artificial Intelligence and Statistics (AISTATS'01)},
pages = {105--112},
title = {{Online bagging and boosting}},
year = {2001}
}
@article{Zhang2016,
author = {Zhang, Zhenyue and Mao, Jiayun},
doi = {10.1016/j.neucom.2016.07.014},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Mao - 2016 - Jointly sparse neighborhood graph for multi-view manifold clustering.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Jointly sparse method,Manifold clustering,Multi-view learning,Neighborhood selection,jointly sparse method,manifold clustering,multi-view learning,neighborhood selection},
publisher = {Elsevier},
title = {{Jointly sparse neighborhood graph for multi-view manifold clustering}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231216307512},
year = {2016}
}
@inproceedings{DBLP:conf/pkdd/BringmannZRN06,
author = {Bringmann, Bj{\"{o}}rn and Zimmermann, Albrecht and Raedt, Luc De and Nijssen, Siegfried},
booktitle = {PKDD},
editor = {F{\"{u}}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
isbn = {3-540-45374-1},
pages = {55--66},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Don't Be Afraid of Simpler Patterns}},
volume = {4213},
year = {2006}
}
@misc{Zeeman1981,
author = {Zeeman, E},
booktitle = {Journal of Theoretical Biology 89},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zeeman - 1981 - Dynamics of the evolution of animal conflicts.html:html},
keywords = {Zeeman},
pages = {249--70},
title = {{Dynamics of the evolution of animal conflicts}},
url = {http://www.sciencedirect.com/science/article/pii/0022519381903118},
urldate = {2011-07-04},
year = {1981}
}
@article{dobson2003,
abstract = {The ability to predict protein function from structure is becoming increasingly important as the number of structures resolved is growing more rapidly than our capacity to study function. Current methods for predicting protein function are mostly reliant on identifying a similar protein of known function. For proteins that are highly dissimilar or are only similar to proteins also lacking functional annotations, these methods fail. Here, we show that protein function can be predicted as enzymatic or not without resorting to alignments. We describe 1178 high-resolution proteins in a structurally non-redundant subset of the Protein Data Bank using simple features such as secondary-structure content, amino acid propensities, surface properties and ligands. The subset is split into two functional groupings, enzymes and non-enzymes. We use the support vector machine-learning algorithm to develop models that are capable of assigning the protein class. Validation of the method shows that the function can be predicted to an accuracy of 77{\%} using 52 features to describe each protein. An adaptive search of possible subsets of features produces a simplified model based on 36 features that predicts at an accuracy of 80{\%}. We compare the method to sequence-based methods that also avoid calculating alignments and predict a recently released set of unrelated proteins. The most useful features for distinguishing enzymes from non-enzymes are secondary-structure content, amino acid frequencies, number of disulphide bonds and size of the largest cleft. This method is applicable to any structure as it does not require the identification of sequence or structural similarity to a protein of known function.},
author = {Dobson, Paul D and Doig, Andrew J},
doi = {10.1016/S0022-2836(03)00628-4},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Dobson, Doig - 2003 - Distinguishing Enzyme Structures from Non-enzymes Without Alignments.pdf:pdf},
isbn = {0022-2836 (Print)},
issn = {0022-2836},
journal = {Journal of Molecular Biology},
keywords = {classification graph},
number = {4},
pages = {771--783},
pmid = {12850146},
title = {{Distinguishing Enzyme Structures from Non-enzymes Without Alignments}},
volume = {330},
year = {2003}
}
@inproceedings{Crammer2003,
author = {Crammer, Koby and Kandola, Jaz S and Singer, Yoram},
booktitle = {NIPS},
editor = {Thrun, Sebastian and Saul, Lawrence K and Sch{\"{o}}lkopf, Bernhard},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Crammer, Kandola, Singer - 2003 - Online Classification on a Budget.pdf:pdf},
isbn = {0-262-20152-6},
publisher = {MIT Press},
title = {{Online Classification on a Budget}},
year = {2003}
}
@article{Horvath2004,
address = {New York, New York, USA},
author = {Horv{\'{a}}th, Tam{\'{a}}s and G{\"{a}}rtner, Thomas and Wrobel, Stefan},
doi = {10.1145/1014052.1014072},
isbn = {1581138889},
journal = {Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '04},
pages = {158},
publisher = {ACM Press},
title = {{Cyclic pattern kernels for predictive graph mining}},
url = {http://portal.acm.org/citation.cfm?doid=1014052.1014072},
year = {2004}
}
@article{Datar2002,
author = {Datar, M and Gionis, A and Indyk, P and Motwani, R},
journal = {SIAM Journ. on Comput.},
number = {6},
pages = {1794--1813},
title = {{Maintaining stream statistics over sliding windows}},
volume = {31},
year = {2002}
}
@article{Fouss2012a,
abstract = {This paper presents a survey as well as an empirical comparison and evaluation of seven kernels on graphs and two related similarity matrices, that we globally refer to as "kernels on graphs" for simplicity. They are the exponential diffusion kernel, the Laplacian exponential diffusion kernel, the von Neumann diffusion kernel, the regularized Laplacian kernel, the commute-time (or resistance-distance) kernel, the random-walk-with-restart similarity matrix, and finally, a kernel first introduced in this paper (the regularized commute-time kernel) and two kernels defined in some of our previous work and further investigated in this paper (the Markov diffusion kernel and the relative-entropy diffusion matrix). The kernel-on-graphs approach is simple and intuitive. It is illustrated by applying the nine kernels to a collaborative-recommendation task, viewed as a link prediction problem, and to a semisupervised classification task, both on several databases. The methods compute proximity measures between nodes that help study the structure of the graph. Our comparisons suggest that the regularized commute-time and the Markov diffusion kernels perform best on the investigated tasks, closely followed by the regularized Laplacian kernel. {\textcopyright} 2012 Elsevier Ltd.},
author = {Fouss, Fran{\c{c}}ois and Francoisse, Kevin and Yen, Luh and Pirotte, Alain and Saerens, Marco},
doi = {10.1016/j.neunet.2012.03.001},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fouss et al. - 2012 - An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classifi(2).pdf:pdf},
isbn = {0-7695-2701-7},
issn = {08936080},
journal = {Neural Networks},
keywords = {Collaborative recommendation,Graph mining,Kernels on graphs,Semisupervised classification},
pages = {53--72},
pmid = {22497802},
title = {{An experimental investigation of kernels on graphs for collaborative recommendation and semisupervised classification}},
volume = {31},
year = {2012}
}
@article{Basin2004,
author = {Basin, David and Sebastian, M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Basin, Sebastian - 2004 - OFMC A symbolic model checker for security protocols.pdf:pdf},
journal = {International Journal},
keywords = {checking,constraints,formal methods,model,security protocols,verification},
title = {{OFMC : A symbolic model checker for security protocols}},
year = {2004}
}
@inproceedings{Graf2004,
author = {Graf, Hans Peter and Cosatto, Eric and Bottou, L{\'{e}}on and Durdanovic, Igor and Vapnik, Vladimir},
booktitle = {NIPS},
title = {{Parallel Support Vector Machines: The Cascade SVM}},
year = {2004}
}
@article{Deshpande2002,
author = {Deshpande, Mukund and Kuramochi, Michihiro and Karypis, George},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Deshpande, Kuramochi, Karypis - 2002 - Automated approaches for classifying structures ∗.pdf:pdf},
keywords = {chemical structures,classification,graphs,svm},
pages = {1--15},
title = {{Automated approaches for classifying structures ∗}},
volume = {0133464},
year = {2002}
}
@article{Broder1997,
author = {Broder, AZ and Glassman, SC and Manasse, MS and Zweig, G},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Broder et al. - 1997 - Syntactic clustering of the web.pdf:pdf},
journal = {Computer Networks and {\ldots}},
pages = {1--13},
title = {{Syntactic clustering of the web}},
url = {http://www.sciencedirect.com/science/article/pii/S0169755297000317},
year = {1997}
}
@article{GIBERT2013,
abstract = {Graph-based representations of patterns are very flexible and powerful, but they are not easily processed due to the lack of learning algorithms in the domain of graphs. Embedding a graph into a vector space solves this problem since graphs are turned into feature vectors and thus all the statistical learning machinery becomes available for graph input patterns. In this work we present a new way of embedding discrete attributed graphs into vector spaces using node and edge label frequencies. The methodology is experimentally tested on graph classification problems, using patterns of different nature, and it is shown to be competitive to state-of-the-art classification algorithms for graphs, while being computationally much more efficient.},
author = {GIBERT, JAUME and VALVENY, ERNEST and BUNKE, HORST},
doi = {10.1142/S0218001413600021},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/GIBERT, VALVENY, BUNKE - 2013 - EMBEDDING OF GRAPHS WITH DISCRETE ATTRIBUTES VIA LABEL FREQUENCIES.pdf:pdf},
isbn = {0218001413600},
issn = {0218-0014},
journal = {International Journal of Pattern Recognition and Artificial Intelligence},
keywords = {discrete attributed graphs,graph classification,graph embedding},
month = {may},
number = {03},
title = {{EMBEDDING OF GRAPHS WITH DISCRETE ATTRIBUTES VIA LABEL FREQUENCIES}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218001413600021},
volume = {27},
year = {2013}
}
@article{Maggiora2006,
author = {Maggiora, GM},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Maggiora - 2006 - On outliers and activity cliffs why QSAR often disappoints.pdf:pdf},
journal = {Journal of chemical information and modeling},
number = {4},
pages = {60117},
title = {{On outliers and activity cliffs why QSAR often disappoints}},
url = {http://pubs.acs.org/doi/pdf/10.1021/ci060117s},
volume = {46},
year = {2006}
}
@inproceedings{PG06,
address = {Paris, France},
author = {Philipp-Foliguet, S and Gony, J},
booktitle = {IPMU},
title = {{FReBIR : Fuzzy Region-Based Image Retrieval}},
url = {http://publi-etis.ensea.fr/2006/PG06},
year = {2006}
}
@inproceedings{Kashima03marginalizedkernels,
annote = {From Duplicate 2 ( 

Marginalized kernels between labeled graphs

- Kashima, Hisashi; Tsuda, Koji; Inokuchi, Akihiro )

},
author = {Kashima, Hisashi and Tsuda, Koji and Inokuchi, Akihiro},
booktitle = {ICML},
editor = {Fawcett, Tom and Mishra, Nina},
isbn = {1-57735-189-4},
pages = {321--328},
publisher = {AAAI Press},
title = {{Marginalized kernels between labeled graphs}},
year = {2003}
}
@inproceedings{Harary1964,
author = {Harary, Frank},
booktitle = {Theory of Graphs and its Applications (Proc. Sympos. Smolenice, 1963)},
pages = {47----52},
title = {{On the reconstruction of a graph from a collection of subgraphs}},
year = {1964}
}
@article{Chapelle2007,
address = {Cambridge, MA, USA},
author = {Chapelle, Olivier},
doi = {http://dx.doi.org/10.1162/neco.2007.19.5.1155},
issn = {0899-7667},
journal = {Neural Comput.},
number = {5},
pages = {1155--1178},
publisher = {MIT Press},
title = {{Training a Support Vector Machine in the Primal}},
volume = {19},
year = {2007}
}
@book{Taylor-Cristianini:Book2004,
address = {New York, NY, USA},
author = {Shawe-Taylor, John and Cristianini, Nello},
isbn = {0521813972},
publisher = {Cambridge University Press},
title = {{Kernel Methods for Pattern Analysis}},
year = {2004}
}
@article{VapnikVladimirandCortes1995,
author = {Vapnik, Vladimir and Cortes, Corinna},
doi = {10.1023/A:1022627411411},
journal = {MACHINE LEARNING},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
title = {{Support-Vector Networks}},
volume = {20},
year = {1995}
}
@inproceedings{Vries2013,
annote = {@inproceedings{\{}DBLP:dblp{\_}conf/dmold/VriesR13,
   author              = {\{}Gerben Klaas Dirk de Vries and 
                          Steven de Rooij{\}},
   title               = {\{}A Fast and Simple Graph Kernel for RDF.{\}},
   booktitle           = {\{}DMoLD{\}},
   year                = {\{}2013{\}},
   ee                  = {\{}http://ceur-ws.org/Vol-1082/paper2.pdf{\}},
   crossref            = {\{}2013{\}},
{\}}

        

      },
author = {Vries, GKD De and Rooij, Steven De},
booktitle = {DMoLD},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vries, Rooij - 2013 - A fast and simple graph kernel for rdf.pdf:pdf},
keywords = {graph kernels,intersec-,rdf,resource description framework},
title = {{A fast and simple graph kernel for rdf}},
year = {2013}
}
@techreport{Haussler99convolutionkernels,
annote = {From Duplicate 1 ( 

Convolution Kernels on Discrete Structures

- Haussler, David )

},
author = {Haussler, David},
institution = {Department of Computer Science, University of California at Santa Cruz},
title = {{Convolution Kernels on Discrete Structures}},
year = {1999}
}
@inproceedings{Bach2008,
abstract = {Point clouds are sets of points in two or three dimensions. Most kernel methods for learning on sets of points have not yet dealt with the specific geometrical invariances and practical constraints associated with point clouds in computer vision and graphics. In this paper, we present extensions of graph kernels for point clouds, which allow to use kernel methods for such ob jects as shapes, line drawings, or any three-dimensional point clouds. In order to design rich and numerically efficient kernels with as few free parameters as possible, we use kernels between covariance matrices and their factorizations on graphical models. We derive polynomial time dynamic programming recursions and present applications to recognition of handwritten digits and Chinese characters from few training examples.},
address = {New York, New York, USA},
annote = {        From Duplicate 1 (                   Graph kernels between point clouds                 - Bach, Francis R. )
                
        
        
      },
author = {Bach, Francis R.},
booktitle = {Proceedings of the 25th international conference on Machine learning - ICML '08},
doi = {10.1145/1390156.1390160},
isbn = {9781605582054},
pages = {25--32},
publisher = {ACM Press},
title = {{Graph kernels between point clouds}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390160 http://arxiv.org/abs/0712.3402v1},
year = {2008}
}
@article{Fan2014,
abstract = {In this paper, we propose a recurrent kernel algorithm with selectively sparse updates for online learning. The algorithm introduces a linear recurrent term in the estimation of the current output. This makes the past information reusable for updating of the algorithm in the form of a recurrent gradient term. To ensure that the reuse of this recurrent gradient indeed accelerates the convergence speed, a novel hybrid recurrent training is proposed to switch on or off learning the recurrent information according to the magnitude of the current training error. Furthermore, the algorithm includes a data-dependent adaptive learning rate which can provide guaranteed system weight convergence at each training iteration. The learning rate is set as zero when the training violates the derived convergence conditions, which makes the algorithm updating process sparse. Theoretical analyses of the weight convergence are presented and experimental results show the good performance of the proposed algorithm in terms of convergence speed and estimation accuracy.},
author = {Fan, Haijin and Song, Qing},
doi = {10.1016/j.neunet.2013.11.011},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fan, Song - 2014 - A linear recurrent kernel online learning algorithm with sparse updates.pdf:pdf},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Computer Simulation,Humans,Learning,Learning: physiology,Linear Models,Neural Networks (Computer),Online Systems},
month = {feb},
pages = {142--53},
pmid = {24300551},
publisher = {Elsevier Ltd},
title = {{A linear recurrent kernel online learning algorithm with sparse updates.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24300551},
volume = {50},
year = {2014}
}
@article{Elghraoui2010,
author = {Elghraoui, Sarah},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Elghraoui - 2010 - Graphs, Zeta Functions, Diameters, and Cospectrality.pdf:pdf},
title = {{Graphs, Zeta Functions, Diameters, and Cospectrality}},
year = {2010}
}
@article{vishwanathan_fast_2003,
author = {Vishwanathan, S V N and Smola, Alexander J},
doi = {10.1.1.4.9887},
journal = {NIPS},
keywords = {kernel{\_}methods,pattern{\_}matching,text{\_}kernels},
title = {{Fast kernels for string and tree matching}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.9887},
volume = {15},
year = {2003}
}
@article{Bunke:fk,
abstract = {One of the major difficulties in graph classification is the lack of mathematical structure in the space of graphs. The use of kernel machines allows us to overcome this fundamental limitation in an elegant manner by addressing the pattern recognition problem in an implicitly existing feature vector space instead of the original space of graphs. In this paper we propose three novel error-tolerant graph kernels -- a diffusion kernel, a convolution kernel, and a random walk kernel. The kernels are closely related to one of the most flexible graph matching methods, graph edit distance. Consequently, our kernels are applicable to virtually any kind of graph. They also show a high degree of robustness against various types of distortion. In an experimental evaluation involving the classification of line drawings, images, diatoms, fingerprints, and molecules, we demonstrate the superior performance of the proposed kernels in conjunction with support vector machines over a standard nearest-neighbor reference method and several other graph kernels including a standard random walk kernel.},
address = {Utrecht, the Netherlands},
author = {Neuhaus, Michel and Riesen, Kaspar and Bunke, Horst H K M M K H},
doi = {10.1163/156856809789476119},
issn = {0169-1015},
journal = {SPAT VIS; Spatial vision},
keywords = {Computer Graphics,Computer Graphics: classification,Distance Perception,Distance Perception: physiology,Humans,Models,Pattern Recognition,Space Perception,Space Perception: physiology,Theoretical,Visual,Visual: physiology},
month = {jan},
number = {5},
pages = {425--441},
pmid = {19814905},
publisher = {VNU Science Press},
title = {{Novel kernels for error-tolerant graph classification.; Spatial vision}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19814905},
volume = {22},
year = {2009}
}
@article{Bisognin2009,
abstract = {{\{}BACKGROUND:Publicly{\}} available datasets of microarray gene expression
signals represent an unprecedented opportunity for extracting genomic
relevant information and validating biological hypotheses. However,
the exploitation of this exceptionally rich mine of information is
still hampered by the lack of appropriate computational tools, able
to overcome the critical issues raised by {\{}meta-analysis.RESULTS:This{\}}
work presents {\{}A-MADMAN,{\}} an open source web application which allows
the retrieval, annotation, organization and meta-analysis of gene
expression datasets obtained from Gene Expression Omnibus. {\{}A-MADMAN{\}}
addresses and resolves several open issues in the meta-analysis of
gene expression {\{}data.CONCLUSION:A-MADMAN{\}} allows i) the batch retrieval
from Gene Expression Omnibus and the local organization of raw data
files and of any related meta-information, ii) the re-annotation
of samples to fix incomplete, or otherwise inadequate, metadata and
to create user-defined batches of data, iii) the integrative analysis
of data obtained from different Affymetrix platforms through custom
chip definition files and meta-normalization. Software and documentation
are available on-line at http://compgen.bio.unipd.it/bioinfo/amadman/.},
author = {Bisognin, Andrea and Coppe, Alessandro and Ferrari, Francesco and Risso, Davide and Romualdi, Chiara and Bicciato, Silvio and Bortoluzzi, Stefania},
doi = {10.1186/1471-2105-10-201},
issn = {1471-2105},
journal = {{\{}BMC{\}} Bioinformatics},
number = {1},
pages = {201},
shorttitle = {A-MADMAN},
title = {{A-MADMAN: Annotation-based microarray data meta-analysis tool}},
volume = {10},
year = {2009}
}
@article{Cortes1995,
author = {Cortes, C and Jackel, Ld and Chiang, Wp},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Jackel, Chiang - 1995 - Limits on learning machine accuracy imposed by data quality.pdf:pdf},
journal = {Kdd},
pages = {57--62},
title = {{Limits on learning machine accuracy imposed by data quality}},
year = {1995}
}
@article{Herne1973,
abstract = {This book is a sort of primer in ways to use statistics to deceive. It may seem altogether too much like a manual for sWindlers. Perhaps I can justify it in the manner of the retired burglar whose published reminiscences amounted to a graduate course in how to pick a lock and mume a footfall: The crooks already know these tricks; honest men must learn them in selfdefense.},
author = {Herne, H. and Huff, Darrell},
doi = {10.2307/2346789},
isbn = {0393310728},
issn = {00359254},
journal = {Applied Statistics},
pages = {401},
pmid = {1880013},
title = {{How to Lie with Statistics.}},
url = {http://www.jstor.org/stable/10.2307/2346789?origin=crossref},
volume = {22},
year = {1973}
}
@inproceedings{Kimura2011,
author = {Kimura, Daisuke and Kuboyama, Tetsuji and Shibuya, Tetsuo and Kashima, Hisashi},
booktitle = {In Proceedings of the 15th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD), Shenzeng, China},
title = {{A Subpath Kernel for Rooted Unordered Trees}},
year = {2011}
}
@inproceedings{Coppola2008,
abstract = {Supervised approaches to data mining are particularly appealing as
they allow for the extraction of complex relations from data objects.
In order to facilitate their application in different areas, ranging
from protein to protein interaction in bioinformatics to text mining
in computational linguistics research, a modular and general mining
framework is needed. The major constraint to the generalization process
concerns the feature design for the description of relational data.
In this paper, we present a machine learning framework for the automatic
mining of relations, where the target objects are structurally organized
in a tree. Object types are generalized by means of the use of roles,
whereas the relation properties are described by means of the underlying
tree structure. The latter is encoded in the learning algorithm thanks
to kernel methods for structured data, which represent structures
in terms of their all possible subparts. This approach can be applied
to any kind of data disregarding their very nature. Experiments with
support vector machines on two text mining datasets for relation
extraction, i.e. the {\{}PropBank{\}} and {\{}FrameNet{\}} corpora, show both
that our approach is general, and that it reaches state-of-the-art
accuracy.},
author = {Coppola, B and Moschitti, A and Pighin, D},
booktitle = {Data Mining, 2008. {\{}ICDM{\}} '08. Eighth {\{}IEEE{\}} International Conference on},
doi = {10.1109/ICDM.2008.153},
isbn = {1550-4786},
pages = {153--162},
title = {{Generalized Framework for {\{}Syntax-Based{\}} Relation Mining}},
year = {2008}
}
@article{Carolina1979,
author = {Carolina, North and Umverslty, State and Carohna, North},
journal = {Computing},
keywords = {and phrases},
number = {3},
pages = {422--433},
title = {{The Tree-to-Tree Correction Problem}},
year = {1979}
}
@article{Orabona,
author = {Orabona, Francesco and Crammer, Koby and Technion, The},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Orabona, Crammer, Technion - Unknown - New Adaptive Algorithms for Online Classification.pdf:pdf},
pages = {1--9},
title = {{New Adaptive Algorithms for Online Classification}}
}
@inproceedings{Alt1997,
author = {Alt, H. and Fuchs, U. and Kriegel, K.},
pages = {15--24},
publisher = {Springer},
title = {{On the number of simple cycles in planar graphs}},
url = {http://www.springerlink.com/index/m60xw717tj7u6144.pdf},
year = {1997}
}
@inproceedings{Williams2001,
abstract = {A major problem for kernel-based predictors (such as Support Vector Machines and Gaussian processes) is that the amount of computation required to find the solution scales as O(n ), where n is the number of training examples. We show that an approximation to the eigendecomposition of the Gram matrix can be computed by the Nystr{\"{o}}m method (which is used for the numerical solution of eigenproblems). This is achieved by carrying out an eigendecomposition on a smaller system of size m {\textless} n, and then expanding the results back up to n dimensions. The computational complexity of a predictor using this approximation is O(m n). We report experiments on the USPS and abalone data sets and show that we can set m n without any significant decrease in the accuracy of the solution.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Williams, Christopher and Seeger, Matthias},
booktitle = {Advances in Neural Information Processing Systems 13},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Williams, Seeger - 2001 - Using the Nystr{\"{o}}m Method to Speed Up Kernel Machines.ps:ps},
isbn = {9788578110796},
issn = {1098-6596},
pages = {682--688},
pmid = {25246403},
title = {{Using the Nystr{\"{o}}m Method to Speed Up Kernel Machines}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.7519},
year = {2001}
}
@article{Neumann2014,
abstract = {We introduce propagation kernels, a general graph-kernel framework for efficiently measuring the similarity of structured data. Propagation kernels are based on monitoring how information spreads through a set of given graphs. They leverage early-stage distributions from propagation schemes such as random walks to capture structural information encoded in node labels, attributes, and edge information. This has two benefits. First, off-the-shelf propagation schemes can be used to naturally construct kernels for many graph types, including labeled, partially labeled, unlabeled, directed, and attributed graphs. Second, by leveraging existing efficient and informative propagation schemes, propagation kernels can be considerably faster than state-of-the-art approaches without sacrificing predictive performance. We will also show that if the graphs at hand have a regular structure, for instance when modeling image or video data, one can exploit this regularity to scale the kernel computation to large databases of graphs with thousands of nodes. We support our contributions by exhaustive experiments on a number of real-world graphs from a variety of application domains.},
archivePrefix = {arXiv},
arxivId = {1410.3314},
author = {Neumann, Marion and Garnett, Roman and Bauckhage, Christian and Kersting, Kristian},
eprint = {1410.3314},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neumann et al. - 2014 - Propagation Kernels.pdf:pdf},
month = {oct},
title = {{Propagation Kernels}},
url = {http://arxiv.org/abs/1410.3314},
year = {2014}
}
@article{Zhang2012,
author = {Zhang, Chao and Bian, Wei and Tao, Dacheng and Lin, Weisi},
doi = {10.1109/TNNLS.2012.2204773},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2012 - Discretized-Vapnik-Chervonenkis Dimension for Analyzing Complexity of Real Function Classes.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {sep},
number = {9},
pages = {1461--1472},
title = {{Discretized-Vapnik-Chervonenkis Dimension for Analyzing Complexity of Real Function Classes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6239601},
volume = {23},
year = {2012}
}
@misc{,
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - Data Structures and Algorithms in C 2nd Edition.pdf.pdf:pdf},
title = {{Data Structures and Algorithms in C++ 2nd Edition.pdf}}
}
@inproceedings{Oneto2016,
author = {Oneto, Luca and Navarin, Nicol{\`{o}} and Donini, Michele and Sperduti., Alessandro and Aiolli, Fabio and Anguita, Davide},
booktitle = {European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
title = {{Measuring the Expressivity of Graph Kernels through the Rademacher Complexity}},
year = {2016}
}
@article{Argyriou2009,
author = {Argyriou, Andreas and Micchelli, Charles A and Pontil, Massimiliano},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {dec},
pages = {2507--2529},
publisher = {JMLR.org},
title = {{When Is There a Representer Theorem? Vector Versus Matrix Regularizers}},
volume = {10},
year = {2009}
}
@article{Bansal2007,
abstract = {Inferring, or 'reverse-engineering', gene networks can be defined
as the process of identifying gene interactions from experimental
data through computational analysis. Gene expression data from microarrays
are typically used for this purpose. Here we compared different reverse-engineering
algorithms for which ready-to-use software was available and that
had been tested on experimental data sets. We show that reverse-engineering
algorithms are indeed able to correctly infer regulatory interactions
among genes, at least when one performs perturbation experiments
complying with the algorithm requirements. These algorithms are superior
to classic clustering algorithms for the purpose of finding regulatory
interactions among genes, and, although further improvements are
needed, have reached a discreet performance for being practically
useful.},
author = {Bansal, Mukesh and Belcastro, Vincenzo and Ambesi-Impiombato, Alberto and di Bernardo, Diego},
doi = {10.1038/msb4100120},
issn = {1744-4292},
journal = {Molecular systems biology},
month = {feb},
publisher = {EMBO and Nature Publishing Group},
title = {{How to infer gene networks from expression profiles.}},
volume = {3},
year = {2007}
}
@article{Waegeman,
author = {Waegeman, Willem},
journal = {Current},
pages = {1--9},
title = {{An Exact Algorithm for F-Measure Maximization}}
}
@inproceedings{Navarin2015,
author = {Navarin, Nicol{\`{o}} and Sperduti, Alessandro and Tesselli, Riccardo},
booktitle = {22nd International Conference on Neural Information Processing},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Navarin, Sperduti, Tesselli - 2015 - Extending local features with contextual information in graph kernels.pdf:pdf},
pages = {271--279},
title = {{Extending local features with contextual information in graph kernels}},
year = {2015}
}
@article{Swamidass2005,
abstract = {Small molecules play a fundamental role in organic chemistry and biology. They can be used to probe biological systems and to discover new drugs and other useful compounds. As increasing numbers of large datasets of small molecules become available, it is necessary to develop computational methods that can deal with molecules of variable size and structure and predict their physical, chemical and biological properties.},
author = {Swamidass, S Joshua and Chen, Jonathan and Bruand, Jocelyne and Phung, Peter and Ralaivola, Liva and Baldi, Pierre},
doi = {10.1093/bioinformatics/bti1055},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Swamidass et al. - 2005 - Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Animals,Antineoplastic Agents,Antineoplastic Agents: pharmacology,Computational Biology,Computational Biology: methods,Computer Simulation,Databases, Factual,Drug Toxicity,Female,Male,Mice,Models, Molecular,Models, Statistical,Mutagens,Neoplasms,Neoplasms: pathology,Pattern Recognition, Automated,ROC Curve,Rats},
month = {jun},
number = {2},
pages = {i359--68},
pmid = {15961479},
title = {{Kernels for small molecules and the prediction of mutagenicity, toxicity and anti-cancer activity.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15961479},
volume = {21 Suppl 1},
year = {2005}
}
@inproceedings{Suzuki2006,
address = {Vancouver},
author = {Suzuki, Jun and Isozaki, Hideki},
booktitle = {Advances in Neural Information Processing Systems},
publisher = {{\{}MIT{\}} Press},
title = {{Sequence and tree kernels with statistical feature mining}},
year = {2006}
}
@book{Russel1995,
author = {Russel, Stuart and Norvig, Peter},
booktitle = {Prentice Hall},
isbn = {0131038052},
issn = {1873-2976},
month = {jan},
title = {{Artificial Intelligence A Modern Approach}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22244610},
year = {1995}
}
@incollection{springerlink:10.1007/978-3-540-35488-8_13,
annote = {10.1007/978-3-540-35488-8{\_}13},
author = {Chen, Yi-Wei and Lin, Chih-Jen},
booktitle = {Feature Extraction},
doi = {file:///home/nick/Scaricati/fulltext.pdf},
editor = {Guyon, Isabelle and Nikravesh, Masoud and Gunn, Steve and Zadeh, Lotfi},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chen, Lin - 2006 - Combining SVMs with Various Feature Selection Strategies.pdf:pdf},
isbn = {978-3-540-35487-1},
pages = {315--324},
publisher = {Springer Berlin / Heidelberg},
series = {Studies in Fuzziness and Soft Computing},
title = {{Combining SVMs with Various Feature Selection Strategies}},
url = {http://dx.doi.org/10.1007/978-3-540-35488-8{\_}13},
volume = {207},
year = {2006}
}
@article{Steffen2006,
abstract = {We introduce RNAshapes, a new software package that integrates three RNA analysis tools based on the abstract shapes approach: the analysis of shape representatives, the calculation of shape probabilities and the consensus shapes approach. This new package is completely reimplemented in C and outruns the original implementations significantly in runtime and memory requirements. Additionally, we added a number of useful features like suboptimal folding with correct dangling energies, structure graph output, shape matching and a sliding window approach.},
author = {Steffen, Peter and Voss, Bj{\"{o}}rn and Rehmsmeier, Marc and Reeder, Jens and Giegerich, Robert},
doi = {10.1093/bioinformatics/btk010},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Steffen et al. - 2006 - RNAshapes an integrated RNA analysis package based on abstract shapes.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Computer Graphics,Computer Simulation,Models, Chemical,Models, Molecular,Nucleic Acid Conformation,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,RNA,RNA: analysis,RNA: chemistry,Sequence Analysis, RNA,Sequence Analysis, RNA: methods,Software,Systems Integration,User-Computer Interface},
month = {mar},
number = {4},
pages = {500--3},
pmid = {16357029},
title = {{RNAshapes: an integrated RNA analysis package based on abstract shapes.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16357029},
volume = {22},
year = {2006}
}
@inproceedings{Ortho,
author = {Flint, Ortho},
organization = {CMS},
title = {{Graph Isomorphism Testing}},
year = {2010}
}
@inproceedings{Dekel:2006,
abstract = {The Perceptron algorithm, despite its simplicity, often performs well in online classification tasks. The Perceptron becomes especially effective when it is used in conjunction with kernels. However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly. In this paper we present and analyze the Forgetron algorithm for kernel-based online learning on a fixed memory budget. To our knowledge, this is the first online learning algorithm which, on one hand, maintains a strict limit on the number of examples it stores and, on the other hand, entertains a relative mistake bound. In addition to the formal results, we also present experiments with real datasets which underscore the merits of our approach.},
address = {Cambridge, MA},
author = {Dekel, Ofer and Shwartz, Shai S and Singer, Yoram},
booktitle = {Advances in Neural Information Processing Systems 18},
keywords = {kernel-trick,perceptron-learning-rule,phd},
pages = {1342--1372},
publisher = {MIT Press},
title = {{The Forgetron: A kernel-based perceptron on a fixed budget}},
year = {2006}
}
@book{svmlight,
author = {Joachims, T},
editor = {{B. Scholkopf C. Burges} and Smola, A},
publisher = {Advances in Kernel Methods - Support Vector Learning},
title = {{Making large-Scale SVM Learning Practical.}},
year = {1999}
}
@article{Dunn1961,
author = {Dunn, O J},
journal = {Journ. of Am. Stat. Ass.},
pages = {52--64},
title = {{Multiple comparisons among means}},
volume = {56},
year = {1961}
}
@inproceedings{DBLP:conf/nips/1999,
booktitle = {NIPS},
editor = {Solla, Sara A and Leen, Todd K and M{\"{u}}ller, Klaus-Robert},
isbn = {0-262-19450-3},
publisher = {The MIT Press},
title = {{Advances in Neural Information Processing Systems 12, [NIPS Conference, Denver, Colorado, USA, November 29 - December 4, 1999]}},
year = {2000}
}
@book{Boyd2004,
abstract = {Convex optimization problems arise frequently in many different fields.
A comprehensive introduction to the subject, this book shows in detail
how such problems can be solved numerically with great efficiency.
The focus is on recognizing convex optimization problems and then
finding the most appropriate technique for solving them. The text
contains many worked examples and homework exercises and will appeal
to students, researchers and practitioners in fields such as engineering,
computer science, mathematics, statistics, finance, and economics.},
annote = {Code, PDF here: http://www.stanford.edu/{\~{}}boyd/cvxbook/},
author = {Boyd, Stephen and Vandenberghe, Lieven},
howpublished = {Hardcover},
isbn = {0521833787},
month = {mar},
publisher = {Cambridge University Press},
title = {{Convex Optimization}},
year = {2004}
}
@article{Fujibuchi2007,
abstract = {{\{}BACKGROUND:There{\}} is a large amount of microarray data accumulating
in public databases, providing various data waiting to be analyzed
jointly. Powerful kernel-based methods are commonly used in microarray
analyses with support vector machines {\{}(SVMs){\}} to approach a wide
range of classification problems. However, the standard vectorial
data kernel family (linear, {\{}RBF,{\}} etc.) that takes vectorial data
as input, often fails in prediction if the data come from different
platforms or laboratories, due to the low gene overlaps or consistencies
between the different {\{}datasets.RESULTS:We{\}} introduce a new type
of kernel called maximum entropy {\{}(ME){\}} kernel, which has no pre-defined
function but is generated by kernel entropy maximization with sample
distance matrices as constraints, into the field of {\{}SVM{\}} classification
of microarray data. We assessed the performance of the {\{}ME{\}} kernel
with three different data: heterogeneous kidney carcinoma, noise-introduced
leukemia, and heterogeneous oral cavity carcinoma metastasis data.
The results clearly show that the {\{}ME{\}} kernel is very robust for
heterogeneous data containing missing values and high-noise, and
gives higher prediction accuracies than the standard kernels, namely,
linear, polynomial and {\{}RBF.CONCLUSION:The{\}} results demonstrate its
utility in effectively analyzing promiscuous microarray data of rare
specimens, e.g., minor diseases or species, that present difficulty
in compiling homogeneous data in a single laboratory.},
author = {Fujibuchi, Wataru and Kato, Tsuyoshi},
doi = {10.1186/1471-2105-8-267},
issn = {1471-2105},
journal = {{\{}BMC{\}} Bioinformatics},
number = {1},
pages = {267},
title = {{Classification of heterogeneous microarray data by maximum entropy kernel}},
volume = {8},
year = {2007}
}
@inproceedings{DBLP:conf/atal/2003esoa,
booktitle = {Engineering Self-Organising Systems},
editor = {{Di Marzo Serugendo}, Giovanna and Karageorgos, Anthony and Rana, Omer F and Zambonelli, Franco},
isbn = {3-540-21201-9},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Engineering Self-Organising Systems, Nature-Inspired Approaches to Software Engineering [revised and extended papers presented at the Engineering Self-Organising Applications Workshop, ESOA 2003, held at AAMAS 2003 in Melbourne, Australia, in July 2003 an}},
volume = {2977},
year = {2004}
}
@article{Chen1984,
author = {Chen, WC},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chen - 1984 - Hierarchy of Graph Isomorphism Testing.pdf:pdf},
title = {{Hierarchy of Graph Isomorphism Testing}},
url = {http://authors.library.caltech.edu/26943/1/5140{\_}TR{\_}84.pdf},
year = {1984}
}
@article{Langford:2009:SOL:1577069.1577097,
author = {Langford, John and Li, Lihong and Zhang, Tong},
issn = {1532­4435},
journal = {J. Mach. Learn. Res.},
month = {jun},
pages = {777­­801},
title = {{Sparse Online Learning via Truncated Gradient}},
volume = {10},
year = {2009}
}
@article{Mattei2014,
abstract = {Structural information is crucial in ribonucleic acid (RNA) analysis and functional annotation; nevertheless, how to include such structural data is still a debated problem. Dot-bracket notation is the most common and simple representation for RNA secondary structures but its simplicity leads also to ambiguity requiring further processing steps to dissolve. Here we present BEAR (Brand nEw Alphabet for RNA), a new context-aware structural encoding represented by a string of characters. Each character in BEAR encodes for a specific secondary structure element (loop, stem, bulge and internal loop) with specific length. Furthermore, exploiting this informative and yet simple encoding in multiple alignments of related RNAs, we captured how much structural variation is tolerated in RNA families and convert it into transition rates among secondary structure elements. This allowed us to compute a substitution matrix for secondary structure elements called MBR (Matrix of BEAR-encoded RNA secondary structures), of which we tested the ability in aligning RNA secondary structures. We propose BEAR and the MBR as powerful resources for the RNA secondary structure analysis, comparison and classification, motif finding and phylogeny.},
author = {Mattei, Eugenio and Ausiello, Gabriele and Ferr{\`{e}}, Fabrizio and Helmer-Citterich, Manuela},
doi = {10.1093/nar/gku283},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mattei et al. - 2014 - A novel approach to represent and compare RNA secondary structures.pdf:pdf},
isbn = {0039062023},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {nucleic acids research},
month = {apr},
pmid = {24753415},
title = {{A novel approach to represent and compare RNA secondary structures.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24753415},
year = {2014}
}
@article{brachman-schmolze:kl-one,
author = {Brachman, Ronald{\~{}}J. and Schmolze, James{\~{}}G.},
journal = {Cognitive Science},
number = {2},
pages = {171--216},
title = {{An overview of the {\{}KL-ONE{\}} knowledge representation system}},
volume = {9},
year = {1985}
}
@inproceedings{Zhao2006,
abstract = {Support vector machine {\{}(SVM){\}} has been extensively studied and has
shown remarkable success in many applications. However, when faced
with unbalanced datasets, the {\{}SVM{\}} can not get ideal classification
result and even in some cases the classification ability was very
bad and unaccepted. The V-support vector machine {\{}(V-SVM){\}} is a new
formulation of the regular {\{}SVM,{\}} and its parameter V has intuitive
meanings compared with C (the penalty constant in {\{}SVM).{\}} By investigating
the relation between {\{}SVM{\}} and {\{}V-SVM,{\}} we gave an equation between
V and C, meanwhile we analyzed the factor behind the classification
failure of {\{}SVM{\}} on unbalanced dataset. Then a classification algorithm
based on {\{}V-SVM{\}} was addressed to overcome this inconvenience. Experimental
results show the effectiveness of the proposed algorithm},
author = {Zhao, Yinggang and He, Qinming},
booktitle = {Intelligent Control and Automation, 2006. {\{}WCICA{\}} 2006. The Sixth World Congress on},
doi = {10.1109/WCICA.2006.1714061},
pages = {10496--10501},
title = {{An Unbalanced Dataset Classification Approach Based on {\{}v-Support{\}} Vector Machine}},
volume = {2},
year = {2006}
}
@article{Shervashidze2015,
abstract = {Structured sparsity has recently emerged in statistics, machine learning and signal processing as a promising paradigm for learning in high-dimensional settings. All existing methods for learning under the assumption of structured sparsity rely on prior knowledge on how to weight (or how to penalize) individual subsets of variables during the subset selection process, which is not available in general. Inferring group weights from data is a key open research problem in structured sparsity. In this paper, we propose a Bayesian approach to the problem of group weight learning. We model the group weights as hyperparameters of heavy-tailed priors on groups of variables and derive an approximate inference scheme to infer these hyperparameters. We empirically show that we are able to recover the model hyperparameters when the data are generated from the model, and we demonstrate the utility of learning weights in synthetic and real denoising problems.},
archivePrefix = {arXiv},
arxivId = {1503.03082},
author = {Shervashidze, Nino and Bach, Francis},
doi = {10.1109/TSP.2015.2446432},
eprint = {1503.03082},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shervashidze, Bach - 2015 - Learning the Structure for Structured Sparsity.pdf:pdf},
isbn = {1053-587X},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {Bayesian statistics,Gaussian scale mixture,Structured sparsity,probabilistic modeling,super-Gaussian prior,variational inference},
number = {18},
pages = {4894--4902},
title = {{Learning the Structure for Structured Sparsity}},
volume = {63},
year = {2015}
}
@inproceedings{bonchi2005,
address = {Hanoi, Vietnam},
author = {Bonchi, F and Lucchese, C},
booktitle = {PAKDD '05},
pages = {114--124},
title = {{Pushing tougher constraints in frequent pattern mining}},
year = {2005}
}
@article{Helma2004,
abstract = {This paper explores the utility of data mining and machine learning algorithms for the induction of mutagenicity structure-activity relationships (SARs) from noncongeneric data sets. We compare (i) a newly developed algorithm (MOLFEA) for the generation of descriptors (molecular fragments) for noncongeneric compounds with traditional SAR approaches (molecular properties) and (ii) different machine learning algorithms for the induction of SARs from these descriptors. In addition we investigate the optimal parameter settings for these programs and give an exemplary interpretation of the derived models. The predictive accuracies of models using MOLFEA derived descriptors is approximately 10-15{\%}age points higher than those using molecular properties alone. Using both types of descriptors together does not improve the derived models. From the applied machine learning techniques the rule learner PART and support vector machines gave the best results, although the differences between the learning algorithms are only marginal. We were able to achieve predictive accuracies up to 78{\%} for 10-fold cross-validation. The resulting models are relatively easy to interpret and usable for predictive as well as for explanatory purposes.},
annote = {From Duplicate 1 ( 

Data mining and machine learning techniques for the identification of mutagenicity inducing substructures and structure activity relationships of noncongeneric compounds.

- Helma, Christoph; Cramer, Tobias; Kramer, Stefan; De Raedt, Luc )




From Duplicate 1 ( 


Data mining and machine learning techniques for the identification of mutagenicity inducing substructures and structure activity relationships of noncongeneric compounds.


- Helma, Christoph; Cramer, Tobias; Kramer, Stefan; De Raedt, Luc )

},
author = {Helma, Christoph and Cramer, Tobias and Kramer, Stefan and {De Raedt}, Luc},
doi = {10.1021/ci034254q},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Helma et al. - 2004 - Data mining and machine learning techniques for the identification of mutagenicity inducing substructures and stru.pdf:pdf},
issn = {0095-2338},
journal = {Journal of Chemical Information and Computer Sciences},
keywords = {Algorithms,Artificial Intelligence,Databases,Factual,Mutagenicity Tests,Mutagenicity Tests: statistics {\&} numerical data,Mutagens,Mutagens: chemistry,Mutagens: toxicity,Structure-Activity Relationship},
number = {4},
pages = {1402--1411},
pmid = {15272848},
title = {{Data mining and machine learning techniques for the identification of mutagenicity inducing substructures and structure activity relationships of noncongeneric compounds}},
volume = {44},
year = {2004}
}
@inproceedings{Ramon03expressivityversus,
abstract = {Recently, kernel methods have become a popular tool for machine learning and data mining. As most "real-world" data is structured, research in kernel methods has begun investigating kernels for various kinds of structured data. One of the most widely used tools for modeling structured data are graphs. In this paper we study the trade-off between expressivity and efficiency of graph kernels. First, we motivate the need for this discussion by showing that fully general graph kernels can not even be approximated efficiently.We also discuss generalizations of graph kernels defined in literature and show that they are either not positive definite or not very useful. Finally, we propose a new graph kernel based on subtree patterns. We argue that while a little more computationally expensive, this kernel is more expressive than kernels based on walks.},
annote = {        From Duplicate 2 (                           Expressivity versus efficiency of graph kernels                         - Ramon, Jan; G{\"{a}}rtner, Thomas )
                
        
        
      },
author = {Ramon, Jan and G{\"{a}}rtner, Thomas},
booktitle = {Proceedings of the First International Workshop on Mining Graphs, Trees and Sequences (MGTS-2003)},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ramon, G{\"{a}}rtner - 2003 - Expressivity versus efficiency of graph kernels(2).pdf:pdf},
pages = {65--74},
title = {{Expressivity versus efficiency of graph kernels}},
year = {2003}
}
@article{Moore,
author = {Moore, Joshua and K, Arnd Christian},
journal = {Learning},
pages = {1--9},
title = {{Hashing Algorithms for Large-Scale Learning}}
}
@article{Russell2008,
author = {Russell, B C and Torralba, Antonio},
journal = {International journal of Computer Vision},
number = {1-3},
pages = {157--173},
title = {{LabelMe: a database and web-based tool for image annotation}},
url = {http://www.springerlink.com/index/76X9J562653K0378.pdf},
volume = {77},
year = {2008}
}
@article{Zhu2005b,
abstract = {We present an algorithm based on convex optimization for constructing kernels for semi-supervised learning. The kernel matrices are derived from the spectral decomposition of graph Laplacians, and combine labeled and unlabeled data in a systematic fashion. Unlike previous work using diffusion kernels and Gaussian random field kernels, a nonparametric kernel approach is presented that incorporates order constraints during optimization. This results in flexible kernels and avoids the need to choose among different parametric forms. Our approach relies on a quadratically constrained quadratic program (QCQP), and is computationally feasible for large datasets. We evaluate the kernels on real datasets using support vector machines, with encouraging results.},
author = {Zhu, Xiaojin and Kandola, Jaz and Ghahramani, Zoubin and Lafferty, John},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2005 - Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning.pdf:pdf},
isbn = {0-262-20152-6},
keywords = {Computational, Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation},
title = {{Nonparametric Transforms of Graph Kernels for Semi-Supervised Learning}},
url = {http://eprints.pascal-network.org/archive/00000770/},
year = {2005}
}
@phdthesis{Schafer2006,
author = {Sch{\"{a}}fer, Juliane Stephanie},
month = {mar},
school = {LMU M{\{}{\"{u}}{\}}nchen: Faculty of Mathematics, Computer Science and Statistics},
title = {{Small-Sample Analysis and Inference of Networked Dependency Structures from Complex Genomic Data}},
year = {2006}
}
@article{Brun2013,
author = {Brun, Luc and Villemin, Didier},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Brun, Villemin - 2013 - Two New Graphs Kernels in Chemoinformatics.pdf:pdf},
title = {{Two New Graphs Kernels in Chemoinformatics}},
year = {2013}
}
@article{Gonzalez-Rosa2011,
abstract = {ABSTRACT:},
author = {Gonzalez-Rosa, Javier J and Vazquez-Marrufo, Manuel and Vaquero, Encarnacion and Duque, Pablo and Borges, Monica and Gomez-Gonzalez, Carlos M and Izquierdo, Guillermo},
doi = {10.1186/1471-2377-11-64},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gonzalez-Rosa et al. - 2011 - Cluster analysis Basis Concepts and Algorithms.pdf:pdf},
issn = {1471-2377},
journal = {BMC neurology},
month = {jan},
pages = {64},
pmid = {21635741},
title = {{Cluster analysis: Basis Concepts and Algorithms}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3128001{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {11},
year = {2011}
}
@article{Neuhaus20061852,
annote = {Similarity-based Pattern Recognition},
author = {Neuhaus, Michel and Bunke, Horst},
doi = {DOI: 10.1016/j.patcog.2006.04.012},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neuhaus, Bunke - 2006 - Edit distance-based kernel functions for structural pattern classification.pdf:pdf},
issn = {0031-3203},
journal = {Pattern Recognition},
keywords = {Support vector machine},
number = {10},
pages = {1852--1863},
title = {{Edit distance-based kernel functions for structural pattern classification}},
url = {http://www.sciencedirect.com/science/article/B6V14-4K48N7S-4/2/1e7743302ffe0f0662da24f14c7d5a8f},
volume = {39},
year = {2006}
}
@article{Baum1966,
author = {Baum, B Y Leonard E and Eagon, J A},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Baum, Eagon - 1966 - An inequality with applications to statistical estimation for probabilistic functions of markov processes and to a.pdf:pdf},
journal = {Ecology},
number = {2},
pages = {360--363},
title = {{An inequality with applications to statistical estimation for probabilistic functions of markov processes and to a model for ecology}},
volume = {3},
year = {1966}
}
@inproceedings{1390275,
address = {New York, NY, USA},
author = {Shin, Kilho and Kuboyama, Tetsuji},
booktitle = {ICML '08: Proceedings of the 25th international conference on Machine learning},
doi = {http://doi.acm.org/10.1145/1390156.1390275},
isbn = {978-1-60558-205-4},
pages = {944--951},
publisher = {ACM},
title = {{A generalization of Haussler's convolution kernel: mapping kernel}},
url = {http://portal.acm.org/ft{\_}gateway.cfm?id=1390275{\&}type=pdf{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=95568645{\&}CFTOKEN=19199977},
year = {2008}
}
@article{Ehrig2013,
author = {Ehrig, Hartmut and Ermel, Claudia and H{\"{u}}ffner, F and Niedermeier, Rolf and Runge, Olga},
doi = {10.3233/COM-13016},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ehrig et al. - 2013 - Confluence in data reduction bridging graph transformation and kernelization.pdf:pdf},
journal = {Computability},
keywords = {confluence,critical pair analysis,formal methods,parameterized algorithmics,problem kernel},
pages = {31--49},
title = {{Confluence in data reduction: bridging graph transformation and kernelization}},
url = {http://iospress.metapress.com/index/X420701656624621.pdf},
volume = {7318},
year = {2013}
}
@article{Aiolli2008,
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro},
doi = {10.1007/978-3-540-87536-9_32},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Aiolli, Da San Martino, Sperduti - 2008 - A kernel method for the optimization of the margin distribution.pdf:pdf},
isbn = {3540875352},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
keywords = {Game theory,Kernel methods,Margin distribution},
pages = {305--314},
title = {{A kernel method for the optimization of the margin distribution}},
volume = {5163},
year = {2008}
}
@article{Mattick2006,
abstract = {The term non-coding RNA (ncRNA) is commonly employed for RNA that does not encode a protein, but this does not mean that such RNAs do not contain information nor have function. Although it has been generally assumed that most genetic information is transacted by proteins, recent evidence suggests that the majority of the genomes of mammals and other complex organisms is in fact transcribed into ncRNAs, many of which are alternatively spliced and/or processed into smaller products. These ncRNAs include microRNAs and snoRNAs (many if not most of which remain to be identified), as well as likely other classes of yet-to-be-discovered small regulatory RNAs, and tens of thousands of longer transcripts (including complex patterns of interlacing and overlapping sense and antisense transcripts), most of whose functions are unknown. These RNAs (including those derived from introns) appear to comprise a hidden layer of internal signals that control various levels of gene expression in physiology and development, including chromatin architecture/epigenetic memory, transcription, RNA splicing, editing, translation and turnover. RNA regulatory networks may determine most of our complex characteristics, play a significant role in disease and constitute an unexplored world of genetic variation both within and between species.},
author = {Mattick, John S and Makunin, Igor V},
doi = {10.1093/hmg/ddl046},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Mattick, Makunin - 2006 - Non-coding RNA.pdf:pdf},
issn = {0964-6906},
journal = {Human molecular genetics},
keywords = {Animals,Gene Expression Regulation,Humans,Introns,MicroRNAs,MicroRNAs: genetics,MicroRNAs: metabolism,Models, Biological,Models, Genetic,RNA Precursors,RNA Precursors: genetics,RNA Precursors: metabolism,RNA, Messenger,RNA, Messenger: genetics,RNA, Messenger: metabolism,RNA, Untranslated,RNA, Untranslated: genetics,RNA, Untranslated: metabolism,Transcription, Genetic},
month = {apr},
number = {1},
pages = {R17--29},
pmid = {16651366},
title = {{Non-coding RNA.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16651366},
volume = {15 Spec No},
year = {2006}
}
@article{,
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - 2015 - CS229TSTAT231 Statistical Learning Theory (Winter 2015).pdf:pdf},
number = {Winter},
title = {{CS229T/STAT231: Statistical Learning Theory (Winter 2015)}},
year = {2015}
}
@inproceedings{Martino2013,
author = {{Da San Martino}, G and Navarin, N and Sperduti, Alessandro},
booktitle = {IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9, 2013.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Da San Martino, Navarin, Sperduti - 2013 - A Lossy Counting Based Approach for Learning on Streams of Graphs on a Budget.pdf:pdf},
pages = {1294--1301},
publisher = {IJCAI/AAAI},
title = {{A Lossy Counting Based Approach for Learning on Streams of Graphs on a Budget}},
url = {http://www.aaai.org/ocs/index.php/IJCAI/IJCAI13/paper/view/6699},
year = {2013}
}
@article{Bapat2003,
author = {Bapat, Ravindra B and Gutman, Ivan and Xiao, Wenjun},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bapat, Gutman, Xiao - 2003 - A Simple Method for Computing Resistance Distance.pdf:pdf},
journal = {Computing},
keywords = {kirchhoff index,laplacian matrix,molecular graph,resistance distance},
number = {1},
pages = {494--498},
title = {{A Simple Method for Computing Resistance Distance}},
year = {2003}
}
@article{moschitti,
abstract = {In recent years tree kernels have been proposed for the automatic
learning of natural language applications. Unfortunately, they show
(a) an inherent super linear complexity and (b) a lower accuracy
than traditional attribute/value methods.},
annote = {From Duplicate 1 ( 


Making tree kernels practical for natural language learning


- Moschitti, Alessandro )

},
author = {Moschitti, Alessandro},
journal = {Proceedings of the Eleventh International Conference on European Association for Computational Linguistics, Trento, Italy},
title = {{Making tree kernels practical for natural language learning}},
year = {2006}
}
@article{Chau,
author = {Chau, By Polo},
doi = {10.1145/1925041.1925044},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chau - Unknown - Catching Bad Guys with Graph Mining.cfm:cfm},
journal = {Spring},
number = {3},
pages = {16--18},
title = {{Catching Bad Guys with Graph Mining}}
}
@article{Dekel2008,
abstract = {The Perceptron algorithm, despite its simplicity, often performs well in online classification tasks. The Perceptron becomes especially effective when it is used in conjunction with kernel functions. However, a common difficulty encountered when implementing kernel-based online algorithms is the amount of memory required to store the online hypothesis, which may grow unboundedly as the algorithm progresses. Moreover, the running time of each online round grows linearly with the amount of memory used to store the hypothesis. In this paper, we present the Forgetron family of kernel-based online classification algorithms, which overcome this problem by restricting themselves to a predefined memory budget. We obtain different members of this family by modifying the kernel-based Perceptron in various ways. We also prove a unified mistake bound for all of the Forgetron algorithms. To our knowledge, this is the first online kernel-based learning paradigm which, on one hand, maintains a strict limit on the amount of memory it uses and, on the other hand, entertains a relative mistake bound. We conclude with experiments using real datasets, which underscore the merits of our approach.},
address = {Cambridge, MA},
author = {Dekel, Ofer and Shwartz, Shai S and Singer, Yoram and Shalev-Shwartz, Shai},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Dekel et al. - 2008 - The Forgetron A Kernel-Based Perceptron on a Budget.pdf:pdf},
journal = {SIAM J. Comput.},
keywords = {kernel-trick,perceptron-learning-rule,phd},
number = {5},
pages = {1342--1372},
publisher = {MIT Press},
shorttitle = {The Forgetron},
title = {{The Forgetron: A Kernel-Based Perceptron on a Budget}},
volume = {37},
year = {2008}
}
@article{Fink2007,
abstract = {All molecules of up to 11 atoms of C, N, O, and F possible under consideration of simple valency, chemical stability, and synthetic feasibility rules were generated and collected in a database (GDB). GDB contains 26.4 million molecules (110.9 million stereoisomers), including three- and four-membered rings and triple bonds. By comparison, only 63 857 compounds of up to 11 atoms were found in public databases (a combination of PubChem, ChemACX, ChemSCX, NCI open database, and the Merck Index). A total of 538 of the 1208 ring systems in GDB are currently unknown in the CAS Registry and Beilstein databases in any carbon/heteroatom/multiple-bond combination or as a substructure. Over 70{\%} of GDB molecules are chiral. Because of their small size, all compounds obey Lipinski's bioavailability rule. A total of 13.2 million compounds also follow Congreve's "Rule of 3" for lead-likeness. A Kohonen map trained with autocorrelation descriptors organizes GDB according to compound classes and shows that leadlike compounds are most abundant in chiral regions of fused carbocycles and fused heterocycles. The projection of known compounds into this map indicates large uncharted areas of chemical space. The potential of GDB for drug discovery is illustrated by virtual screening for kinase inhibitors, G-protein coupled receptor ligands, and ion-channel modulators. The database is available from the author's Web page.},
author = {Fink, Tobias and Reymond, Jean-Louis},
doi = {10.1021/ci600423u},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Fink, Reymond - 2007 - Virtual exploration of the chemical universe up to 11 atoms of C, N, O, F assembly of 26.4 million structures (11.pdf:pdf},
issn = {1549-9596},
journal = {Journal of chemical information and modeling},
keywords = {Chemistry, Physical,Computational Biology,Cyclization,Databases, Factual,Drug Design,Drug Evaluation, Preclinical,Models, Molecular,Molecular Structure,Pharmaceutical Preparations,Pharmaceutical Preparations: analysis,Pharmaceutical Preparations: chemistry,Pharmaceutical Preparations: classification,Physicochemical Phenomena,Stereoisomerism},
number = {2},
pages = {342--53},
pmid = {17260980},
title = {{Virtual exploration of the chemical universe up to 11 atoms of C, N, O, F: assembly of 26.4 million structures (110.9 million stereoisomers) and analysis for new ring systems, stereochemistry, physicochemical properties, compound classes, and drug discove}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17260980},
volume = {47},
year = {2007}
}
@inproceedings{Kingsbury2002,
abstract = {This paper describes our approach to the development of a Proposition
Bank, which involves the addition of semantic information to the
Penn English Treebank. Our primary goal is the labeling of syntactic
nodes with specific argument labels that preserve the similarity
of roles such as the window in John broke the window and the window
broke. After motivating the need for explicit predicate argument
structure labels, we briefly discuss the theoretical considerations
of predicate argument...},
author = {Kingsbury, P and Palmer, M},
booktitle = {Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Spain},
title = {{From Treebank to PropBank}},
year = {2002}
}
@inproceedings{Burges1999,
author = {Burges, Christopher J C and Crisp, David J},
booktitle = {Advances in Neural Information Processing Systems 12},
editor = {Solla, Sara A and Leen, Todd K and M{\"{u}}ller, Klaus-Robert},
isbn = {0-262-19450-3},
pages = {223--229},
publisher = {The MIT Press},
title = {{Uniqueness of the SVM Solution}},
year = {1999}
}
@article{Keselman2003,
address = {Los Alamitos, CA, USA},
author = {Keselman, Yakov and Shokoufandeh, Ali and Demirci, M Fatih and Dickinson, Sven},
doi = {http://doi.ieeecomputersociety.org/10.1109/CVPR.2003.1211441},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition, IEEE Computer Society Conference on},
pages = {850},
publisher = {IEEE Computer Society},
title = {{Many-to-Many Graph Matching via Metric Embedding}},
volume = {1},
year = {2003}
}
@inproceedings{Aggarwal,
author = {Aggarwal, Charu C},
booktitle = {SDM},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal - 2011 - On Classification of Graph Streams.pdf:pdf},
keywords = {classification,graphs},
title = {{On Classification of Graph Streams}},
year = {2011}
}
@inproceedings{Simoes2013,
annote = {@inproceedings{\{}DBLP:dblp{\_}journals/corr/abs-1302-4874,
   author              = {\{}Gon{\c{c}}alo Sim{\~{o}}es and 
                          Helena Galhardas and 
                          David Martins de Matos{\}},
   title               = {\{}A Labeled Graph Kernel for Relationship Extraction{\}},
   journal             = {\{}CoRR{\}},
   year                = {\{}2013{\}},
   ee                  = {\{}http://arxiv.org/abs/1302.4874{\}},
{\}}

        

        

        

      },
archivePrefix = {arXiv},
arxivId = {arXiv:1302.4874v1},
author = {Sim{\~{o}}es, G and Galhardas, Helena and Matos, David},
booktitle = {CoRR},
eprint = {arXiv:1302.4874v1},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sim{\~{o}}es, Galhardas, Matos - 2013 - A Labeled Graph Kernel for Relationship Extraction.pdf:pdf},
keywords = {graph kernels,information extraction,machine learning},
title = {{A Labeled Graph Kernel for Relationship Extraction}},
url = {http://arxiv.org/abs/1302.4874},
year = {2013}
}
@article{Crammer2006,
author = {Crammer, K and Dekel, O},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Crammer, Dekel - 2006 - Online passive-aggressive algorithms.pdf:pdf},
journal = {The Journal of Machine Learning Research},
pages = {551--585},
title = {{Online passive-aggressive algorithms}},
url = {http://dl.acm.org/citation.cfm?id=1248566},
volume = {7},
year = {2006}
}
@article{Childs2009,
abstract = {The study of non-coding RNA genes has received increased attention in recent years fuelled by accumulating evidence that larger portions of genomes than previously acknowledged are transcribed into RNA molecules of mostly unknown function, as well as the discovery of novel non-coding RNA types and functional RNA elements. Here, we demonstrate that specific properties of graphs that represent the predicted RNA secondary structure reflect functional information. We introduce a computational algorithm and an associated web-based tool (GraPPLE) for classifying non-coding RNA molecules as functional and, furthermore, into Rfam families based on their graph properties. Unlike sequence-similarity-based methods and covariance models, GraPPLE is demonstrated to be more robust with regard to increasing sequence divergence, and when combined with existing methods, leads to a significant improvement of prediction accuracy. Furthermore, graph properties identified as most informative are shown to provide an understanding as to what particular structural features render RNA molecules functional. Thus, GraPPLE may offer a valuable computational filtering tool to identify potentially interesting RNA molecules among large candidate datasets.},
author = {Childs, Liam and Nikoloski, Zoran and May, Patrick and Walther, Dirk},
doi = {10.1093/nar/gkp206},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Childs et al. - 2009 - Identification and classification of ncRNA molecules using graph properties.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {Algorithms,Computer Graphics,Nucleic Acid Conformation,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: classification,Sequence Alignment,Sequence Analysis, RNA,Software},
month = {may},
number = {9},
pages = {e66},
pmid = {19339518},
title = {{Identification and classification of ncRNA molecules using graph properties.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2685108{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {37},
year = {2009}
}
@article{Florencio2007,
annote = {Examples of 3 graphs that have the same random walks set},
author = {Florencio, C Costa},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Florencio - 2007 - Unsupervised learning of graph languages using kernels New results.pdf:pdf},
journal = {LNVD 2007 Learning from Non-Vectorial Data},
pages = {1--4},
title = {{Unsupervised learning of graph languages using kernels: New results}},
url = {https://lirias.kuleuven.be/handle/123456789/184890},
volume = {6},
year = {2007}
}
@article{Cho,
author = {Cho, Youngmin and Saul, Lawrence K and Diego, San and Jolla, La},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cho et al. - Unknown - Kernel Methods for Deep Learning.pdf:pdf},
pages = {1--9},
title = {{Kernel Methods for Deep Learning}}
}
@inproceedings{DaSanMartino2015,
author = {{Da San Martino}, Giovanni and Navarin, Nicolo' and Sperduti, Alessandro},
booktitle = {proceedings of the 23th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
keywords = {navarin2015},
title = {{Exploiting the ODD framework to define a novel effective graph kernel.}},
year = {2015}
}
@inproceedings{DBLP:conf/ausai/2008,
booktitle = {Australasian Conference on Artificial Intelligence},
editor = {Wobcke, Wayne and Zhang, Mengjie},
isbn = {978-3-540-89377-6},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{AI 2008: Advances in Artificial Intelligence, 21st Australasian Joint Conference on Artificial Intelligence, Auckland, New Zealand, December 1-5, 2008. Proceedings}},
volume = {5360},
year = {2008}
}
@article{Di2009,
author = {Di, Leontina and Melissari, Erika and Mariotti, Veronica and Iofrida, Caterina and Galli, Alvaro and Guidugli, Lucia and Lombardi, Grazia and Adelaide, Maria and Iacopetti, Paola and Pellegrini, Silvia},
doi = {10.1016/j.ejca.2009.04.025},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Di et al. - 2009 - Characterisation of gene expression profiles of yeast cells expressing BRCA1 missense variants.pdf:pdf},
issn = {0959-8049},
journal = {European Journal of Cancer},
number = {12},
pages = {2187--2196},
publisher = {Elsevier Ltd},
title = {{Characterisation of gene expression profiles of yeast cells expressing BRCA1 missense variants}},
url = {http://dx.doi.org/10.1016/j.ejca.2009.04.025},
volume = {45},
year = {2009}
}
@inproceedings{Shervashidze2009,
abstract = {State-of-the-art graph kernels do not scale to large graphs with hundreds of nodes and thousands of edges. In this article we propose to compare graphs by counting common {\{}$\backslash$it graphlets{\}}, $\backslash$ie subgraphs with k nodes where k $\backslash$in {\{} 3, 4, 5 {\}}. Exhaustive enumeration of all graphlets being prohibitively expensive, we introduce two theoretically grounded speedup schemes, one based on sampling and the second one specifically designed for bounded degree graphs. In our experimental evaluation, our novel kernels allow us to efficiently compare large graphs that cannot be tackled by existing graph kernels.},
address = {Clearwater Beach, Florida, USA},
annote = {From Duplicate 1 (Efficient graphlet kernels for large graph comparison - Shervashidze, Nino; Vishwanathan, S V N; Petri, Tobias H; Mehlhorn, Kurt; Borgwardt, Karsten M)

From Duplicate 1 ( Efficient graphlet kernels for large graph comparison - Shervashidze, Nino; Mehlhorn, Kurt; Petri, Tobias H; Vishwanathan, S V N; Petri, Tobias H; Mehlhorn, Kurt; Borgwardt, Karsten )
},
author = {Shervashidze, Nino and Mehlhorn, Kurt and Petri, Tobias H and Vishwanathan, S V N and Borgwardt, Karsten M and Petri, Tobias H and Mehlhorn, Kurt and Borgwardt, Karsten M},
booktitle = {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics (AISTATS)},
editor = {van Dyk, David and Welling, Max},
isbn = {1938-7228},
keywords = {Learning/Statistics {\&} Optimisation},
organization = {PASCAL EPrints [http://eprints.pascal-network.org/perl/oai2] (United Kingdom)},
pages = {488--495},
publisher = {CSAIL},
series = {JMLR: Workshop and Conference Proceedings},
title = {{Efficient graphlet kernels for large graph comparison}},
volume = {5},
year = {2009}
}
@inproceedings{Dasan2012,
author = {{Da San Martino}, Giovanni and Navarin, Nicol{\`{o}} and Sperduti, Alessandro},
booktitle = {Proceedings of the Twelfth SIAM International Conference on Data Mining},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Da San Martino, Navarin, Sperduti - 2012 - A Tree-Based Kernel for Graphs.pdf:pdf},
pages = {975--986},
title = {{A Tree-Based Kernel for Graphs}},
year = {2012}
}
@inproceedings{MacDonald2000,
abstract = {We review a recently-developed method of performing k-means clustering
in a high-dimensional feature space and extend it to give the resultant
mapping topology-preserving properties. We show the results of the
new algorithm on the standard data set, on random numbers drawn uniformly
from [0,1)2 and on the Olivetti database of faces. The new algorithm
converges extremely quickly},
author = {MacDonald, D and Fyfe, C},
booktitle = {{\{}Knowledge-Based{\}} Intelligent Engineering Systems and Allied Technologies, 2000. Proceedings. Fourth International Conference on},
doi = {10.1109/KES.2000.885820},
pages = {317--320},
title = {{The kernel self-organising map}},
volume = {1},
year = {2000}
}
@article{Pan2015,
abstract = {Classification on structure data, such as graphs, has drawn wide interest in recent years. Due to the lack of explicit features to represent graphs for training classification models, extensive studies have been focused on extracting the most discriminative subgraphs features from the training graph dataset to transfer graphs into vector data. However, such filter-based methods suffer from two major disadvantages: (1) the subgraph feature selection is separated from the model learning process, so the selected most discriminative subgraphs may not best fit the subsequent learning model, resulting in deteriorated classification results; (2) all these methods rely on users to specify the number of subgraph features K, and suboptimally specified K values often result in significantly reduced classification accuracy. In this paper, we propose a new graph classification paradigm which overcomes the above disadvantages by formulating subgraph feature selection as learning a K-dimensional feature space from an implicit and large subgraph space, with the optimal K value being automatically determined. To achieve the goal, we propose a regularized loss minimization-driven (RLMD) feature selection method for graph classification. RLMD integrates subgraph selection and model learning into a unified framework to find discriminative subgraphs with guaranteed minimum loss w.r.t. the objective function. To automatically determine the optimal number of subgraphs K from the exponentially large subgraph space, an effective elastic net and a subgradient method are proposed to derive the stopping criterion, so that K can be automatically obtained once RLMD converges. The proposed RLMD method enjoys gratifying property including proved convergence and applicability to various loss functions. Experimental results on real-life graph datasets demonstrate significant performance gain.},
author = {Pan, Shirui and Wu, Jia and Zhu, Xingquan and Long, Guodong and Zhang, Chengqi},
doi = {10.1016/j.patcog.2015.05.019},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Pan et al. - 2015 - Finding the best not the most Regularized loss minimization subgraph selection for graph classification.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Classification,Feature selection,Graph classification,Sparse learning},
number = {11},
pages = {3783--3796},
title = {{Finding the best not the most: Regularized loss minimization subgraph selection for graph classification}},
volume = {48},
year = {2015}
}
@article{Cormode,
author = {Cormode, Graham and Laboratories, Bell and Muthukrishnan, S},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cormode, Laboratories, Muthukrishnan - Unknown - Space Efficient Mining of Multigraph Streams.pdf:pdf},
isbn = {1595930620},
title = {{Space Efficient Mining of Multigraph Streams}}
}
@inproceedings{DaSanMartino2012,
author = {{Da San Martino}, Giovanni and Navarin, Nicolo and Sperduti, Alessandro},
booktitle = {the 2012 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2012.6252831},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Da San Martino, Navarin, Sperduti - 2012 - A memory efficient graph kernel.pdf:pdf},
isbn = {978-1-4673-1490-9},
month = {jun},
publisher = {IEEE},
title = {{A memory efficient graph kernel}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6252831},
year = {2012}
}
@article{Geard2004,
author = {Geard, Nicholas},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Geard - 2004 - Modelling Gene Regulatory Networks Systems Biology to Complex Systems ACCS Draft Technical Report.pdf:pdf},
pages = {1--48},
title = {{Modelling Gene Regulatory Networks: Systems Biology to Complex Systems ACCS Draft Technical Report}},
year = {2004}
}
@article{Langford2009,
author = {Langford, John and Li, Lihong and Zhang, Tong},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Langford, Li, Zhang - 2009 - Sparse Online Learning via Truncated Gradient.pdf:pdf},
keywords = {online learning,regulariza-,sparsity,stochastic gradient descent,truncated gradient},
pages = {777--801},
title = {{Sparse Online Learning via Truncated Gradient}},
volume = {10},
year = {2009}
}
@article{Missura,
author = {Missura, Olana and Thomas, G and Augustin, Sankt},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Missura, Thomas, Augustin - Unknown - Predicting Dynamic Difficulty.pdf:pdf},
pages = {1--9},
title = {{Predicting Dynamic Difficulty}}
}
@inproceedings{DBLP:conf/icann/2011-1,
booktitle = {ICANN (1)},
editor = {Honkela, Timo and Duch, Wlodzislaw and Girolami, Mark A and Kaski, Samuel},
isbn = {978-3-642-21734-0},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Artificial Neural Networks and Machine Learning - ICANN 2011 - 21st International Conference on Artificial Neural Networks, Espoo, Finland, June 14-17, 2011, Proceedings, Part I}},
volume = {6791},
year = {2011}
}
@article{Bernhart2008,
abstract = {BACKGROUND: The prediction of a consensus structure for a set of related RNAs is an important first step for subsequent analyses. RNAalifold, which computes the minimum energy structure that is simultaneously formed by a set of aligned sequences, is one of the oldest and most widely used tools for this task. In recent years, several alternative approaches have been advocated, pointing to several shortcomings of the original RNAalifold approach.

RESULTS: We show that the accuracy of RNAalifold predictions can be improved substantially by introducing a different, more rational handling of alignment gaps, and by replacing the rather simplistic model of covariance scoring with more sophisticated RIBOSUM-like scoring matrices. These improvements are achieved without compromising the computational efficiency of the algorithm. We show here that the new version of RNAalifold not only outperforms the old one, but also several other tools recently developed, on different datasets.

CONCLUSION: The new version of RNAalifold not only can replace the old one for almost any application but it is also competitive with other approaches including those based on SCFGs, maximum expected accuracy, or hierarchical nearest neighbor classifiers.},
author = {Bernhart, Stephan H and Hofacker, Ivo L and Will, Sebastian and Gruber, Andreas R and Stadler, Peter F},
doi = {10.1186/1471-2105-9-474},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bernhart et al. - 2008 - RNAalifold improved consensus structure prediction for RNA alignments.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Amino Acid Sequence,Computational Biology,Computational Biology: methods,Models, Chemical,Nucleic Acid Conformation,RNA,RNA: chemistry,Sequence Alignment,Software},
month = {jan},
pages = {474},
pmid = {19014431},
title = {{RNAalifold: improved consensus structure prediction for RNA alignments.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2621365{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Achananuparp,
author = {Achananuparp, Palakorn and Hu, Xiaohua and He, Tingting and Yang, Christopher C and An, Yuan},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Achananuparp et al. - Unknown - Answer Diversification for Complex Question Answering on the Web.pdf:pdf},
keywords = {answer diversification,answer reranking,complex question answering,edge graph,negative-,random walk},
title = {{Answer Diversification for Complex Question Answering on the Web}}
}
@incollection{Zoppis2007,
abstract = {The biological interpretation of large-scale gene expression data
is one of the challenges in current bioinformatics. The state-of-the-art
approach is to perform clustering and then compute a functional characterization
via enrichments by Gene Ontology terms [1]. To better assist the
interpretation of results, it may be useful to establish connections
among different clusters. This machine learning step is sometimes
termed cluster meta-analysis, and several approaches have already
been proposed; in particular, they usually rely on enrichments based
on flat lists of {\{}GO{\}} terms. However, {\{}GO{\}} terms are organized in
taxonomical graphs, whose structure should be taken into account
when performing enrichment studies. To tackle this problem, we propose
a kernel approach that can exploit such structured graphical nature.
Finally, we compare our approach against a specific flat list method
by analyzing the cdc15-subset of the well known Spellman???s Yeast
Cell Cycle dataset [2].},
author = {Zoppis, Italo and Merico, Daniele and Antoniotti, Marco and Mishra, Bud and Mauri, Giancarlo},
booktitle = {Bioinformatics Research and Applications},
pages = {158--169},
title = {{Discovering Relations Among {\{}GO-Annotated{\}} Clusters by Graph Kernel Methods}},
year = {2007}
}
@article{Jr1999,
abstract = {We describe the RNA folding problem and contrast it with the much more difficult protein folding problem. RNA has four similar monomer units, whereas proteins have 20 very different residues. The folding of RNA is hierarchical in that secondary structure is much more stable than tertiary folding. In RNA the two levels of folding (secondary and tertiary) can be experimentally separated by the presence or absence of Mg2+. Secondary structure can be predicted successfully from experimental thermodynamic data on secondary structure elements: helices, loops, and bulges. Tertiary interactions can then be added without much distortion of the secondary structure. These observations suggest a folding algorithm to predict the structure of an RNA from its sequence. However, to solve the RNA folding problem one needs thermodynamic data on tertiary structure interactions, and identification and characterization of metal-ion binding sites. These data, together with force versus extension measurements on single RNA molecules, should provide the information necessary to test and refine the proposed algorithm.},
author = {Tinoco, Ignacio and Bustamante, Carlos},
doi = {10.1006/jmbi.1999.3001},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Tinoco, Bustamante - 1999 - How RNA folds.pdf:pdf;:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Tinoco, Bustamante - 1999 - How RNA folds(2).pdf:pdf},
issn = {0022-2836},
journal = {Journal of molecular biology},
keywords = {Algorithms,Base Sequence,Metals,Metals: pharmacology,Nucleic Acid Conformation,Nucleic Acid Conformation: drug effects,Protein Folding,RNA,RNA: chemistry,RNA: genetics,RNA: metabolism,Thermodynamics},
month = {oct},
number = {2},
pages = {271--81},
pmid = {10550208},
title = {{How RNA folds.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10550208},
volume = {293},
year = {1999}
}
@article{Schietgat2010a,
author = {Schietgat, Leander and Costa, Fabrizio and Ramon, Jan and Raedt, Luc},
doi = {10.1007/s10994-010-5193-8},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Schietgat et al. - 2010 - Effective feature construction by maximum common subgraph sampling.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jul},
number = {2},
pages = {137--161},
title = {{Effective feature construction by maximum common subgraph sampling}},
url = {http://www.springerlink.com/index/10.1007/s10994-010-5193-8},
volume = {83},
year = {2010}
}
@article{Nguyen2006,
abstract = {The high generalization ability of support vector machines (SVMs)
has been shown in many practical applications, however, they are
considerably slower in test phase than other learning approaches
due to the possibly big number of support vectors comprised in their
solution. In this letter, we describe a method to reduce such number
of support vectors. The reduction process iteratively selects two
nearest support vectors belonging to the same class and replaces
them by a newly constructed one. Through the analysis of relation
between vectors in input and feature spaces, we present the construction
of the new vectors that requires to find the unique maximum point
of a one-variable function on (0,1), not to minimize a function of
many variables with local minima in previous reduced set methods.
Experimental results on real life dataset show that the proposed
method is effective in reducing number of support vectors and preserving
machine's generalization performance.},
author = {Nguyen, Ducdung and Ho, Tubao},
doi = {10.1109/TNN.2006.873287},
issn = {1045-9227},
journal = {IEEE transactions on neural networks},
number = {3},
pages = {792--796},
title = {{A bottom-up method for simplifying support vector solutions}},
volume = {17},
year = {2006}
}
@article{Chang2010,
author = {Chang, YW and Hsieh, CJ},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chang, Hsieh - 2010 - Training and testing low-degree polynomial data mappings via linear SVM.pdf:pdf},
journal = {The Journal of Machine Learning Research},
keywords = {decomposition methods,dependency parsing,kernel functions,low-degree polynomial mapping,natural language processing,support vector machines},
number = {11},
pages = {1471--1490},
title = {{Training and testing low-degree polynomial data mappings via linear SVM}},
url = {http://dl.acm.org/citation.cfm?id=1859899},
year = {2010}
}
@article{Star,
author = {Star, A},
doi = {10.1145/2806416.2806548},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Star - Unknown - A Min-Max Optimization Framework for Online Graph Classification.pdf:pdf},
isbn = {9781450337946},
keywords = {graph,min-max optimization,online learning,sampling},
pages = {643--652},
title = {{A Min-Max Optimization Framework for Online Graph Classification}}
}
@article{Tsymbal2004,
author = {Tsymbal, Alexey},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Tsymbal - 2004 - The problem of concept drift definitions and related work.pdf:pdf},
journal = {Processing},
title = {{The problem of concept drift : definitions and related work}},
year = {2004}
}
@article{Kuznetsov2005,
author = {Kuznetsov, Sergei O and Samokhin, Mikhail V},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kuznetsov, Samokhin - 2005 - Learning Closed Sets of Labeled Graphs for Chemical Applications.pdf:pdf},
pages = {190--208},
title = {{Learning Closed Sets of Labeled Graphs for Chemical Applications}},
year = {2005}
}
@article{Scholkopf1999,
author = {Sch{\"{o}}lkopf, Bernhard and Mika, Sebastian and Burges, Christopher J C and Knirsch, Phil and M{\"{u}}ller, Klaus R and R{\"{a}}tsch, Gunnar and Smola, Alexander J},
journal = {IEEE Transactions on Neural Networks},
number = {5},
pages = {1000--1017},
title = {{Input space versus feature space in kernel-based methods}},
volume = {10},
year = {1999}
}
@article{Dali2009,
author = {Dali, Lorand and Rusu, Delia and Grobelnik, Marko},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Dali, Rusu, Grobelnik - 2009 - Question Answering Based on Semantic Graphs.pdf:pdf},
number = {April},
title = {{Question Answering Based on Semantic Graphs}},
year = {2009}
}
@misc{_gartner_????,
title = {gartner - kernel for structured data.pdf}
}
@article{Diligenti2003,
abstract = {{\{}Classification{\}} is an important problem in image document processing
and is often a preliminary step toward recognition, understanding,
and information extraction. In this paper, the problem is formulated
in the framework of concept learning and each category corresponds
to the set of image documents with similar physical structure. We
propose a solution based on two algorithmic ideas. First, we obtain
a structured representation of images based on labeled {\{}XY-trees{\}}
(this representation informs the learner about important relationships
between image subconstituents). Second, we propose a probabilistic
architecture that extends hidden Markov models for learning probability
distributions defined on spaces of labeled trees. Finally, a successful
application of this method to the categorization of commercial invoices
is presented.},
author = {Diligenti, Michelangelo and Frasconi, Paolo and Gori, Marco},
doi = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2003.1190578},
issn = {0162-8828},
journal = {{\{}IEEE{\}} Transactions on Pattern Analysis and Machine Intelligence},
keywords = {document{\_}classification,image,image{\_}classification,machine{\_}learning,markovian models,structured information.,tree},
number = {4},
pages = {519--523},
title = {{Hidden Tree Markov Models for Document Image Classification}},
volume = {25},
year = {2003}
}
@inproceedings{Bloehdorn2007,
abstract = {Several Text Categorization applications require a representation
beyond the standard bag-of-words paradigm. Kernel-based learning
has approached this problem by (i) considering information about
syntactic structure or by (ii) incorporating knowledge about the
semantic similarity of term features. We propose a generalized framework
consisting of a family of kernels that jointly incorporate syntactic
and semantic similarity and demonstrate the power of this approach
in a series of experiments.},
address = {Lisbon, Portugal},
author = {Bloehdorn, Stephan and Moschitti, Alessandro},
booktitle = {Proceedings of the sixteenth {\{}ACM{\}} conference on Conference on information and knowledge management},
doi = {10.1145/1321440.1321561},
isbn = {978-1-59593-803-9},
pages = {861--864},
publisher = {ACM},
title = {{Structure and semantics for expressive text kernels}},
year = {2007}
}
@inproceedings{Bottou2008,
author = {Bottou, L and Bousquet, Olivier},
booktitle = {Advances in Neural Information Processing Systems},
editor = {{Platt, J.C. and Koller, D. and Singer, Y. and Roweis}, S.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bottou, Bousquet - 2008 - The Tradeoffs of Large-Scale Learning.pdf:pdf},
pages = {161----168},
publisher = {NIPS Foundation (http://books.nips.cc)},
title = {{The Tradeoffs of Large-Scale Learning}},
year = {2008}
}
@article{Revesz2010,
abstract = {Objective We propose classification integration as a new method for
data integration from different sources. We also propose reclassification
as a new method of combining existing medical classifications for
different classes.Background In many problems the raw data are already
classified according to a set of features but need to be reclassified.
Data reclassification is usually achieved using data integration
methods that require the raw data, which may not be available or
sharable because of privacy and legal concerns.Methodology We introduce
general classification integration and reclassification methods that
create new classes by combining in a flexible way the existing classes
without requiring access to the raw data. The flexibility is achieved
by representing any linear classification in a constraint database.Results
The experiments using support vector machines and decision trees
on heart disease diagnosis and primary biliary cirrhosis data show
that our classification integration method is more accurate than
current data integration methods when there are many missing values
in the data. The reclassification problem also can be solved using
constraint databases without requiring access to the raw data.Conclusions
The classification integration and the reclassification methods are
applied to two particular data sets. Beside these particular cases,
our general method is also appropriate for many other application
areas and may yield similar accuracy improvements. These methods
may be also extended to non-linear classifiers.},
author = {Revesz, Peter and Triplet, Thomas},
doi = {DOI: 10.1016/j.artmed.2010.02.003},
issn = {0933-3657},
journal = {Artificial Intelligence in Medicine},
number = {2},
pages = {79--91},
title = {{Classification integration and reclassification using constraint databases}},
volume = {49},
year = {2010}
}
@inproceedings{Shin2008,
abstract = {Haussler's convolution kernel provides a successful framework for
engineering new positive semidefinite kernels, and has been applied
to a wide range of data types and applications. In the framework,
each data object represents a finite set of finer grained components.
Then, Haussler's convolution kernel takes a pair of data objects
as input, and returns the sum of the return values of the predetermined
primitive positive semidefinite kernel calculated for all the possible
pairs of the components of the input data objects. On the other hand,
the mapping kernel that we introduce in this paper is a natural generalization
of Haussler's convolution kernel, in that the input to the primitive
kernel moves over a predetermined subset rather than the entire cross
product. Although we have plural instances of the mapping kernel
in the literature, their positive semidefiniteness was investigated
in case-by-case manners, and worse yet, was sometimes incorrectly
concluded. In fact, there exists a simple and easily checkable necessary
and sufficient condition, which is generic in the sense that it enables
us to investigate the positive semidefiniteness of an arbitrary instance
of the mapping kernel. This is the first paper that presents and
proves the validity of the condition. In addition, we introduce two
important instances of the mapping kernel, which we refer to as the
size-of-index-structure-distribution kernel and the editcost-distribution
kernel. Both of them are naturally derived from well known (dis)similarity
measurements in the literature (e.g. the maximum agreement tree,
the edit distance), and are reasonably expected to improve the performance
of the existing measures by evaluating their distributional features
rather than their peak (maximum/minimum) features.},
address = {New York, NY, USA},
annote = {        From Duplicate 2 (                   A generalization of Haussler's convolution kernel: mapping kernel                 - Shin, Kilho; Kuboyama, Tetsuji )
                
        
        
      },
author = {Shin, Kilho and Kuboyama, Tetsuji},
booktitle = {ICML '08: Proceedings of the 25th international conference on Machine learning},
doi = {10.1145/1390156.1390275},
isbn = {978-1-60558-205-4},
pages = {944--951},
publisher = {ACM},
shorttitle = {A generalization of Haussler's convolution kernel},
title = {{A generalization of Haussler's convolution kernel: mapping kernel}},
url = {http://portal.acm.org/ft{\_}gateway.cfm?id=1390275{\&}type=pdf{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=95568645{\&}CFTOKEN=19199977},
year = {2008}
}
@article{Chevalier2001,
author = {Chevalier, Y and Compagna, L and Cuellar, J and Drielsma, P Hankes and Mantovani, J and Vigneron, L},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chevalier et al. - 2001 - A High-Level Protocol Specification Language for Industrial Security-Sensitive Protocols ∗.pdf:pdf},
journal = {Artificial Intelligence},
title = {{A High-Level Protocol Specification Language for Industrial Security-Sensitive Protocols ∗}},
year = {2001}
}
@inproceedings{Gao07a,
address = {Omaha, NE},
author = {Gao, J and Fan, W and Han, J and Yu, P S},
booktitle = {Proc. of the 7th IEEE Internl. Conf. on Data Mining (ICDM'07)},
pages = {143--152},
title = {{On Appropriate Assumptions to Mine Data Streams: Analysis and Practice}},
year = {2007}
}
@article{discretization,
author = {Bondu, A and Boulle, M and Lemaire, V},
journal = {Knowl. and Inform. Sys.},
number = {1},
pages = {35--57},
title = {{A non-parametric semi-supervised discretization method}},
volume = {24},
year = {2010}
}
@article{JGT:JGT3190010410,
author = {Read, Ronald C and Corneil, Derek G},
doi = {10.1002/jgt.3190010410},
issn = {1097-0118},
journal = {Journal of Graph Theory},
number = {4},
pages = {339--363},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{The graph isomorphism disease}},
url = {http://dx.doi.org/10.1002/jgt.3190010410},
volume = {1},
year = {1977}
}
@article{Gupta2008,
abstract = {Static expression experiments analyze samples from many individuals.
These samples are often snapshots of the progression of a certain
disease such as cancer. This raises an intriguing question: Can we
determine a temporal order for these samples? Such an ordering can
lead to better understanding of the dynamics of the disease and to
the identification of genes associated with its progression. In this
paper we formally prove, for the first time, that under a model for
the dynamics of the expression levels of a single gene, it is indeed
possible to recover the correct ordering of the static expression
datasets by solving an instance of the traveling salesman problem
{\{}(TSP).{\}} In addition, we devise an algorithm that combines a {\{}TSP{\}}
heuristic and probabilistic modeling for inferring the underlying
temporal order of the microarray experiments. This algorithm constructs
probabilistic continuous curves to represent expression profiles
leading to accurate temporal reconstruction for human data. Applying
our method to cancer expression data we show that the ordering derived
agrees well with survival duration. A classifier that utilizes this
ordering improves upon other classifiers suggested for this task.
The set of genes displaying consistent behavior for the determined
ordering are enriched for genes associated with cancer progression.},
author = {Gupta, Anupam and Bar-Joseph, Ziv},
journal = {{\{}IEEE/ACM{\}} Trans. Comput. Biol. Bioinformatics},
number = {2},
pages = {172--182},
title = {{Extracting Dynamics from Static Cancer Expression Data}},
volume = {5},
year = {2008}
}
@inproceedings{He2003,
abstract = {Many problems in information processing involve some form of dimensionality
reduction. In this paper, we introduce Locality Preserving Projections
{\{}(LPP).{\}} These are linear projective maps that arise by solving a
variational problem that optimally preserves the neighborhood structure
of the data set. {\{}LPP{\}} should be seen as an alternative to Principal
Component Analysis {\{}(PCA){\}} ??? a classical linear technique that
projects the data along the directions of maximal variance. When
the high dimensional data lies on a low dimensional manifold embedded
in the ambient space, the Locality Preserving Projections are obtained
by finding the optimal linear approximations to the eigenfunctions
of the Laplace Beltrami operator on the manifold. As a result, {\{}LPP{\}}
shares many of the data representation properties of nonlinear techniques
such as Laplacian Eigenmaps or Locally Linear Embedding. Yet {\{}LPP{\}}
is linear and more crucially is defined everywhere in ambient space
rather than just on the training data points. This is borne out by
illustrative examples on some high dimensional data sets.},
author = {He, Xiaofei and Niyogi, Partha},
booktitle = {Advances in Neural Information Processing Systems 16},
howpublished = {http://books.nips.cc/papers/files/nips16/NIPS2003{\_}AA20.pdf},
title = {{Locality Preserving Projections}},
year = {2003}
}
@phdthesis{Kashima2007,
author = {Kashima, Hisashi},
school = {Graduate School of Informatics, Kyoto University, Japan},
title = {{Machine Learning Approaches for Structured Data}},
year = {2007}
}
@article{Martin2002,
address = {Albuquerque, NM, and Livermore, CA},
author = {Martin, Shawn B.},
doi = {10.2172/810934},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Martin - 2002 - Kernel Near Principal Component Analysis.pdf:pdf},
institution = {Sandia National Laboratories (SNL)},
month = {jul},
pages = {2--7},
title = {{Kernel Near Principal Component Analysis}},
url = {http://www.osti.gov/servlets/purl/810934-cHjE05/native/},
year = {2002}
}
@article{Chen2014a,
author = {Chen, BoLin and Li, Min and Wang, JianXin and Wu, Fang-Xiang},
doi = {10.1007/s11427-014-4745-8},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Disease gene identification by using graph kernels and Markov random fields(2).pdf:pdf},
issn = {1674-7305},
journal = {Science China Life Sciences},
number = {11},
pages = {1054--1063},
title = {{Disease gene identification by using graph kernels and Markov random fields}},
url = {http://link.springer.com/10.1007/s11427-014-4745-8},
volume = {57},
year = {2014}
}
@inproceedings{lin2008,
address = {Shenyang, China},
author = {Lin, X and Zhang, Y},
booktitle = {Procedings of the 10th Asia Pacific Web Conf. (APWeb'08)},
pages = {10--25},
title = {{Aggregate Computation over Data Streams}},
year = {2008}
}
@article{Bordes2005,
author = {Bordes, Antoine},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bordes - 2005 - Fast Kernel Classifiers with Online and Active Learning.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1579--1619},
title = {{Fast Kernel Classifiers with Online and Active Learning}},
volume = {6},
year = {2005}
}
@article{Filice2013,
author = {Filice, Simone and Castellucci, Giuseppe and Croce, Danilo and Basili, Roberto},
doi = {10.1109/ICDMW.2013.87},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Filice et al. - 2013 - Robust Language Learning via Efficient Budgeted Online Algorithms.pdf:pdf},
isbn = {978-1-4799-3142-2},
journal = {2013 IEEE 13th International Conference on Data Mining Workshops},
month = {dec},
pages = {913--920},
publisher = {Ieee},
title = {{Robust Language Learning via Efficient Budgeted Online Algorithms}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6754019},
year = {2013}
}
@inproceedings{Menchetti2003,
author = {Menchetti, Sauro and Costa, Fabrizio and Frasconi, Paolo and Pontil, Massimiliano},
booktitle = {Proceedings of IAPR--TC3 International Workshop on Artificial Neural Networks in Pattern Recognition (ANNPR 2003)},
title = {{Comparing Convolution Kernels and Recursive Neural Networks for learning preferences on structured data}},
year = {2003}
}
@article{Larranaga2006,
abstract = {This article reviews machine learning methods for bioinformatics.
It presents modelling methods, such as supervised classification,
clustering and probabilistic graphical models for knowledge discovery,
as well as deterministic and stochastic heuristics for optimization.
Applications in genomics, proteomics, systems biology, evolution
and text mining are also shown.},
author = {Larranaga, Pedro and Calvo, Borja and Santana, Roberto and Bielza, Concha and Galdiano, Josu and Inza, Inaki and Lozano, Jose A and Armananzas, Ruben and Santafe, Guzman and Perez, Aritz and Robles, Victor},
doi = {10.1093/bib/bbk007},
journal = {Brief Bioinform},
month = {mar},
number = {1},
pages = {86--112},
title = {{Machine learning in bioinformatics}},
volume = {7},
year = {2006}
}
@inproceedings{Folino2007,
address = {Valencia, Spain},
author = {Folino, G and Pizzuti, C and Spezzano, G},
booktitle = {Proc. of the 10th European Conf. Genetic Programming (EuroGP'07)},
pages = {160--169},
title = {{Mining Distributed Evolving Data Streams Using Fractal GP Ensembles}},
year = {2007}
}
@article{Carlini2006,
abstract = {The human SNP database was used to detect selection on 238 hexamers previously identified as exonic splicing enhancers (ESEs). We compared the distribution of the 238 putative ESEs in biallelic and triallelic SNPs within five different functional categories of the SNP database: synonymous, nonsynonymous, introns, UTRs, and nongenic SNPs. Since true ESEs do not function outside of exons, SNPs that disrupt ESE motifs were expected to be more common in nonexonic portions of the genome. Our results supported this expectation: ESEs were least prevalent within synonymous SNPs and most common in nongenic SNPs. There were approximately 11{\%} fewer ESEs within synonymous biallelic SNPs than expected under no selective constraint. We also compared the frequency of neutral SNPs, those where neither allele was an ESE, with deleterious SNPs, those where one or more alleles was an ESE, across the five different functional classes of SNPs. In comparison with the other functional classes of SNPs, synonymous SNPs contained an excess of neutral variants (+1.64{\%} and +6.04{\%} for biallelic and triallelic SNPs, respectively) and a dearth of deleterious variants (-13.11{\%} and -52.39{\%} for biallelic and triallelic SNPs, respectively). The observed patterns were consistent with purifying selection on the 238 hexamers to maintain their function as ESEs. However, in contrast to previous work, we did not find evidence for selection to maintain ESE function at nonsynonymous SNPs because selection at the protein level probably obscured any difference at the level of ESE function.},
author = {Carlini, David B and Genut, Jordan E},
doi = {10.1007/s00239-005-0055-x},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Carlini, Genut - 2006 - Synonymous SNPs provide evidence for selective constraint on human exonic splicing enhancers.pdf:pdf},
issn = {0022-2844},
journal = {Journal of molecular evolution},
keywords = {Alternative Splicing,Alternative Splicing: genetics,Alternative Splicing: physiology,Enhancer Elements, Genetic,Exons,Exons: genetics,Humans,Mutation,Polymorphism, Single Nucleotide,Polymorphism, Single Nucleotide: genetics},
month = {jan},
number = {1},
pages = {89--98},
pmid = {16320116},
title = {{Synonymous SNPs provide evidence for selective constraint on human exonic splicing enhancers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16320116},
volume = {62},
year = {2006}
}
@article{Lottaz2005,
abstract = {Motivation: Today, the characterization of clinical phenotypes by
gene-expression patterns is widely used in clinical research. If
the investigated phenotype is complex from the molecular point of
view, new challanges arise and these have not been adressed systematically.
For instance, the same clinical phenotype can be caused by various
molecular disorders, such that one observes different characteristic
expression patterns in different patients. Results: In this paper
we describe a novel algorithm called Structured Analysis of Microarrays
(StAM), which accounts for molecular heterogeneity of complex clinical
phenotypes. Our algorithm goes beyond established methodology in
several aspects: in addition to the expression data, it exploits
functional annotations from the Gene Ontology database to build biologically
focussed classifiers. These are used to uncover potential molecular
disease subentities and associate them to biological processes without
compromising overall prediction accuracy. Availability: Bioconductor
compliant R package Contact: Claudio.Lottaz@molgen.mpg.de Supplementary
information: Complete analyses are available at http://compdiag.molgen.mpg.de/supplements/lottaz05},
author = {Lottaz, Claudio and Spang, Rainer},
doi = {10.1093/bioinformatics/bti292},
journal = {Bioinformatics},
number = {9},
pages = {1971--1978},
title = {{Molecular decomposition of complex clinical phenotypes using biologically structured analysis of microarray data}},
volume = {21},
year = {2005}
}
@techreport{Zhu2005,
author = {Zhu, Xiaojin},
institution = {Computer Sciences, University of Wisconsin-Madison},
number = {1530},
title = {{Semi-Supervised Learning Literature Survey}},
year = {2005}
}
@phdthesis{Zhu2005a,
author = {Zhu, Xiaojin},
month = {may},
school = {Language Technologies Institute School of Computer Science Carnegie Mellon University},
title = {{Semi-Supervised Learning with Graphs}},
year = {2005}
}
@inproceedings{Mart'in-Merino2004,
author = {Mart{\'{i}}n-Merino, Manuel and Mu{\~{n}}oz, Alberto},
booktitle = {ICONIP},
editor = {Pal, Nikhil R and Kasabov, Nikola and Mudi, Rajani K and Pal, Srimanta and Parui, Swapan K},
isbn = {3-540-23931-6},
pages = {150--157},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Extending the SOM Algorithm to Non-Euclidean Distances via the Kernel Trick}},
volume = {3316},
year = {2004}
}
@article{Wang2013,
author = {Wang, Zhuang},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang - 2013 - Large-scale Non-linear Classification Algorithms and Evaluations.pdf:pdf},
title = {{Large-scale Non-linear Classification : Algorithms and Evaluations}},
year = {2013}
}
@article{Nanz2006,
author = {Nanz, S and Hankin, C},
doi = {10.1016/j.tcs.2006.08.036},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Nanz, Hankin - 2006 - A framework for security analysis of mobile wireless networks.pdf:pdf},
issn = {03043975},
journal = {Theoretical Computer Science},
month = {nov},
number = {1-2},
pages = {203--227},
title = {{A framework for security analysis of mobile wireless networks}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0304397506005792},
volume = {367},
year = {2006}
}
@article{McCormick2010,
abstract = {The analysis of climate data has relied heavily on hypothesis-driven statistical methods, while projections of future climate are based primarily on physics-based computational models. However, in recent years a wealth of new datasets has become available. Therefore, we take a more data-centric approach and propose a unified framework for studying climate, with an aim toward characterizing observed phenomena as well as discovering new knowledge in climate science. Specifically, we posit that complex networks are well suited for both descriptive analysis and predictive modeling tasks. We show that the structural properties of 'climate networks' have useful interpretation within the domain. Further, we extract clusters from these networks and demonstrate their predictive power as climate indices. Our experimental results establish that the network clusters are statistically significantly better predictors than clusters derived using a more traditional clustering approach. Using complex networks as data representation thus enables the unique opportunity for descriptive and predictive modeling to inform each other. 2010 Wiley Periodicals, Inc.},
archivePrefix = {arXiv},
arxivId = {1206.3552},
author = {McCormick, Tyler H. and Ferrell, Rebecca and Karr, Alan F. and Ryan, Patrick B.},
doi = {10.1002/sam},
eprint = {1206.3552},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/McCormick et al. - 2010 - Complex Networks as a Unified Framework for Descriptive Analysis and Predictive Modeling in Climate Science.pdf:pdf},
isbn = {1932-1872},
issn = {19321872},
journal = {Science And Technology},
keywords = {climate data,community detection,complex networks,multivariate predictive modeling,network analysis},
number = {5},
pages = {497--511},
pmid = {21824845},
title = {{Complex Networks as a Unified Framework for Descriptive Analysis and Predictive Modeling in Climate Science}},
volume = {4},
year = {2010}
}
@article{Cortes2010a,
abstract = {This paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results. We point out simple cases where importance weighting can fail, which suggests the need for an analysis of the properties of this technique. We then give both upper and lower bounds for generalization with bounded importance weights and, more significantly, give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded, a condition related to the R´enyi divergence of the training and test distributions. These results are based on a series of novel and general bounds we derive for unbounded loss functions, which are of independent interest. We use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits. Finally, we analyze the properties of normalized importance weights which are also commonly used.},
author = {Cortes, Corinna and Mansour, Y and Mohri, M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Cortes, Mansour, Mohri - 2010 - Learning Bounds for Importance Weighting.pdf:pdf},
isbn = {9781617823800},
journal = {Nips},
pages = {1--9},
title = {{Learning Bounds for Importance Weighting.}},
url = {https://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting.pdf},
year = {2010}
}
@article{DiMassa,
author = {{Di Massa}, V. and Monfardini, G. and Sarti, L. and Scarselli, F. and Maggini, M. and Gori, M.},
doi = {10.1109/IJCNN.2006.1716174},
isbn = {0-7803-9490-9},
journal = {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
pages = {778--785},
publisher = {Ieee},
title = {{A Comparison between Recursive Neural Networks and Graph Neural Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1716174}
}
@article{Gnecco2008,
author = {Gnecco, Giorgio and Sanguineti, Marcello},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Gnecco, Sanguineti - 2008 - Approximation error bounds via Rademacher's complexity.pdf:pdf},
journal = {Applied Mathematical Sciences},
keywords = {ality,approximation error,curse of dimension-,model complexity,rademacher,s complexity,s inequality,talagrand,union bounds,vc di-},
number = {4},
pages = {153--176},
title = {{Approximation error bounds via Rademacher's complexity}},
url = {http://eprints.imtlucca.it/1749/},
volume = {2},
year = {2008}
}
@article{Hofmann2008,
author = {Hofmann, Thomas and Scholkopf, Bernhard and Smola, Alexander J},
journal = {Annals of Statistics},
number = {3},
pages = {1171--1220},
title = {{Kernel methods in machine learning}},
volume = {36},
year = {2008}
}
@article{Zimmermann,
author = {Zimmermann, Albrecht and Raedt, Luc De and Nijssen, Siegfried},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zimmermann, Raedt, Nijssen - Unknown - Don't Be Afraid of Simpler Patterns.pdf:pdf},
title = {{Don't Be Afraid of Simpler Patterns}}
}
@article{Klinkenberg2004,
author = {Klinkenberg, Ralf},
issn = {1088-467X},
journal = {Intelligent Data Analysis},
month = {aug},
number = {3},
pages = {281--300},
title = {{Learning drifting concepts: Example selection vs. example weighting}},
url = {http://dl.acm.org/citation.cfm?id=1293831.1293836},
volume = {8},
year = {2004}
}
@article{Karaman2014,
author = {Karaman, Svebor and Seidenari, Lorenzo and Ma, S},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Karaman, Seidenari, Ma - 2014 - Adaptive Structured Pooling for Action Recognition.pdf:pdf},
journal = {Proc. of British Machine {\ldots}},
pages = {1--12},
title = {{Adaptive Structured Pooling for Action Recognition}},
url = {http://www.bmva.org/bmvc/2014/files/paper109.pdf},
year = {2014}
}
@inproceedings{Menchetti:fk,
abstract = {We introduce a family of kernels on discrete data structures within the general class of decomposition kernels. A weighted decomposition kernel {\{}(WDK){\}} is computed by dividing objects into substructures indexed by a selector. Two substructures are then matched if their selectors satisfy an equality predicate, while the importance of the match is determined by a probability kernel on local distributions fitted on the substructures. Under reasonable assumptions, a {\{}WDK{\}} can be computed efficiently and can avoid combinatorial explosion of the feature space. We report experimental evidence that the proposed kernel is highly competitive with respect to more complex state-of-the-art methods on a set of problems in bioinformatics.},
address = {New York, New York, USA},
annote = {
        From Duplicate 1 ( 
        

        

        
          

        
        
          Weighted decomposition kernels
        
        
          

        
        

        

         - Menchetti, S; Costa, F; Frasconi, P )

          

        
        

        

        

        

        

        

        

      },
author = {Menchetti, Sauro and Costa, Fabrizio and Frasconi, Paolo},
booktitle = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
doi = {10.1145/1102351.1102425},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Menchetti, Costa, Frasconi - 2005 - Weighted decomposition kernels.pdf:pdf},
isbn = {1595931805},
pages = {585--592},
publisher = {ACM Press},
title = {{Weighted decomposition kernels}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102425},
year = {2005}
}
@article{Sato2008,
abstract = {BACKGROUND: Recent discoveries of a large variety of important roles for non-coding RNAs (ncRNAs) have been reported by numerous researchers. In order to analyze ncRNAs by kernel methods including support vector machines, we propose stem kernels as an extension of string kernels for measuring the similarities between two RNA sequences from the viewpoint of secondary structures. However, applying stem kernels directly to large data sets of ncRNAs is impractical due to their computational complexity. RESULTS: We have developed a new technique based on directed acyclic graphs (DAGs) derived from base-pairing probability matrices of RNA sequences that significantly increases the computation speed of stem kernels. Furthermore, we propose profile-profile stem kernels for multiple alignments of RNA sequences which utilize base-pairing probability matrices for multiple alignments instead of those for individual sequences. Our kernels outperformed the existing methods with respect to the detection of known ncRNAs and kernel hierarchical clustering. CONCLUSION: Stem kernels can be utilized as a reliable similarity measure of structural RNAs, and can be used in various kernel-based applications.},
author = {Sato, Kengo and Mituyama, Toutai and Asai, Kiyoshi and Sakakibara, Yasubumi},
doi = {10.1186/1471-2105-9-318},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sato et al. - 2008 - Directed acyclic graph kernels for structural RNA analysis.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Base Pairing,Base Sequence,Methods,Models,Molecular,Nucleic Acid Conformation,Probability,RNA,Sequence Alignment,Sequence Alignment: methods,Untranslated,Untranslated: chemistry},
month = {jan},
pages = {318},
pmid = {18647390},
title = {{Directed acyclic graph kernels for structural RNA analysis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18647390},
volume = {9},
year = {2008}
}
@article{Burattin2014,
abstract = {Today's business processes are often controlled and supported by information systems. These systems record real-time information about business processes during their executions. This enables the analysis at runtime of the process behavior. However, many modern systems produce “big data”, i.e., collections of data sets so large and complex that it becomes impossible to store and process all of them. Moreover, few processes are in steady-state but, due to changing circumstances, they evolve, and systems need to adapt continuously. In this paper, we present a novel framework for the discovery of LTL-based declarative process models from streaming event data in settings where it is impossible to store all events over an extended period of time or where processes evolve while being analyzed. The framework continuously updates a set of valid business constraints based on the events occurred in the event stream. In addition, our approach is able to provide meaningful information about the most significant concept drifts, i.e., changes occurring in a process during its execution.We report about experimental results obtained using synthetic logs and a real-life event log pertaining to the treatment of patients diagnosed with cancer in a large Dutch academic hospital. Index},
author = {Burattin, Andrea and Cimitile, Marta and Maggi, Fabrizio M and Sperduti, Alessandro},
doi = {10.1109/TSC.2015.2459703},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Burattin et al. - 2014 - Online Discovery of Declarative Process Models from Event Streams.pdf:pdf},
issn = {1939-1374},
number = {6},
pages = {1--14},
title = {{Online Discovery of Declarative Process Models from Event Streams}},
volume = {8},
year = {2014}
}
@article{Herbster2005,
address = {New York, New York, USA},
author = {Herbster, Mark and Pontil, Massimiliano and Wainer, Lisa},
doi = {10.1145/1102351.1102390},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Herbster, Pontil, Wainer - 2005 - Online learning over graphs.pdf:pdf},
isbn = {1595931805},
journal = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
pages = {305--312},
publisher = {ACM Press},
title = {{Online learning over graphs}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102390},
year = {2005}
}
@inproceedings{Cuturi2009,
author = {Shin, K and Cuturi, Marco and Kuboyama, T},
booktitle = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
pages = {961--968},
title = {{Mapping kernels for trees}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/ICML2011Shin{\_}503.pdf},
year = {2011}
}
@inproceedings{Rahimi2007a,
author = {Rahimi, Ali and Recht, Ben},
booktitle = {In Neural Infomration Processing Systems},
doi = {10.1.1.145.8736},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rahimi, Recht - 2007 - Random Features for Large-Scale Kernel Machines.pdf:pdf},
number = {1},
title = {{Random Features for Large-Scale Kernel Machines}},
year = {2007}
}
@article{Block1962,
author = {Block, H.},
doi = {10.1103/RevModPhys.34.123},
issn = {0034-6861},
journal = {Reviews of Modern Physics},
month = {jan},
number = {1},
pages = {123--135},
title = {{The Perceptron: A Model for Brain Functioning. I}},
url = {http://link.aps.org/doi/10.1103/RevModPhys.34.123},
volume = {34},
year = {1962}
}
@incollection{Geibel2007,
abstract = {In this paper, we discuss tree kernels that can be applied for the
classification of {\{}XML{\}} documents based on their {\{}DOM{\}} trees. {\{}DOM{\}}
trees are ordered trees, in which every node might be labeled by
a vector of attributes including its {\{}XML{\}} tag and the textual content.
We describe four new kernels suitable for this kind of trees: a tree
kernel derived from the well-known parse tree kernel, the set tree
kernel that allows permutations of children, the string tree kernel
being an extension of the so-called partial tree kernel, and the
soft tree kernel, which is based on the set tree kernel and takes
into a account a ???fuzzy??? comparison of child positions. We present
first results on an artificial data set, a corpus of newspaper articles,
for which we want to determine the type (genre) of an article based
on its structure alone, and the well-known {\{}SUSANNE{\}} corpus.},
author = {Geibel, Peter and Gust, Helmar and K{\"{u}}hnberger, Kai-Uwe},
booktitle = {{\{}MICAI{\}} 2007: Advances in Artificial Intelligence},
issn = {0302-9743 (Print) 1611-3349 (Online)},
pages = {850--860},
publisher = {Springer Berlin / Heidelberg},
title = {{Variants of Tree Kernels for {\{}XML{\}} Documents}},
year = {2007}
}
@article{Fink2007,
author = {Fink, Tobias and Reymond, Jean L},
doi = {10.1021/ci600423u},
institution = {Department of Chemistry and Biochemistry, University of Berne, Freiestrasse 3, CH-3012 Berne, Switzerland.},
journal = {J Chem Inf Model},
number = {2},
pages = {342--353},
title = {{Virtual exploration of the chemical universe up to 11 atoms of C, N, O, F: assembly of 26.4 million structures (110.9 million stereoisomers) and analysis for new ring systems, stereochemistry, physicochemical properties, compound classes, and drug discove}},
volume = {47},
year = {2007}
}
@article{Zhou,
author = {Zhou, Dengyong and Huang, Jiayuan and Sch, Bernhard},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Huang, Sch - Unknown - Learning with Hypergraphs Clustering , Classification , and Embedding.pdf:pdf},
title = {{Learning with Hypergraphs : Clustering , Classification , and Embedding}}
}
@article{Suard2007,
author = {Suard, F. and Rakotomamonjy, a. and Bensrhair, a.},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Suard, Rakotomamonjy, Bensrhair - 2007 - Kernel on bag of paths for measuring similarity of shapes.pdf:pdf},
isbn = {2930307099},
journal = {European Symposium on Artificial Neural Networks},
pages = {1--6},
title = {{Kernel on bag of paths for measuring similarity of shapes}},
year = {2007}
}
@article{Zhou2008,
author = {Zhou, Bo and Trinajsti{\'{c}}, Nenad},
doi = {10.1007/s10910-008-9459-3},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhou, Trinajsti{\'{c}} - 2008 - On resistance-distance and Kirchhoff index.pdf:pdf},
issn = {0259-9791},
journal = {Journal of Mathematical Chemistry},
keywords = {kirchhoff index,laplacian eigenvalues,laplacian matrix},
month = {sep},
number = {1},
pages = {283--289},
title = {{On resistance-distance and Kirchhoff index}},
url = {http://www.springerlink.com/index/10.1007/s10910-008-9459-3},
volume = {46},
year = {2008}
}
@article{Yamanishi2007,
address = {Oxford, UK},
author = {Yamanishi, Yoshihiro and Bach, Francis and Vert, Jean-Philippe},
doi = {http://dx.doi.org/10.1093/bioinformatics/btm090},
issn = {1367-4803},
journal = {Bioinformatics},
number = {10},
pages = {1211--1216},
publisher = {Oxford University Press},
title = {{Glycan classification with tree kernels}},
volume = {23},
year = {2007}
}
@book{Giannella2003,
author = {Giannella, C and Han, J and Pei, J and Yan, X and Yu, P S},
publisher = {AAAI/MIT},
title = {{Mining Frequent Patterns in Data Streams at Multiple Time Granularities in H. Kargupta, A. Joshi, K. Sivakumar, and Y. Yesha (eds.), Next Generation Data Mining}},
year = {2003}
}
@inproceedings{Sonnenburg2006,
author = {Sonnenburg, Soeren and Franc, V},
booktitle = {ICML 2010: Proceedings of the 27th international conference on Machine learning},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sonnenburg, Franc - 2010 - COFFIN A computational framework for linear SVMs.pdf:pdf},
title = {{COFFIN: A computational framework for linear SVMs}},
url = {http://eprints.pascal-network.org/archive/00007940/},
year = {2010}
}
@inproceedings{Collins2002,
abstract = {This paper introduces new learning algorithms for natural language
processing based on the perceptron algorithm. We show how the algorithms
can be efficiently applied to exponential sized representations of
parse trees, such as the "all subtrees" {\{}(DOP){\}} representation described
by {\{}(Bod{\}} 1998), or a representation tracking all sub-fragments of
a tagged sentence. We give experimental results showing significant
improvements on two tasks: parsing Wall Street Journal text, and
named-entity extraction from web data.},
address = {Philadelphia, Pennsylvania},
author = {Collins, Michael and Duffy, Nigel},
booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
pages = {263--270},
publisher = {Association for Computational Linguistics},
shorttitle = {New ranking algorithms for parsing and tagging},
title = {{New ranking algorithms for parsing and tagging: kernels over discrete structures, and the voted perceptron}},
year = {2002}
}
@article{Orabona2009,
author = {Orabona, Francesco and Keshet, Joseph and Caputo, Barbara},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Orabona, Keshet, Caputo - 2009 - Bounded Kernel-Based Online Learning ∗.pdf:pdf},
keywords = {bounded support set,kernel methods,online learning,support vector machines},
pages = {2643--2666},
title = {{Bounded Kernel-Based Online Learning ∗}},
volume = {10},
year = {2009}
}
@article{Yu2007,
author = {Yu, Hsiang-fu and Hsieh, Cho-jui and Chang, Kai-wei and Lin, Chih-jen},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Yu et al. - 2007 - Large Linear Classification When Data Cannot Fit in Memory.pdf:pdf},
keywords = {Best Paper Track},
pages = {2777--2782},
title = {{Large Linear Classification When Data Cannot Fit in Memory}},
year = {2007}
}
@article{Harchaoui:fk,
author = {Harchaoui, Z Z and Harchaoui, Z},
doi = {10.1109/CVPR.2007.383049},
title = {{Image classification with segmentation graph kernels}},
year = {2007}
}
@inproceedings{Shin2008a,
author = {Shin, Kilho and Kuboyama, Tetsuji},
booktitle = {Australasian Conference on Artificial Intelligence},
editor = {Wobcke, Wayne and Zhang, Mengjie},
isbn = {978-3-540-89377-6},
pages = {236--246},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Kernels Based on Distributions of Agreement Subtrees}},
volume = {5360},
year = {2008}
}
@inproceedings{Guha2001,
address = {Heraklion, Crete, Greece},
author = {Guha, S and Koudas, N and Shim, K},
booktitle = {Proc. of the 2001 Annual ACM Symposium on Theory of Computing (STOC'01)},
pages = {471--475},
title = {{Data-streams and histograms}},
year = {2001}
}
@article{Wang2010,
author = {Wang, Y. and Bolton, E. and Dracheva, S. and Karapetyan, K. and Shoemaker, B. a. and Suzek, T. O. and Wang, J. and Xiao, J. and Zhang, J. and Bryant, S. H.},
doi = {10.1093/nar/gkp965},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2010 - An overview of the PubChem BioAssay resource.pdf:pdf},
issn = {0305-1048},
journal = {Nucleic Acids Research},
number = {Database},
pages = {D255--D266},
title = {{An overview of the PubChem BioAssay resource}},
url = {http://nar.oxfordjournals.org/lookup/doi/10.1093/nar/gkp965},
volume = {38},
year = {2010}
}
@article{Kargupta1999,
author = {Kargupta, Hillol and Huang, Weiyun and Sivakumar, Krishnamoorthy and Johnson, Erik},
journal = {{\{}KNOWLEDGE{\}} {\{}AND{\}} {\{}INFORMATION{\}} {\{}SYSTEMS{\}}},
pages = {2001},
title = {{Distributed Clustering Using Collective Principal Component Analysis}},
volume = {3},
year = {1999}
}
@article{Lia,
author = {Li, Geng and Semerci, Murat and Zaki, Mohammed J},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Li, Semerci, Zaki - Unknown - Graph Classification via Topological and Label Attributes.pdf:pdf},
isbn = {9781450308342},
title = {{Graph Classification via Topological and Label Attributes}}
}
@article{Helma2001,
abstract = {We initiated the Predictive Toxicology Challenge (PTC) to stimulate
the development of advanced SAR techniques for predictive toxicology
models. 

The goal of this challenge is to predict the rodent carcinogenicity
of new compounds based on the experimental results of the US National
Toxicology Program (NTP).

Submissions will be evaluated on quantitative and qualitative scales
to select the most predictive models and those with the highest toxicological
relevance.},
annote = {        From Duplicate 1 (                   The predictive toxicology challenge 2000-2001                 - C. Helma R. D. King, S Kramer; Srinivasan, A )
                
        
        
      },
author = {Helma, C and King, R D and Kramer, S and Srinivasan, A},
doi = {10.1093/bioinformatics/17.1.107},
journal = {Bioinformatics},
number = {1},
pages = {107--108},
title = {{The Predictive Toxicology Challenge 2000-2001}},
volume = {17},
year = {2001}
}
@article{Kulasiri2008,
author = {Kulasiri, Don and Nguyen, Lan K. and Samarasinghe, Sandhya and Xie, Zhi},
doi = {10.2174/157489308785909214},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Kulasiri et al. - 2008 - A Review of Systems Biology Perspective on Genetic Regulatory Networks with Examples.pdf:pdf},
issn = {15748936},
journal = {Current Bioinformatics},
keywords = {circadian rhythms,genetic regulatory networks,modelling,noise,systems biology},
month = {sep},
number = {3},
pages = {197--225},
title = {{A Review of Systems Biology Perspective on Genetic Regulatory Networks with Examples}},
url = {http://openurl.ingenta.com/content/xref?genre=article{\&}issn=1574-8936{\&}volume=3{\&}issue=3{\&}spage=197},
volume = {3},
year = {2008}
}
@inproceedings{Aggarwal2003,
address = {Berlin, Germany},
author = {Aggarwal, C C and Han, J and Wang, J and Yu, P},
booktitle = {Proc. of the 2003 Internl. Conf. on Very Large Data Bases (VLDB'03)},
pages = {81--92},
title = {{A framework for clustering evolving data streams}},
year = {2003}
}
@inproceedings{DBLP:conf/iconip/2015-4,
doi = {10.1007/978-3-319-26561-2},
editor = {Arik, Sabri and Huang, Tingwen and Lai, Weng Kin and Liu, Qingshan},
isbn = {978-3-319-26560-5},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Neural Information Processing - 22nd International Conference, {\{}ICONIP{\}} 2015, Istanbul, Turkey, November 9-12, 2015, Proceedings, Part {\{}IV{\}}}},
url = {http://dx.doi.org/10.1007/978-3-319-26561-2},
volume = {9492},
year = {2015}
}
@article{Frohlich2007,
abstract = {BACKGROUND:With the increased availability of high throughput data,
such as DNA microarray data, researchers are capable of producing
large amounts of biological data. During the analysis of such data
often there is the need to further explore the similarity of genes
not only with respect to their expression, but also with respect
to their functional annotation which can be obtained from Gene Ontology
(GO).RESULTS:We present the freely available software package GOSim,
which allows to calculate the functional similarity of genes based
on various information theoretic similarity concepts for GO terms.
GOSim extends existing tools by providing additional lately developed
functional similarity measures for genes. These can e.g. be used
to cluster genes according to their biological function. Vice versa,
they can also be used to evaluate the homogeneity of a given grouping
of genes with respect to their GO annotation. GOSim hence provides
the researcher with a flexible and powerful tool to combine knowledge
stored in GO with experimental data. It can be seen as complementary
to other tools that, for instance, search for significantly overrepresented
GO terms within a given group of genes.CONCLUSION:GOSim is implemented
as a package for the statistical computing environment R and is distributed
under GPL within the CRAN project.},
author = {Frohlich, Holger and Speer, Nora and Poustka, Annemarie and BeiSZbarth, Tim},
doi = {10.1186/1471-2105-8-166},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {166},
title = {{GOSim - an R-package for computation of information theoretic GO similarities between terms and gene products}},
volume = {8},
year = {2007}
}
@article{levesque:functional-foundations,
author = {Levesque, Hector{\~{}}J.},
journal = {Artificial Intelligence},
month = {jul},
number = {2},
pages = {155--212},
title = {{Foundations of a functional approach to knowledge representation}},
volume = {23},
year = {1984}
}
@article{Ralaivola2005,
abstract = {Increased availability of large repositories of chemical compounds is creating new challenges and opportunities for the application of machine learning methods to problems in computational chemistry and chemical informatics. Because chemical compounds are often represented by the graph of their covalent bonds, machine learning methods in this domain must be capable of processing graphical structures with variable size. Here, we first briefly review the literature on graph kernels and then introduce three new kernels (Tanimoto, MinMax, Hybrid) based on the idea of molecular fingerprints and counting labeled paths of depth up to d using depth-first search from each possible vertex. The kernels are applied to three classification problems to predict mutagenicity, toxicity, and anti-cancer activity on three publicly available data sets. The kernels achieve performances at least comparable, and most often superior, to those previously reported in the literature reaching accuracies of 91.5{\%} on the Mutag dataset, 65-67{\%} on the PTC (Predictive Toxicology Challenge) dataset, and 72{\%} on the NCI (National Cancer Institute) dataset. Properties and tradeoffs of these kernels, as well as other proposed kernels that leverage 1D or 3D representations of molecules, are briefly discussed.},
address = {Oxford, UK, UK},
author = {Ralaivola, Liva and Swamidass, Sanjay J and Saigo, Hiroto and Baldi, Pierre},
doi = {10.1016/j.neunet.2005.07.009},
issn = {0893-6080},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Anticarcinogenic Agents,Artificial Intelligence,Automated,Chemical,Computer Graphics,Databases,Drug Toxicity,Genetic,Information Storage and Retrieval,Models,National Practitioner Data Bank,Pattern Recognition,Protein,Sequence Analysis,Structure-Activity Relationship,United States},
month = {oct},
number = {8},
pages = {1093--110},
pmid = {16157471},
publisher = {Elsevier Science Ltd.},
title = {{Graph kernels for chemical informatics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16157471},
volume = {18},
year = {2005}
}
@article{Getoor2005,
address = {New York, NY, USA},
author = {Getoor, Lise and Diehl, Christopher P},
doi = {http://doi.acm.org/10.1145/1117454.1117456},
issn = {1931-0145},
journal = {SIGKDD Explor. Newsl.},
number = {2},
pages = {3--12},
publisher = {ACM},
title = {{Link mining: a survey}},
volume = {7},
year = {2005}
}
@misc{da2009,
author = {{Da San Martino}, G},
title = {{Kernel Methods for Tree Structured Data (Ph.D. Thesis)}},
year = {2009}
}
@inproceedings{Cumby2003,
abstract = {Kernel methods have gained a great deal of popularity in the machine
learning community as a method to learn indirectly in highdimensional
feature spaces. Those interested in relational learning have recently
begun to cast learning from structured and relational data in terms
of kernel operations. We describe a general family of kernel functions
built up from a description language of limited expressivity and
use it to study the benefits and drawbacks of kernel learning in
relational domains. Learning with kernels in this family directly
models learning over an expanded feature space constructed using
the same description language. This allows us to examine issues of
time complexity in terms of learning with these and other relational
kernels, and how these relate to generalization ability. The tradeoffs
between using kernels in a very high dimensional implicit space versus
a restricted feature space, is highlighted through two experiments,
in bioinformatics and in natural language processing.},
author = {Cumby, Chad and Roth, Dan},
booktitle = {In Proceedings of the International Conference on Machine Learning},
doi = {10.1.1.73.1303},
pages = {107--114},
title = {{On kernel methods for relational learning}},
year = {2003}
}
@article{Denoyer2007,
address = {New York, NY, USA},
author = {Denoyer, Ludovic and Gallinari, Patrick},
doi = {http://doi.acm.org/10.1145/1273221.1273230},
issn = {0163-5840},
journal = {SIGIR Forum},
number = {1},
pages = {79--90},
publisher = {ACM},
title = {{Report on the XML mining track at INEX 2005 and INEX 2006: categorization and clustering of XML documents}},
volume = {41},
year = {2007}
}
@article{Chen2014,
author = {Chen, BoLin and Li, Min and Wang, JianXin and Wu, Fang-Xiang},
doi = {10.1007/s11427-014-4745-8},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2014 - Disease gene identification by using graph kernels and Markov random fields.pdf:pdf},
issn = {1674-7305},
journal = {Science China Life Sciences},
number = {11},
pages = {1054--1063},
title = {{Disease gene identification by using graph kernels and Markov random fields}},
url = {http://link.springer.com/10.1007/s11427-014-4745-8},
volume = {57},
year = {2014}
}
@inproceedings{Bouckaert2004,
address = {New York, New York, USA},
author = {Bouckaert, Remco R.},
booktitle = {Twenty-first international conference on Machine learning - ICML '04},
doi = {10.1145/1015330.1015338},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bouckaert - 2004 - Estimating replicability of classifier learning experiments.pdf:pdf},
isbn = {1581138285},
pages = {15},
publisher = {ACM Press},
title = {{Estimating replicability of classifier learning experiments}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015338},
year = {2004}
}
@article{Wolpert1995,
author = {Wolpert, David H.},
journal = {Artificial Intelligence},
number = {1978},
pages = {392--394},
title = {{THE LACK OFA PRIORI DISTINCTIONS BETWEEN LEARNING ALGORITHMS}},
volume = {III},
year = {1995}
}
@article{Vert2002a,
abstract = {MOTIVATION: The phylogenetic profile of a protein is a string that encodes the presence or absence of the protein in every fully sequenced genome. Because proteins that participate in a common structural complex or metabolic pathway are likely to evolve in a correlated fashion, the phylogenetic profiles of such proteins are often 'similar' or at least 'related' to each other. The question we address in this paper is the following: how to measure the 'similarity' between two profiles, in an evolutionarily relevant way, in order to develop efficient function prediction methods? RESULTS: We show how the profiles can be mapped to a high-dimensional vector space which incorporates evolutionarily relevant information, and we provide an algorithm to compute efficiently the inner product in that space, which we call the tree kernel. The tree kernel can be used by any kernel-based analysis method for classification or data mining of phylogenetic profiles. As an application a Support Vector Machine (SVM) trained to predict the functional class of a gene from its phylogenetic profile is shown to perform better with the tree kernel than with a naive kernel that does not include any information about the phylogenetic relationships among species. Moreover a kernel principal component analysis (KPCA) of the phylogenetic profiles illustrates the sensitivity of the tree kernel to evolutionarily relevant variations.},
author = {Vert, Jean-Philippe},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vert - 2002 - A tree kernel to analyse phylogenetic profiles.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Artificial Intelligence,Evolution, Molecular,Gene Expression Profiling,Gene Expression Profiling: methods,Gene Expression Regulation,Gene Expression Regulation: genetics,Models, Genetic,Models, Statistical,Pattern Recognition, Automated,Phylogeny,Saccharomyces cerevisiae,Saccharomyces cerevisiae Proteins,Saccharomyces cerevisiae Proteins: genetics,Saccharomyces cerevisiae: genetics},
month = {jan},
pages = {S276--84},
pmid = {12169557},
title = {{A tree kernel to analyse phylogenetic profiles.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12169557},
volume = {18 Suppl 1},
year = {2002}
}
@book{Robinson,
author = {Robinson, Ian and Webber, Jim and Eifrem, Emil},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Robinson, Webber, Eifrem - Unknown - Graph Databases.pdf:pdf},
isbn = {9781449356262},
title = {{Graph Databases}}
}
@inproceedings{Vries2013a,
annote = {@inproceedings{\{}DBLP:dblp{\_}conf/dmold/Vries13,
   author              = {\{}Gerben Klaas Dirk de Vries{\}},
   title               = {\{}Graph Kernels for Task 1 and 2 of the Linked Data Data Mining Challenge 2013.{\}},
   booktitle           = {\{}DMoLD{\}},
   year                = {\{}2013{\}},
   ee                  = {\{}http://ceur-ws.org/Vol-1082/paper3.pdf{\}},
   crossref            = {\{}2013{\}},
{\}}

        

      },
author = {Vries, GKD De},
booktitle = {DMoLD},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Vries - 2013 - Graph Kernels for Task 1 and 2 of the Linked Data Data-Mining Challenge 2013.pdf:pdf},
title = {{Graph Kernels for Task 1 and 2 of the Linked Data Data-Mining Challenge 2013}},
year = {2013}
}
@article{Aiolli2015,
author = {Aiolli, Fabio and Donini, Michele},
doi = {10.1016/j.neucom.2014.11.078},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Aiolli, Donini - 2015 - EasyMKL a scalable multiple kernel learning algorithm.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Feature learning,Feature selection,Kernel methods,Multiple kernel learning,multiple kernel learning},
pages = {215--224},
publisher = {Elsevier},
title = {{EasyMKL: a scalable multiple kernel learning algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0925231215003653},
volume = {169},
year = {2015}
}
@article{Menzel2009,
abstract = {User-driven in silico RNA homology search is still a nontrivial task. In part, this is the consequence of a limited precision of the computational tools in spite of recent exciting progress in this area, and to a certain extent, computational costs are still problematic in practice. An important, and as we argue here, dominating issue is the dependence on good curated (secondary) structural alignments of the RNAs. These are often hard to obtain, not so much because of an inherent limitation in the available data, but because they require substantial manual curation, an effort that is rarely acknowledged. Here, we qualitatively describe a realistic scenario for what a "regular user" (i.e., a nonexpert in a particular RNA family) can do in practice, and what kind of results are likely to be achieved. Despite the indisputable advances in computational RNA biology, the conclusion is discouraging: BLAST still works better or equally good as other methods unless extensive expert knowledge on the RNA family is included. However, when good curated data are available the recent development yields further improvements in finding remote homologs. Homology search beyond the reach of BLAST hence is not at all a routine task.},
author = {Menzel, Peter and Gorodkin, Jan and Stadler, Peter F},
doi = {10.1261/rna.1556009},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Menzel, Gorodkin, Stadler - 2009 - The tedious task of finding homologous noncoding RNA genes.pdf:pdf},
issn = {1469-9001},
journal = {RNA (New York, N.Y.)},
keywords = {Animals,Computational Biology,Humans,Nucleic Acid Conformation,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: genetics,Sequence Analysis, DNA,Sequence Homology, Nucleic Acid},
month = {dec},
number = {12},
pages = {2075--82},
pmid = {19861422},
title = {{The tedious task of finding homologous noncoding RNA genes.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2779685{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {15},
year = {2009}
}
@article{Rivas2001,
abstract = {Some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic RNAs [1, 2]. Unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding RNA (ncRNA) gene sequences have no obvious inherent statistical biases [3]. Thus, genome sequence analyses reveal novel protein-coding genes, but any novel ncRNA genes remain invisible. Here, we describe a computational comparative genomic screen for ncRNA genes. The key idea is to distinguish conserved RNA secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. We report the first whole-genome screen for ncRNA genes done with this method, in which we applied it to the "intergenic" spacers of Escherichia coli using comparative sequence data from four related bacteria. Starting from {\textgreater}23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural RNA loci. A sample of 49 candidate loci was assayed experimentally. At least 11 loci expressed small, apparently noncoding RNA transcripts of unknown function. Our computational approach may be used to discover structural ncRNA genes in any genome for which appropriate comparative genome sequence data are available.},
author = {Rivas, E and Klein, R J and Jones, T a and Eddy, S R},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rivas et al. - 2001 - Computational identification of noncoding RNAs in E. coli by comparative genomics.pdf:pdf},
issn = {0960-9822},
journal = {Current biology : CB},
keywords = {Animals,Escherichia coli,Escherichia coli: genetics,Gene Expression,Genome, Bacterial,Humans,RNA, Bacterial,RNA, Bacterial: analysis,RNA, Untranslated,RNA, Untranslated: analysis},
month = {sep},
number = {17},
pages = {1369--73},
pmid = {11553332},
title = {{Computational identification of noncoding RNAs in E. coli by comparative genomics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11553332},
volume = {11},
year = {2001}
}
@unpublished{Vert2008,
abstract = {We review a recent trend in computational systems biology which aims
at using pattern recognition algorithms to infer the structure of
large-scale biological networks from heterogeneous genomic data.
We present several strategies that have been proposed and that lead
to different pattern recognition problems and algorithms. The strenght
of these approaches is illustrated on the reconstruction of metabolic,
protein-protein and regulatory networks of model organisms. In all
cases, state-of-the-art performance is reported.},
author = {Vert, Jean-Philippe},
booktitle = {0806.0215},
month = {jun},
title = {{Reconstruction of biological networks by supervised machine learning approaches}},
year = {2008}
}
@article{Ferragina2009,
author = {Ferragina, Paolo and Luccio, Fabrizio and Manzini, Giovanni and Muthukrishnan, S.},
doi = {10.1145/1613676.1613680},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Ferragina et al. - 2009 - Compressing and indexing labeled trees, with applications.pdf:pdf},
issn = {00045411},
journal = {Journal of the ACM},
month = {nov},
number = {1},
pages = {1--33},
title = {{Compressing and indexing labeled trees, with applications}},
url = {http://portal.acm.org/citation.cfm?doid=1613676.1613680},
volume = {57},
year = {2009}
}
@article{Chebotarev1997,
abstract = {We propose a family of graph structural indices related to the Matrix-forest theorem. The properties of the basic index that expresses the mutual connectivity of two vertices are studied in detail. The derivative indices that measure "dissociation," "solitariness," and "provinciality" of vertices are also considered. A nonstandard metric on the set of vertices is introduced, which is determined by their connectivity. The application of these indices in sociometry is discussed.},
archivePrefix = {arXiv},
arxivId = {math/0602070},
author = {Chebotarev, Pavel and Shamis, Elena},
eprint = {0602070},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Chebotarev, Shamis - 1997 - The Matrix-Forest Theorem and Measuring Relations in Small Social Groups.pdf:pdf},
journal = {Automation and Remote Control},
number = {9},
pages = {10},
primaryClass = {math},
title = {{The Matrix-Forest Theorem and Measuring Relations in Small Social Groups}},
url = {http://arxiv.org/abs/math/0602070},
volume = {58},
year = {1997}
}
@article{Schafer2005a,
abstract = {Motivation: Genetic networks are often described statistically using
graphical models (e.g. Bayesian networks). However, inferring the
network structure offers a serious challenge in microarray analysis
where the sample size is small compared to the number of considered
genes. This renders many standard algorithms for graphical models
inapplicable, and inferring genetic networks an ill-posed' inverse
problem. Methods: We introduce a novel framework for small-sample
inference of graphical models from gene expression data. Specifically,
we focus on the so-called graphical Gaussian models {\{}(GGMs){\}} that
are now frequently used to describe gene association networks and
to detect conditionally dependent genes. Our new approach is based
on (1) improved (regularized) small-sample point estimates of partial
correlation, (2) an exact test of edge inclusion with adaptive estimation
of the degree of freedom and (3) a heuristic network search based
on false discovery rate multiple testing. Steps (2) and (3) correspond
to an empirical Bayes estimate of the network topology. Results:
Using computer simulations, we investigate the sensitivity (power)
and specificity (true negative rate) of the proposed framework to
estimate {\{}GGMs{\}} from microarray data. This shows that it is possible
to recover the true network topology with high accuracy even for
small-sample datasets. Subsequently, we analyze gene expression data
from a breast cancer tumor study and illustrate our approach by inferring
a corresponding large-scale gene association network for 3883 genes.
Availability: The authors have implemented the approach in the R
package {\{}GeneTS'{\}} that is freely available from http://www.stat.uni-muenchen.de/{\{}$\backslash$textasciitilde{\}}strimmer/genets/,
from the R archive {\{}(CRAN){\}} and from the Bioconductor website. Contact:
korbinian.strimmer@lmu.de},
author = {Schafer, Juliane and Strimmer, Korbinian},
doi = {10.1093/bioinformatics/bti062},
journal = {Bioinformatics},
month = {mar},
number = {6},
pages = {754--764},
title = {{An empirical Bayes approach to inferring large-scale gene association networks}},
volume = {21},
year = {2005}
}
@article{Muthukrishnan2006,
author = {Muthukrishnan, S},
title = {{Compressing and indexing labeled trees , with applications}},
volume = {V},
year = {2006}
}
@phdthesis{Martino2009,
author = {{Da San Martino}, Giovanni},
booktitle = {Science},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Da San Martino - 2009 - Kernel Methods for Tree Structured Data.pdf:pdf},
number = {February},
school = {University of Bologna},
title = {{Kernel Methods for Tree Structured Data}},
year = {2009}
}
@article{Bleik2013,
abstract = {Recently, graph representations of text have been showing improved performance over conventional bag-of-words representations in text categorization applications. In this paper, we present a graph-based representation for biomedical articles and use graph kernels to classify those articles into high-level categories. In our representation, common biomedical concepts and semantic relationships are identified with the help of an existing ontology and are used to build a rich graph structure that provides a consistent feature set and preserves additional semantic information that could improve a classifier's performance. We attempt to classify the graphs using both a set-based graph kernel that is capable of dealing with the disconnected nature of the graphs and a simple linear kernel. Finally, we report the results comparing the classification performance of the kernel classifiers to common text-based classifiers.},
author = {Bleik, Said and Mishra, Meenakshi and Huan, Jun and Song, Min},
doi = {10.1109/TCBB.2013.16},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bleik et al. - 2013 - Text categorization of biomedical data sets using graph kernels and a controlled vocabulary.pdf:pdf},
issn = {1557-9964},
journal = {IEEE/ACM transactions on computational biology and bioinformatics / IEEE, ACM},
number = {5},
pages = {1211--7},
pmid = {24384709},
title = {{Text categorization of biomedical data sets using graph kernels and a controlled vocabulary.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24384709},
volume = {10},
year = {2013}
}
@article{Fouss:fk,
author = {Fouss, Francois F and Yen, Luh and Pirotte, Alain and Saerens, Marco},
doi = {10.1109/ICDM.2006.18},
isbn = {0-7695-2701-7},
issn = {1550-4786},
journal = {Sixth International Conference on Data Mining (ICDM'06)},
month = {dec},
pages = {863--868},
publisher = {Ieee},
title = {{An Experimental Investigation of Graph Kernels on a Collaborative Recommendation Task}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4053117},
year = {2006}
}
@article{Opitz99,
author = {Opitz, D and Maclin, R},
journal = {Journ. of Artif. Intell. Res.},
pages = {160--198},
title = {{Popular ensemble methods: an empirical study}},
volume = {11},
year = {1999}
}
@article{Manerikar2009,
author = {Manerikar, Nishad and Palpanas, Themis},
doi = {10.1016/j.datak.2008.11.001},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Manerikar, Palpanas - 2009 - Frequent items in streaming data An experimental evaluation of the state-of-the-art.pdf:pdf},
issn = {0169023X},
journal = {Data {\&} Knowledge Engineering},
month = {apr},
number = {4},
pages = {415--430},
publisher = {Elsevier B.V.},
title = {{Frequent items in streaming data: An experimental evaluation of the state-of-the-art}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169023X08001663},
volume = {68},
year = {2009}
}
@phdthesis{GrossiTesi,
author = {Grossi, V},
school = {Supervisor Prof. Franco Turini, University of Pisa},
title = {{A New Framework for Data Streams Classification}},
url = {http://etd.adm.unipi.it/theses/available/etd-11242009-124601/},
year = {2009}
}
@inproceedings{Pasa2014,
author = {Pasa, Luca and Sperduti, Alessandro},
booktitle = {NIPS},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Pasa, Sperduti - 2014 - Pre-training of Recurrent Neural Networks via Linear Autoencoders.pdf:pdf},
title = {{Pre-training of Recurrent Neural Networks via Linear Autoencoders}},
year = {2014}
}
@inproceedings{AggarwalDemand2004,
address = {Seattle, WA},
author = {Aggarwal, C C and Han, J and Wang, J and Yu, P},
booktitle = {Proc. of the 10th Internl. Conf. on Knowl. Disc. and Data Mining (KDD'04)},
pages = {503--508},
title = {{On Demand Classification of Data Streams}},
year = {2004}
}
@article{Kuboyama2007,
abstract = {Learning from tree-structured data has received increasing interest
with the rapid growth of tree-encodable data in the World Wide Web,
in biology, and in other areas. Our kernel function measures the
similarity between two trees by counting the number of shared sub-patterns
called tree q-grams, and runs, in effect, in linear time with respect
to the number of tree nodes. We apply our kernel function with a
support vector machine {\{}(SVM){\}} to classify biological data, the glycans
of several blood components. The experimental results show that our
kernel function performs as well as one exclusively tailored to glycan
properties.},
author = {Kuboyama, Tetsuji and Hirata, Kouichi and Kashima, Hisashi and Aoki-Kinoshita, Kiyoko F},
journal = {Information and Media Technologies},
number = {1},
pages = {292--299},
title = {{A Spectrum Tree Kernel}},
volume = {2},
year = {2007}
}
@inproceedings{Aiolli2009,
abstract = {Almost all tree kernels proposed in the literature match substructures without taking into account their relative positioning with respect to one another. In this paper, we propose a novel family of kernels which explicitly focus on this type of information. Specifically, after defining a family of tree kernels based on routes between nodes, we present an efficient implementation for a member of this family. Experimental results on four different datasets show that our method is able to reach state of the art performances, obtaining in some cases performances better than computationally more demanding tree kernels.},
address = {New York, New York, USA},
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553377},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Aiolli, Da San Martino, Sperduti - 2009 - Route kernels for trees.pdf:pdf},
isbn = {9781605585161},
pages = {17--24},
publisher = {ACM Press},
title = {{Route kernels for trees}},
year = {2009}
}
@article{Sami,
author = {Sami, Ashkan and Yadegari, Babak and Rahimi, Hossein and Peiravian, Naser and Hashemi, Sattar and Hamze, Ali},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Sami et al. - Unknown - Malware Detection based on Mining API Calls.pdf:pdf},
isbn = {9781605586380},
keywords = {API Call, Frequent Pattern Mining, Malware Detecti,api call,frequent pattern mining,lic dataset,malware detection,metamorphic malware,polymorphic,portable,pub-},
pages = {1020--1025},
title = {{Malware Detection based on Mining API Calls}}
}
@article{Severyn,
author = {Severyn, Aliaksei and Moschitti, Alessandro},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Severyn, Moschitti - Unknown - Fast Support Vector Machines for Structural Kernels.pdf:pdf},
title = {{Fast Support Vector Machines for Structural Kernels}}
}
@article{Shrivastava,
author = {Shrivastava, Anshumali and Science, Information and Li, Ping},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shrivastava, Science, Li - Unknown - Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS).pdf:pdf},
pages = {1--9},
title = {{Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)}}
}
@article{Campagna,
archivePrefix = {arXiv},
arxivId = {arXiv:1005.0239v1},
author = {Campagna, Andrea},
eprint = {arXiv:1005.0239v1},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Campagna - Unknown - On Finding Frequent Patterns in Directed Acyclic Graphs ∗.pdf:pdf},
title = {{On Finding Frequent Patterns in Directed Acyclic Graphs ∗}}
}
@article{Shin2010,
author = {Shin, Kilho and Kuboyama, Tetsuji},
doi = {10.1007/s11390-010-1082-7},
journal = {Journal of Computer Science and Technology},
keywords = {convolution kernel,edit distance,kernel,tree},
number = {5},
pages = {1040--1054},
title = {{A Generalization of Haussler's Convolution Kernel-Mapping Kernel and Its Application to Tree Kernels}},
url = {http://link.springer.com/article/10.1007/s11390-010-9386-1},
volume = {25},
year = {2010}
}
@article{Orabona2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1304.2994v3},
author = {Orabona, Francesco and Crammer, Koby and Technion, The},
eprint = {arXiv:1304.2994v3},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Orabona, Crammer, Technion - 2014 - A Generalized Online Mirror Descent with Applications to Classification and Regression.pdf:pdf},
pages = {1--23},
title = {{A Generalized Online Mirror Descent with Applications to Classification and Regression}},
year = {2014}
}
@article{Furer2001,
author = {F{\"{u}}rer, M},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/F{\"{u}}rer - 2001 - Weisfeiler-Lehman refinement requires at least a linear number of iterations.pdf:pdf},
journal = {Automata, Languages and Programming},
keywords = {descriptive complexity,games,graph isomorphism testing,ment,weisfeiler-lehman refine-},
pages = {322--333},
title = {{Weisfeiler-Lehman refinement requires at least a linear number of iterations}},
url = {http://link.springer.com/chapter/10.1007/3-540-48224-5{\_}27},
year = {2001}
}
@article{Zhu2009,
author = {Zhu, Zeyuan Allen and Chen, Weizhu and Wang, Gang and Zhu, Chenguang and Chen, Zheng},
doi = {10.1109/ICDM.2009.29},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Zhu et al. - 2009 - P-packSVM Parallel Primal grAdient desCent Kernel SVM.pdf:pdf},
isbn = {978-1-4244-5242-2},
journal = {2009 Ninth IEEE International Conference on Data Mining},
keywords = {kernel,packing strategy,parallel,stochastic gradient descent,support vector},
month = {dec},
pages = {677--686},
publisher = {Ieee},
title = {{P-packSVM: Parallel Primal grAdient desCent Kernel SVM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5360294},
year = {2009}
}
@misc{svm-dlight,
author = {{Alessio Ceroni}, Paolo Frasconi},
title = {{SVM-Dlight}},
url = {http://www.dsi.unifi.it/neural/src/svm-Dlight/}
}
@article{Rao2015,
abstract = {Low rank matrix completion plays a fundamental role in collaborative filtering applications, the key idea being that the variables lie in a smaller subspace than the ambient space. Often, additional information about the variables is known, and it is reasonable to assume that incorporating this information will lead to better predictions. We tackle the problem of matrix completion when pairwise relationships among variables are known, via a graph. We formulate and derive a highly efficient, conjugate gradient based alternating minimization scheme that solves optimizations with over 55 million observations up to 2 orders of magni-tude faster than state-of-the-art (stochastic) gradient-descent based methods. On the theoretical front, we show that such methods generalize weighted nuclear norm formulations, and derive statistical consistency guarantees. We validate our results on both real and synthetic datasets.},
author = {Rao, Nikhil and Yu, Hsiang-Fu and Ravikumar, Pradeep and Dhillon, Inderjit S},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Rao et al. - 2015 - Collaborative Filtering with Graph Information Consistency and Scalable Methods.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems (to appear)},
pages = {1--9},
title = {{Collaborative Filtering with Graph Information: Consistency and Scalable Methods}},
year = {2015}
}
@article{Neumann2015,
author = {Neumann, Marion and Garnett, Roman and Bauckhage, Christian and Kersting, Kristian},
doi = {10.1007/s10994-015-5517-9},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Neumann et al. - 2015 - Propagation kernels efficient graph kernels from propagated information.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Convolutions,Graph kernels,Learning with graphs,Locality sensitive hashing,Random walks},
month = {jul},
publisher = {Springer US},
title = {{Propagation kernels: efficient graph kernels from propagated information}},
url = {http://link.springer.com/10.1007/s10994-015-5517-9},
year = {2015}
}
@article{Polyvyanyy2011,
author = {Polyvyanyy, Artem and Vanhatalo, Jussi and V{\"{o}}lzer, H},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Polyvyanyy, Vanhatalo, V{\"{o}}lzer - 2011 - Simplified computation and generalization of the refined process structure tree.pdf:pdf},
journal = {Web Services and Formal Methods},
title = {{Simplified computation and generalization of the refined process structure tree}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-19589-1{\_}2},
year = {2011}
}
@techreport{Platt1998,
abstract = {This paper proposes a new algorithm for training support vector machines:
Sequential

Minimal Optimization, or SMO. Training a support vector machine requires
the solution of

a very large quadratic programming (QP) optimization problem. SMO
breaks this large

QP problem into a series of smallest possible QP problems. These small
QP problems are

solved analytically, which avoids using a time-consuming numerical
QP optimization as an

inner loop. The amount of memory required for SMO is linear in the
training set size,

which allows SMO to handle very large training sets. Because matrix
computation is

avoided, SMO scales somewhere between linear and quadratic in the
training set size for

various test problems, while the standard chunking SVM algorithm scales
somewhere

between linear and cubic in the training set size. SMO's computation
time is dominated by

SVM evaluation, hence SMO is fastest for linear SVMs and sparse data
sets. 

On real-world sparse data sets, SMO can be more than 1000 times faster
than the chunking

algorithm.},
author = {Platt, John C},
institution = {Microsoft Research},
number = {MSR-TR-98-14},
title = {{Sequential minimal optimization: A fast algorithm for training support vector machines}},
year = {1998}
}
@article{Li2010,
abstract = {MOTIVATION: Clinical diseases are characterized by distinct phenotypes. To identify disease genes is to elucidate the gene-phenotype relationships. Mutations in functionally related genes may result in similar phenotypes. It is reasonable to predict disease-causing genes by integrating phenotypic data and genomic data. Some genetic diseases are genetically or phenotypically similar. They may share the common pathogenetic mechanisms. Identifying the relationship between diseases will facilitate better understanding of the pathogenetic mechanism of diseases.

RESULTS: In this article, we constructed a heterogeneous network by connecting the gene network and phenotype network using the phenotype-gene relationship information from the OMIM database. We extended the random walk with restart algorithm to the heterogeneous network. The algorithm prioritizes the genes and phenotypes simultaneously. We use leave-one-out cross-validation to evaluate the ability of finding the gene-phenotype relationship. Results showed improved performance than previous works. We also used the algorithm to disclose hidden disease associations that cannot be found by gene network or phenotype network alone. We identified 18 hidden disease associations, most of which were supported by literature evidence.

AVAILABILITY: The MATLAB code of the program is available at http://www3.ntu.edu.sg/home/aspatra/research/Yongjin{\_}BI2010.zip.},
author = {Li, Yongjin and Patra, Jagdish C},
doi = {10.1093/bioinformatics/btq108},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Li, Patra - 2010 - Genome-wide inferring gene-phenotype relationship by walking on the heterogeneous network.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Alzheimer Disease,Alzheimer Disease: genetics,Computational Biology,Computational Biology: methods,Gene Regulatory Networks,Genome-Wide Association Study,Genomics,Genotype,Humans,Models, Biological,Models, Genetic,Models, Statistical,Mutation,Phenotype},
month = {may},
number = {9},
pages = {1219--24},
pmid = {20215462},
title = {{Genome-wide inferring gene-phenotype relationship by walking on the heterogeneous network.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20215462},
volume = {26},
year = {2010}
}
@incollection{NIPS2003_AA29,
address = {Cambridge, MA},
author = {Crammer, Koby and Kandola, Jaz and Singer, Yoram},
booktitle = {Advances in Neural Information Processing Systems 16},
editor = {Thrun, Sebastian and Saul, Lawrence and Sch{\"{o}}lkopf, Bernhard},
keywords = {classification,kernel methods,large margin methods,multi-class learning,online learning},
publisher = {MIT Press},
title = {{Online Classification on a Budget}},
year = {2004}
}
@inproceedings{Aiolli2006,
address = {Los Alamitos, CA, USA},
author = {Aiolli, Fabio and {Da San Martino}, Giovanni and Sperduti, Alessandro and Moschitti, Alessandro},
booktitle = {Proceedings of the 2006 IEEE Conference on Data Mining},
doi = {http://doi.ieeecomputersociety.org/10.1109/ICDM.2006.69},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Aiolli et al. - 2006 - Fast On-line Kernel Learning for Trees.pdf:pdf},
issn = {1550-4786},
pages = {787--791},
publisher = {IEEE Computer Society},
title = {{Fast On-line Kernel Learning for Trees}},
year = {2006}
}
@article{Bai2012,
author = {Bai, Lu and Hancock, Edwin R.},
doi = {10.1007/s10851-012-0383-6},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bai, Hancock - 2012 - Graph Kernels from the Jensen-Shannon Divergence.pdf:pdf},
issn = {0924-9907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {divergence,entropy,jensen-shannon kernel},
month = {sep},
number = {1-2},
pages = {60--69},
title = {{Graph Kernels from the Jensen-Shannon Divergence}},
url = {http://link.springer.com/10.1007/s10851-012-0383-6},
volume = {47},
year = {2012}
}
@article{Hilario2008,
abstract = {Mass-spectra based proteomic profiles have received widespread attention
as potential tools for biomarker discovery and early disease diagnosis.
A major data-analytical problem involved is the extremely high dimensionality
(i.e. number of features or variables) of proteomic data, in particular
when the sample size is small. This article reviews dimensionality
reduction methods that have been used in proteomic biomarker studies.
It then focuses on the problem of selecting the most appropriate
method for a specific task or dataset, and proposes method combination
as a potential alternative to single-method selection. Finally, it
points out the potential of novel dimension reduction techniques,
in particular those that incorporate domain knowledge through the
use of informative priors or causal inference.},
author = {Hilario, Melanie and Kalousis, Alexandros},
doi = {10.1093/bib/bbn005},
journal = {Brief Bioinform},
month = {mar},
number = {2},
pages = {102--118},
title = {{Approaches to dimensionality reduction in proteomic biomarker studies}},
volume = {9},
year = {2008}
}
@article{Sanfilippo2007,
author = {Sanfilippo, Antonio and Posse, Christian and Gopalan, Banu and Riensche, Rick and Beagley, Nathaniel and Baddeley, Bob and Tratz, Stephen and Gregory, Michelle},
journal = {IEEE Trans Nanobioscience},
month = {mar},
number = {1},
pages = {1241--1536},
title = {{Combining hierarchical and associative gene ontology relations with textual evidence in estimating gene and gene product similarity}},
volume = {6},
year = {2007}
}
@article{Huan,
author = {Huan, Jun and Wang, Wei and Prins, Jan},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Huan, Wang, Prins - Unknown - Efficient Mining of Frequent Subgraph in the Presence of Isomorphism.pdf:pdf},
journal = {Matrix},
title = {{Efficient Mining of Frequent Subgraph in the Presence of Isomorphism}}
}
@book{Gartner2008,
author = {G{\"{a}}rtner, Thomas},
isbn = {9812814558},
publisher = {World Scientific Publishing Co Pte Ltd},
title = {{Kernels for Structured Data}},
year = {2008}
}
@article{Deursenvan07,
author = {van Deursen, Ruud and Reymond, Jean-Louis L},
issn = {1860-7187},
journal = {ChemMedChem},
month = {may},
number = {5},
pages = {636--640},
title = {{Chemical space travel.}},
volume = {2},
year = {2007}
}
@article{Oneto2015,
abstract = {We derive in this paper a new Local Rademacher Complexity risk bound on the generalization ability of a model, which is able to take advantage of the availability of unlabeled samples. Moreover, this new bound improves state-of-the-art results even when no unlabeled samples are available.},
author = {Oneto, Luca and Ghio, Alessandro and Ridella, Sandro and Anguita, Davide},
doi = {10.1016/j.neunet.2015.02.006},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Oneto et al. - 2015 - Local Rademacher Complexity sharper risk bounds with and without unlabeled samples.pdf:pdf},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {local,performance estimation,rademacher complexity,statistical learning theory,unlabeled samples},
month = {may},
pages = {115--25},
pmid = {25734890},
title = {{Local Rademacher Complexity: sharper risk bounds with and without unlabeled samples.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25734890},
volume = {65},
year = {2015}
}
@article{Iman1980,
author = {Iman, R L and Davenport, J M},
journal = {Comm. in Stat.},
pages = {571--595},
title = {{Approximation of the critical region of the $\backslash$upshape{\{}F{\}}riedman statistics}},
year = {1980}
}
@article{Kazius2006,
abstract = {Abstract: Substructure mining algorithms are important drug discovery tools since they can find substructures that affect physicochemical and biological properties. Current methods, however, only consider a part of all chemical information that is present within a data set of compounds. Therefore, the overall aim of our study was to enable more exhaustive data mining by designing methods that detect all substructures of any size, shape, and level of chemical detail. A means of chemical representation was developed that uses atomic hierarchies, thus enabling substructure mining to consider general and/or highly specific features. As a proof-of-concept, the efficient, multipurpose graph mining system Gaston learned substructures of any size and shape from a mutagenicity data set that was represented in this manner. From these substructures, we extracted a set of only six nonredundant, discriminative substructures that represent relevant biochemical knowledge. Our results demonstrate the individual and synergistic importance of elaborate chemical representation and mining for nonlinear substructures. We conclude that the combination of elaborate chemical representation and Gaston provides an excellent method for 2D substructure mining as this recipe systematically explores all substructures in different levels of chemical detail.},
address = {Division of Medicinal Chemistry, Leiden-Amsterdam Center for Drug Research, Leiden University, P.O. Box 9502, Einsteinweg 55, 2300 RA Leiden, The Netherlands, and Algorithms Cluster, Leiden Institute of Advanced Computer Science, Leiden University, Niels },
author = {Kazius, J and Nijssen, S and Kok, J and Back, T and Ijzerman, A P},
doi = {10.1021/ci0503715},
journal = {J. Chem. Inf. Model.},
keywords = {cheminformatics,graph,mining},
month = {mar},
number = {2},
pages = {597--605},
title = {{Substructure Mining Using Elaborate Chemical Representation}},
url = {http://dx.doi.org/10.1021/ci0503715},
volume = {46},
year = {2006}
}
@article{Nguyen-tuong2012,
author = {Nguyen-tuong, Duy and Peters, Jan},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Nguyen-tuong, Peters - 2012 - Online Kernel-Based Learning for Task-Space Tracking Robot Control.pdf:pdf},
number = {9},
pages = {1417--1425},
title = {{Online Kernel-Based Learning for Task-Space Tracking Robot Control}},
volume = {23},
year = {2012}
}
@article{Cavallanti:2007:TBH:1296038.1296052,
address = {Hingham, MA, USA},
author = {Cavallanti, Giovanni and Cesa-Bianchi, Nicol{\`{o}} and Gentile, Claudio},
doi = {10.1007/s10994-007-5003-0},
issn = {0885-6125},
journal = {Mach. Learn.},
keywords = {Budget algorithms,Mistake bounds,Pattern classification,Perceptron algorithm},
month = {dec},
number = {2-3},
pages = {143--167},
publisher = {Kluwer Academic Publishers},
title = {{Tracking the best hyperplane with a simple budget Perceptron}},
url = {http://dx.doi.org/10.1007/s10994-007-5003-0},
volume = {69},
year = {2007}
}
@incollection{Kundu2010,
author = {Kundu, Achintya and vikram Tankasali and Bhattacharyya, Chiranjib and Ben-Tal, Aharon},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {Lafferty, J and Williams, C K I and Shawe-Taylor, J and Zemel, R S and Culotta, A},
pages = {1198--1206},
title = {{Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions}},
year = {2010}
}
@inproceedings{anomalydetection,
address = {Miami, FL},
author = {Budhaditya, S and Pham, Duc-Son and Lazarescu, M and Venkatesh, S},
booktitle = {Proc. of the 2009 Internl. Conf. on Data Mining (ICDM'09)},
pages = {722--727},
title = {{Effective Anomaly Detection in Sensor Networks Data Streams}},
year = {2009}
}
@article{Giegerich2004,
abstract = {The function of a non-protein-coding RNA is often determined by its structure. Since experimental determination of RNA structure is time-consuming and expensive, its computational prediction is of great interest, and efficient solutions based on thermodynamic parameters are known. Frequently, however, the predicted minimum free energy structures are not the native ones, leading to the necessity of generating suboptimal solutions. While this can be accomplished by a number of programs, the user is often confronted with large outputs of similar structures, although he or she is interested in structures with more fundamental differences, or, in other words, with different abstract shapes. Here, we formalize the concept of abstract shapes and introduce their efficient computation. Each shape of an RNA molecule comprises a class of similar structures and has a representative structure of minimal free energy within the class. Shape analysis is implemented in the program RNAshapes. We applied RNAshapes to the prediction of optimal and suboptimal abstract shapes of several RNAs. For a given energy range, the number of shapes is considerably smaller than the number of structures, and in all cases, the native structures were among the top shape representatives. This demonstrates that the researcher can quickly focus on the structures of interest, without processing up to thousands of near-optimal solutions. We complement this study with a large-scale analysis of the growth behaviour of structure and shape spaces. RNAshapes is available for download and as an online version on the Bielefeld Bioinformatics Server.},
author = {Giegerich, Robert and Voss, Bj{\"{o}}rn and Rehmsmeier, Marc},
doi = {10.1093/nar/gkh779},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Giegerich, Voss, Rehmsmeier - 2004 - Abstract shapes of RNA.pdf:pdf},
issn = {1362-4962},
journal = {Nucleic acids research},
keywords = {5' Untranslated Regions,5' Untranslated Regions: chemistry,Base Sequence,Computational Biology,Computational Biology: methods,HIV-1,HIV-1: genetics,Humans,Internet,Molecular Sequence Data,Nucleic Acid Conformation,RNA, Small Nuclear,RNA, Small Nuclear: chemistry,RNA, Transfer,RNA, Transfer: chemistry,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Viral,RNA, Viral: chemistry,Software,Terminology as Topic},
month = {jan},
number = {16},
pages = {4843--51},
pmid = {15371549},
title = {{Abstract shapes of RNA.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=519098{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {32},
year = {2004}
}
@inproceedings{Bender2000,
author = {Bender, Michael A and Farach-Colton, Martin},
booktitle = {LATIN},
editor = {Gonnet, Gaston H and Panario, Daniel and Viola, Alfredo},
isbn = {3-540-67306-7},
pages = {88--94},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{The LCA Problem Revisited.}},
volume = {1776},
year = {2000}
}
@article{Eronen2012,
author = {Eronen, L and Toivonen, H},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Eronen, Toivonen - 2012 - Biomine predicting links between biological entities using network models of heterogeneous databases.pdf:pdf},
journal = {BMC bioinformatics},
pages = {1--21},
title = {{Biomine: predicting links between biological entities using network models of heterogeneous databases}},
url = {http://www.biomedcentral.com/1471-2105/13/119/},
year = {2012}
}
@article{Freund1999a,
address = {Hingham, MA, USA},
author = {Freund, Yoav and Schapire, Robert E},
doi = {10.1023/A:1007662407062},
issn = {0885-6125},
journal = {Machine Learning},
month = {dec},
number = {3},
pages = {277--296},
publisher = {Kluwer Academic Publishers},
title = {{Large Margin Classification Using the Perceptron Algorithm}},
volume = {37},
year = {1999}
}
@article{Shi2009,
abstract = {We propose hashing to facilitate efficient kernels. This generalizes
previous work using sampling and we show a principled way to compute
the kernel matrix for data streams and sparse feature spaces. Moreover,
we give deviation bounds from the exact kernel matrix. This has applications
to estimation on strings and graphs.},
author = {Shi, Qinfeng and Petterson, James and Dror, Gideon and Langford, John and Smola, Alex and Vishwanathan, S V N},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Shi et al. - 2009 - Hash Kernels for Structured Data(2).pdf:pdf},
journal = {Journal of Machine Learning Research},
month = {nov},
pages = {2615--2637},
title = {{Hash Kernels for Structured Data}},
volume = {10},
year = {2009}
}
@inproceedings{Bifet2008,
address = {New York, NY, USA},
author = {Bifet, A and Gavald{\`{a}}, R},
booktitle = {Proceeding of the 14th ACM SIGKDD Internl. Conf. on Knowl. Disc. and data mining},
pages = {34--42},
publisher = {ACM},
series = {KDD '08},
title = {{Mining adaptively frequent closed unlabeled rooted trees in data streams}},
year = {2008}
}
@article{McHugh2000,
author = {McHugh, J},
journal = {ACM Trans. on Inform. and Sys. Sec.},
number = {4},
pages = {262--294},
title = {{Testing intrusion detection systems: a critique of the 1998 and 1999 darpa intrusion detection system evaluations as performed by Lincoln laboratory}},
volume = {3},
year = {2000}
}
@inproceedings{Airiau2003,
author = {Airiau, St{\'{e}}phane and Sen, Sandip and Wolpert, David and Tumer, Kagan},
booktitle = {Engineering Self-Organising Systems},
editor = {{Di Marzo Serugendo}, Giovanna and Karageorgos, Anthony and Rana, Omer F and Zambonelli, Franco},
isbn = {3-540-21201-9},
pages = {249--264},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{Providing Effective Access to Shared Resources: A COIN Approach}},
volume = {2977},
year = {2003}
}
@article{Ikeda2006,
abstract = {How we should choose a kernel function in support vector machines
{\{}(SVMs),{\}} is an important but difficult problem. In this paper, we
discuss the properties of the solution of the {\{}Nu-SVM's,{\}} a variation
of {\{}SVM's,{\}} for normalized feature vectors in two extreme cases:
All feature vectors are almost orthogonal and all feature vectors
are almost the same. In the former case, the solution of the {\{}Nu-SVM{\}}
is nearly the center of gravity of the examples given while the solution
is approximated to that of the {\{}Nu-SVM{\}} with the linear kernel in
the latter case. Although extreme kernels are not employed in practice,
analyzes are helpful to understand the effects of a kernel function
on the generalization performance.},
author = {Ikeda, Kazushi},
doi = {10.1109/TNN.2005.860832},
issn = {1045-9227},
journal = {Neural Networks, {\{}IEEE{\}} Transactions on},
keywords = {Asymptotic{\_}properties,extreme,feature{\_}vector,generalization{\_}ability,kernel functions,kernel method,nu support vector machines,parameter estimation,support vector machine {\{}(SVM),{\}} support vector machines},
number = {1},
pages = {1--9},
title = {{Effects of kernel function on Nu support vector machines in extreme cases}},
volume = {17},
year = {2006}
}
@article{Wilusz2009,
abstract = {Most of the eukaryotic genome is transcribed, yielding a complex network of transcripts that includes tens of thousands of long noncoding RNAs with little or no protein-coding capacity. Although the vast majority of long noncoding RNAs have yet to be characterized thoroughly, many of these transcripts are unlikely to represent transcriptional "noise" as a significant number have been shown to exhibit cell type-specific expression, localization to subcellular compartments, and association with human diseases. Here, we highlight recent efforts that have identified a myriad of molecular functions for long noncoding RNAs. In some cases, it appears that simply the act of noncoding RNA transcription is sufficient to positively or negatively affect the expression of nearby genes. However, in many cases, the long noncoding RNAs themselves serve key regulatory roles that were assumed previously to be reserved for proteins, such as regulating the activity or localization of proteins and serving as organizational frameworks of subcellular structures. In addition, many long noncoding RNAs are processed to yield small RNAs or, conversely, modulate how other RNAs are processed. It is thus becoming increasingly clear that long noncoding RNAs can function via numerous paradigms and are key regulatory molecules in the cell.},
author = {Wilusz, Jeremy E and Sunwoo, Hongjae and Spector, David L},
doi = {10.1101/gad.1800909},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Wilusz, Sunwoo, Spector - 2009 - Long noncoding RNAs functional surprises from the RNA world.pdf:pdf},
issn = {1549-5477},
journal = {Genes {\&} development},
keywords = {Animals,Gene Expression Regulation,Open Reading Frames,Protein Binding,RNA Precursors,RNA Processing, Post-Transcriptional,RNA, Untranslated,RNA, Untranslated: chemistry,RNA, Untranslated: metabolism},
month = {jul},
number = {13},
pages = {1494--504},
pmid = {19571179},
title = {{Long noncoding RNAs: functional surprises from the RNA world.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3152381{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {23},
year = {2009}
}
@article{Macskassy2007,
abstract = {This paper is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classification in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classifiers comprise a local classifier, a relational classifier, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classification, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classification in machine learning benchmark data sets. The results demonstrate that very simple network-classification models perform quite well--well enough that they should be used regularly as baseline classifiers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes--that is, Gaussian-field classifiers, Hopfield networks, and relational-neighbor classifiers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection.},
author = {Macskassy, Sofus a and Provost, Foster},
doi = {10.1021/jf901618z},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {collective classification,collective inference,network analysis,network data,network learning,networked data,probabilistic relational models,relational learning},
number = {December 2004},
pages = {935--983},
pmid = {19705801},
title = {{Classification in Networked Data : A Toolkit and a Univariate Case Study}},
volume = {8},
year = {2007}
}
@incollection{Massimo2016,
author = {Massimo, Carlo M. and Navarin, Nicol{\`{o}} and Sperduti, Alessandro},
booktitle = {Neural Information Processing: 23rd International Conference, ICONIP 2016, Kyoto, Japan, October 16--21, 2016, Proceedings, Part II},
doi = {10.1007/978-3-319-46672-9_25},
editor = {Hirose, Akira and and Ozawa, Seiichi and and Doya, Kenji and and Ikeda, Kazushi and and Lee, Minho and and Liu, Derong},
isbn = {978-3-319-46672-9},
pages = {214--223},
publisher = {Springer International Publishing},
title = {{Hyper-Parameter Tuning for Graph Kernels via Multiple Kernel Learning}},
url = {http://link.springer.com/10.1007/978-3-319-46672-9{\_}25},
year = {2016}
}
@book{Cormen2009,
abstract = {Essential reading for all students of European cultural history, and a key text for Marxism in the post-Communist world of the late twentieth century.},
archivePrefix = {arXiv},
arxivId = {0712.0689},
author = {Cormen, Thomas and Leiserson, Charles and Rivest, Ronald and Stein, Clifford},
booktitle = {Contemporary Sociology},
doi = {10.2307/2077150},
eprint = {0712.0689},
isbn = {1859840566},
issn = {00943061},
number = {4},
pages = {590},
pmid = {24809974},
title = {{Introduction to Algorithms}},
volume = {25},
year = {2009}
}
@article{Broder2000,
abstract = {The mathematical concept of document resemblance cap- tures well the informal notion of syntactic similarity. The resemblance can be estimated using a fixed size “sketch” for each document. For a large collection of documents (say hundreds of millions) the size of this sketch is of the order of a few hundred bytes per document. However, for efficient large scale web indexing it is not necessary to de- termine the actual resemblance value: it suffices to determine whether newly encountered documents are duplicates or near-duplicates of docu- ments already indexed. In other words, it suffices to determine whether the resemblance is above a certain threshold. In this talk we show how this determination can be made using a ”sample” of less than 50 bytes per document. The basic approach for computing resemblance has two aspects: first, resemblance is expressed as a set (of strings) intersection problem, and second, the relative size of intersections is evaluated by a process of random sampling that can be done independently for each document. The process of estimating the relative size of intersection of sets and the thresholdtest discussedabove canbe appliedtoarbitrary sets, and thus might be of independent interest. The algorithm for filtering near-duplicate documents discussed here has been successfully implemented and has been used for the last three years in the context of the AltaVista search engine.},
author = {Broder, A.},
doi = {10.1007/3-540-45123-4_1},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Broder - 2000 - Identifying and filtering near-duplicate documents.pdf:pdf},
isbn = {3540676333},
issn = {16113349},
journal = {Combinatorial Pattern Matching},
pages = {1--10},
title = {{Identifying and filtering near-duplicate documents}},
url = {http://www.springerlink.com/index/KTN21YJUL3R379XY.pdf},
year = {2000}
}
@article{Hocevar2014,
abstract = {MOTIVATION: Small-induced subgraphs called graphlets are emerging as a possible tool for exploration of global and local structure of networks and for analysis of roles of individual nodes. One of the obstacles to their wider use is the computational complexity of algorithms for their discovery and counting.$\backslash$n$\backslash$nRESULTS: We propose a new combinatorial method for counting graphlets and orbit signatures of network nodes. The algorithm builds a system of equations that connect counts of orbits from graphlets with up to five nodes, which allows to compute all orbit counts by enumerating just a single one. This reduces its practical time complexity in sparse graphs by an order of magnitude as compared with the existing pure enumeration-based algorithms.$\backslash$n$\backslash$nAVAILABILITY AND IMPLEMENTATION: Source code is available freely at http://www.biolab.si/supp/orca/orca.html.$\backslash$n$\backslash$nCONTACT: tomaz.hocevar@fri.uni-lj.si$\backslash$n$\backslash$nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
author = {Ho{\v{c}}evar, Toma{\v{z}} and Dem{\v{s}}ar, Janez},
doi = {10.1093/bioinformatics/btt717},
isbn = {1460-2059},
issn = {13674803},
journal = {Bioinformatics},
number = {4},
pmid = {24336411},
title = {{A combinatorial approach to graphlet counting}},
volume = {30},
year = {2014}
}
@article{Elenberg2015,
abstract = {We study the problem of approximating the {\$}3{\$}-profile of a large graph. {\$}3{\$}-profiles are generalizations of triangle counts that specify the number of times a small graph appears as an induced subgraph of a large graph. Our algorithm uses the novel concept of {\$}3{\$}-profile sparsifiers: sparse graphs that can be used to approximate the full {\$}3{\$}-profile counts for a given large graph. Further, we study the problem of estimating local and ego {\$}3{\$}-profiles, two graph quantities that characterize the local neighborhood of each vertex of a graph. Our algorithm is distributed and operates as a vertex program over the GraphLab PowerGraph framework. We introduce the concept of edge pivoting which allows us to collect {\$}2{\$}-hop information without maintaining an explicit {\$}2{\$}-hop neighborhood list at each vertex. This enables the computation of all the local {\$}3{\$}-profiles in parallel with minimal communication. We test out implementation in several experiments scaling up to {\$}640{\$} cores on Amazon EC2. We find that our algorithm can estimate the {\$}3{\$}-profile of a graph in approximately the same time as triangle counting. For the harder problem of ego {\$}3{\$}-profiles, we introduce an algorithm that can estimate profiles of hundreds of thousands of vertices in parallel, in the timescale of minutes.},
archivePrefix = {arXiv},
arxivId = {1506.06671},
author = {Elenberg, Ethan R. and Shanmugam, Karthikeyan and Borokhovich, Michael and Dimakis, Alexandros G.},
doi = {10.1145/2783258.2783413},
eprint = {1506.06671},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
title = {{Beyond Triangles: A Distributed Framework for Estimating 3-profiles of Large Graphs}},
year = {2015}
}
@inproceedings{Bai2015,
author = {Bai, Lu and Rossi, Luca and Zhang, Zhihong and Hancock, Edwin R},
booktitle = {ICML},
file = {:Users/nick/Library/Application Support/Mendeley Desktop/Downloaded/Bai et al. - 2015 - An Aligned Subtree Kernel for Weighted Graphs.pdf:pdf},
pages = {30--39},
title = {{An Aligned Subtree Kernel for Weighted Graphs}},
year = {2015}
}
@misc{Nickel2016,
abstract = {Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be "trained" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.},
archivePrefix = {arXiv},
arxivId = {1503.00759},
author = {Nickel, Maximilian and Murphy, Kevin and Tresp, Volker and Gabrilovich, Evgeniy},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2015.2483592},
eprint = {1503.00759},
isbn = {1089801300},
issn = {00189219},
number = {1},
title = {{A review of relational machine learning for knowledge graphs}},
volume = {104},
year = {2016}
}
@article{Helma2004a,
abstract = {This paper explores the utility of data mining and machine learning algorithms for the induction of mutagenicity structure-activity relationships (SARs) from noncongeneric data sets. We compare (i) a newly developed algorithm (MOLFEA) for the generation of descriptors (molecular fragments) for noncongeneric compounds with traditional SAR approaches (molecular properties) and (ii) different machine learning algorithms for the induction of SARs from these descriptors. In addition we investigate the optimal parameter settings for these programs and give an exemplary interpretation of the derived models. The predictive accuracies of models using MOLFEA derived descriptors is approximately 10-15{\%}age points higher than those using molecular properties alone. Using both types of descriptors together does not improve the derived models. From the applied machine learning techniques the rule learner PART and support vector machines gave the best results, although the differences between the learning algorithms are only marginal. We were able to achieve predictive accuracies up to 78{\%} for 10-fold cross-validation. The resulting models are relatively easy to interpret and usable for predictive as well as for explanatory purposes.},
author = {Helma, Christoph and Cramer, Tobias and Kramer, Stefan and {De Raedt}, Luc},
doi = {10.1021/ci034254q},
isbn = {0095-2338},
issn = {00952338},
journal = {Journal of Chemical Information and Computer Sciences},
number = {4},
pages = {1402--1411},
pmid = {15272848},
title = {{Data mining and machine learning techniques for the identification of mutagenicity inducing substructures and structure activity relationships of noncongeneric compounds}},
volume = {44},
year = {2004}
}
@inproceedings{Wale2006,
abstract = {In recent years the development of computational techniques that build models to correctly assign chemical compounds to various classes or to retrieve potential drug-like compounds has been an active area of research. Many of the best-performing techniques for these tasks utilize a descriptor-based representation of the compound that captures various aspects of the underlying molecular graph's topology. In this paper we compare different set of descriptors that are currently used for chemical compound classification. In this process, we also introduce four different descriptors derived from all connected fragments present in the molecular graphs. In addition, we introduce an extension to existing vector-based kernel functions to take into account the length of the fragments present in the descriptors. We experimentally evaluate the performance of the previously introduced and the new descriptors in the context of SVM-based classification and ranked-retrieval on 28 classification and retrieval problems derived from 18 datasets. Our experiments show that for both these tasks, the new descriptors consistently and statistically outperform previously developed schemes based on the widely used fingerprint- and Maces keys-based descriptors, as well as recently introduced descriptors obtained by mining and analyzing the structure of the molecular graphs.},
author = {Wale, Nikil and Karypis, George},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDM.2006.39},
isbn = {0769527019},
issn = {15504786},
pages = {678--689},
title = {{Comparison of descriptor spaces for chemical compound retrieval and classification}},
year = {2006}
}
@article{Weislow1989a,
abstract = {We have developed an effective and optimally safe microculture method for rapid and convenient assay of the in vitro cytopathic effects of human immunodeficiency virus (HIV-1) on human lymphoblastoid or other suitable host cells. The assay procedure is applicable to the evaluation of drug effects on in vitro infections induced directly in cultured host cells by cell-free HIV-1 or by coculture with H9 cells chronically infected with HIV-1. The assay uses a newly developed tetrazolium reagent that is metabolically reduced by viable cells to yield a soluble, colored formazan product measurable by conventional colorimetric techniques. This simple microassay minimizes the number of plate manipulations typically required with other assay methods and, coupled with computerized data collection and analysis, facilitates large-scale screening of agents for potential antiviral activity. To support and enhance the discovery of new anti-HIV-1 agents, the National Cancer Institute is offering investigators worldwide the opportunity to submit new candidate agents for anti-HIV-1 screening with this method.},
author = {Weislow, Owen S. and Kiser, Rebecca and Fine, Donald L. and Bader, John and Shoemaker, Robert H. and Boyd, Michael R.},
doi = {10.1093/jnci/81.8.577},
isbn = {0027-8874 (Print)$\backslash$r0027-8874 (Linking)},
issn = {00278874},
journal = {Journal of the National Cancer Institute},
number = {8},
pages = {577--586},
pmid = {2495366},
title = {{New soluble-formazan assay for HIV-1 cytopathic effects: Application to high-flux screening of synthetic and natural products for AIDS-antiviral activity}},
volume = {81},
year = {1989}
}
