\documentclass{esannV2}
\usepackage{subfig}
\usepackage{color}
\usepackage{url}
 
% My packages
\usepackage{ctable} % for \specialrule command
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usepackage{filecontents}
%\usepackage{amsfonts}
%\usepackage{booktabs}
%\usepackage{amsthm}
\usepackage{amssymb,amsmath,array}
%\usepackage{color}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{multirow}
%\usepackage{array}
%\usepackage{subfig}
%\usepackage{caption}
%\usepackage{graphicx}
%\usepackage{hyperref}
%\usepackage{algorithmicx}
%\usepackage{float}
%\usepackage{soul}
%\usepackage{amsfonts}
%\usepackage{subfigure}
\usepackage{amsbsy}
%\usepackage{setspace}
%\usepackage{hyperref}
%\usepackage[bookmarks=false]{hyperref}
%\usepackage{babel,blindtext}
\usepackage{color}

% My commands
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Natu}{\mathbb{N}}
\newcommand{\dd}{{\bf d}}
\newcommand{\xx}{{\bf x}}
\newcommand{\yy}{{\bf y}}
\newcommand{\zz}{{\bf z}}
\newcommand{\ff}{{\bf f}}
\newcommand{\ww}{{\bf w}}
\newcommand{\vv}{{\bf v}}
\newcommand{\AAA}{{\bf A}}
\newcommand{\CC}{{\bf C}}
\newcommand{\DD}{{\bf D}}
\newcommand{\EE}{{\bf E}}
\newcommand{\GG}{{\bf G}}
\newcommand{\KK}{{\bf K}}
\newcommand{\YY}{{\bf Y}}
\newcommand{\MM}{{\bf M}}
\newcommand{\HH}{{\bf H}}
\newcommand{\II}{{\bf I}}
\newcommand{\XX}{{\bf X}}
%\newcommand{\NN}{{\bf N}}
\newcommand{\cU}{{\cal U}}
\newcommand{\cI}{{\cal I}}
\newcommand{\cR}{{\cal R}}
\newcommand{\cN}{{\cal N}}
\newcommand{\cQ}{{\mathcal Q}}
\newcommand{\1}{{\bf 1}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\labelfun}{L}
\newcommand{\node}{v}
\newcommand{\cK}{{\cal K}}
%\newtheorem{lemma}{Lemma}
%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}{Corollary}
%\newtheorem{proposition}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{remark}{Remark}
\newcommand{\cC}{{\cal C}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\ggamma}{\pmb{\gamma}}
\newcommand{\aalpha}{\pmb{\alpha}}
\newcommand{\bbeta}{\pmb{\beta}}
\newcommand{\eeta}{\pmb{\eta}}
\newcommand{\ssigma}{\pmb{\sigma}}
\newcommand{\llambda}{\pmb{\lambda}}
\newcommand{\mmu}{\pmb{\mu}}
\newcommand{\pphi}{\pmb{\phi}}
\newcommand{\res}[2]{$#1_{\pm#2}$}

%***********************************************************************
% !!!! IMPORTANT NOTICE ON TEXT MARGINS !!!!!
%***********************************************************************
%
% Please avoid using DVI2PDF or PS2PDF converters: some undesired
% shifting/scaling may occur when using these programs
% It is strongly recommended to use the DVIPS converters, and to submit
% PS file. You may submit a PDF file if and only if you use ADOBE ACROBAT
% to convert your PS file to PDF.
%
% Check that you have set the paper size to A4 (and NOT to letter) in your
% dvi2ps converter, in Adobe Acrobat if you use it, and in any printer driver
% that you could use.  You also have to disable the 'scale to fit paper' option
% of your printer driver.
%
% In any case, please check carefully that the final size of the top and
% bottom margins is 5.2 cm and of the left and right margins is 4.4 cm.
% It is your responsibility to verify this important requirement.  If these margin requirements and not fulfilled at the end of your file generation process, please use the following commands to correct them.  Otherwise, please do not modify these commands.
%
\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}

%***********************************************************************
% !!!! USE OF THE esannV2 LaTeX STYLE FILE !!!!!
%***********************************************************************
%
% Some commands are inserted in the following .tex example file.  Therefore to
% set up your ESANN submission, please use this file and modify it to insert
% your text, rather than staring from a blank .tex file.  In this way, you will
% have the commands inserted in the right place.



\begin{document}
%style file for ESANN manuscripts
\title{Title}

%***********************************************************************
% AUTHORS INFORMATION AREA
%***********************************************************************
%\author{First author$^1$ and Second author$^2$
\author{A$^1$, B$^2$ and C$^3$
%
% Optional short acknowledgment: remove next line if non-needed
%\thanks{This is an optional funding source acknowledgement.}
%
% DO NOT MODIFY THE FOLLOWING '\vspace' ARGUMENT
\vspace{.3cm}\\
%
% Addresses and institutions (remove "1- " in case of a single institution)
1- Department of Mathematics, University of Padova\\
via Trieste 63, Padova, Italy
\vspace{.1cm}\\
2- Computational Statistics and Machine Learning (CSML) \\
Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy
\vspace{.1cm}\\
3- ... \\
...
}

%commento sta roba
%1- School of First Author - Dept of First Author \\
%Address of First Author's school - Country of First Author's
%school
%
% Remove the next three lines in case of a single institution
%\vspace{.1cm}\\
%2- School of Second Author - Dept of Second Author \\
%Address of Second Author's school - Country of Second Author's school\\
%}
%***********************************************************************
% END OF AUTHORS INFORMATION AREA
%***********************************************************************

\maketitle

%\renewcommand{\baselinestretch}{.95}

\begin{abstract}
...
\end{abstract}


\section{Introduction}
In different real-world domains, it may be easy to encode examples in a structured form. For example, in chemoinformatics there is a well known graph representation of the secondary structures of chemical compounds. Moreover, images can easily be represented as graph after segmentation, where adjacent objects (or regions) are linked.
In these cases, specific machine learning techniques have to be adopted.
Kernel methods are one option, where it suffices to adopt a kernel function defined for the kind of data under consideration. Then, standard (kernelized) learning algorithms (e.g. SVM) can be adopted.

With kernel methods, a user has usually to deal with a number of hyper-parameters to fix at hand. The optimization of these hyper-parameters can strongly affect the predictive performance of the method.
Usually, these parameters are: (i) the choice of the kernel to adopt; (ii) the hyper-parameters of the selected kernel; (iii)the hyper-parameters of the learning algorithm.
A commonly adopted and simple approach to hyper-parameters optimization is to fix a-priori some possible values for each parameter (the combinations of these paraeters are referres to as \textit{parameters grid}).
Then, the learning algorithm is trained for each possible parameters combination, and its performances are evaluated. This approach is commonly referred to as grid-search.


The learning algorithm has to be trained for each hyper-parameter combination, and this procedure may be very time consuming.
A possible alleviation to this problem has been proposed in~\cite{Massimo2016}, where Multiple Kernel Learning has been used for combining different kernel matrices in a single learning procedure, instead of selecting the best parameters configuration.
In the case of kernel algorithms, a different kernel matrix has to be computed for each kernel and for each of their hyper-parameters combination. This step can also be time consuming, especially when dealing with data in a structured form.
In this paper, we propose a new approach to parameter selection that, based on sub-sampling and Multiple Kernel Learning, is able to speedup the parameter selection phase by orders of manitude on real-world datasets.

%\section{Kernel for structures and Multiple Kernel Learning}
\section{Backgroung}
When dealing with structured data, the choice of the kernel to adopt is a key step. Depending on the type of data under consideration, different proposals are available in literature, each one with a different trade-off between predictive performance and computational time (of course, this trade-off depends from the considered task).
In this paper we focus on graphs, i.e. we consider each example to be a different graph.
In this case, there are several graph kernels defined in literature based on random walks~\cite{Kashima2003,Mah'e2004} ($O(n^3)$ computational complexity), shortest paths~\cite{Borgwardt2005} ($O(n^4)$ computational complexity), subgraphs up to a fixed size~\citep{Shervashidze2009} . The problem with these kernels is the high computational complexity, that makes them inapplicable to several real-world datasets.
Recently, different almost linear time graph kernels have been defined in literature \citep{Heinonen2009,Shervashidze2009a,Costa2010,Shervashidze2011,Dasan2012,DaSanMartino2016}, and these are the ones we focus o in this paper.
In the following we will give some details about the considered alternatives.
\textcolor{red}{TODO}\\
\begin{itemize}
 \item WL
 \item NSPDK
 \item ODD$_{ST}$
\end{itemize}

\subsection{Multiple Kernel Learning (MKL)}
\label{MKL}
MKL \cite{Bach2004,Gonen2011} is one of the most popular paradigms used to learn kernels in real world applications \cite{Bucak2014,Castro2014}. % Zien2007,Wang2015a,Sun
The kernels generated by these techniques are combinations of a prescribed set of basic kernels $\KK_1,...,\KK_R$ with a constraint in the form:
$
	H_R^q = \{ \xx \mapsto \textbf{w} \cdot \pphi_\KK(\xx) : \KK = \sum_{r=1}^R \eta_r \KK_r, \mmu \in \Psi_q, \|\textbf{w}\|_2 \leqslant 1 \}
$
with $\Psi_q = \{ \mmu : \mmu \succcurlyeq 0, \| \mmu \|_q = 1 \}$ and considering the function $\pphi_\KK$ as the feature mapping from the input space to the feature space. The value $q$ being the kind of mean used, is typically fixed to $1$ or $2$.

Using this formulation, we are studying the family of sums of kernels in the feature space. It is well know that the sum of two kernels can be seen as the concatenation of the features contained in both the RKHS \cite{Shawe-Taylor2004}. Extending the same idea, the weighted sum of a list of basic kernels can be seen as a weighted concatenation of all the features contained in all the RKHS (where the weights are the square roots of the learned weights $\eta_k$).

 %structured sparsity regularizers \cite{Micchelli2010,Micchelli2013}.
The problem of searching for a combinations of basic kernels can be rephrased as a regularization problem in which the prediction function is the sum of functions $f_r$ in the RKHS of kernel $\KK_r$ and the regularization term is the combination of the norms of the $f_r$ in the associated RKHS \cite{Micchelli2005a}. %,Micchelli2007
For example if $q=1$ this is a parametric version of the group Lasso \cite{Meier2009}, see e.g. \cite{Maurer2012}.

These algorithms are supported by several theoretical results that bound the \emph{estimation error} (i.e. the difference between the true error and the empirical margin error). These bounds exploit the \emph{Rademacher complexity} applied to the combination of kernels \cite{Maurer2012,Srebro2006,Cortes2009c,Hussain2011,Hussain2011a}. %,Kakade2009,Micchelli2005,Kloft2012

Existing MKL approaches can be divided in two main categories. In the first category, \emph{Fixed or Heuristic}, some fixed rule is applied to obtain the combination. They usually get results scalable with respect to the number of kernels combined but their effectiveness critically depends on the domain at hand. They use a parameterized combination function and find the parameters of this function generally by looking at some measure obtained from each kernel separately,  often giving a suboptimal solution (since no information sharing among the kernels is exploited).

On the other hand, the \emph{Optimization based} approaches learn the combination parameters by solving a single optimization problem directly integrated in the learning machine (e.g. structural risk based target function) or formulated as a different model (e.g. alignment, or other kernel similarity maximization) \cite{Rakotomamonjy2008,Bach2004,Varma2009}. %,Kloft2011,SorenSonnenburg2006,Vishwanathan2010,Xu2010

%*** good refs. [F. R. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 4:1?106, 2011.] \cite{Bach2012} ***

\subsubsection{EasyMKL}
\label{EasyMKL}
EasyMKL \cite{Aiolli2015} is a recent MKL algorithm able to combine sets of basic kernels by solving a simple quadratic optimization problem. Besides its proved empirical effectiveness, a clear advantage of EasyMKL compared to other MKL methods is its high scalability with respect to the number of kernels to be combined. Specifically, its computational complexity is constant in memory and linear in time.

EasyMKL finds the coefficients $\boldsymbol{\eta}$ that maximize the margin on the training set, where the margin is computed as the distance between the convex hulls of positive and negative examples. In particular, the general problem EasyMKL tries to optimize is the following:
\vspace{-0.4cm}
\begin{equation}
\label{eq:easymklotig}
\max_{\boldsymbol{\eta}:||\boldsymbol{\eta}||_2=1} \min_{\boldsymbol{\gamma} \in \Gamma} \mbox { }  \boldsymbol{\gamma}^{\top} \YY ( \sum_{r=0}^R \eta_r \KK_r) \YY \boldsymbol{\gamma} + \lambda ||\boldsymbol{\gamma}||^2.
\end{equation}
\vspace{-0.3cm}

where $\YY$ is a diagonal matrix with training labels on the diagonal, and $\lambda$ is a regularization hyper-parameter. The domain $\Gamma$ represents two probability distributions over the set of positive and negative examples of the training set, that is $\Gamma = \{\boldsymbol{\gamma} \in \Real_+^{\ell} \hspace{0.2cm} |  \sum_{y_i = +1} \boldsymbol{\gamma}_i = 1 , \sum_{y_i = -1} \boldsymbol{\gamma}_i = 1\}.
$
Note that any element $\boldsymbol{\gamma} \in \Gamma$ corresponds to a pair of points, the first in the convex hull of positive training examples and the second in the convex hull of negative training examples.
At the solution, the first term of the objective function represents the obtained margin, that is the (squared) distance between a point in the convex hull of positive examples and a point in the convex hull of negative examples, in the compounded feature space.

The objective function in Eq. \ref{eq:easymklotig} can be interpreted as the dual problem of a regularized empirical objective function using the kernel $\sum_{r=1}^R \eta_r \KK_r$. This equation is a minimax problem that can be reduced  to a simple quadratic problem with a technical derivation described  in \cite{Aiolli2015}. The solution of the quadratic problem is an optimal $\ggamma^*$ for the original min-max formulation. Due to the particular structure of EasyMKL, it is sufficient to provide the average kernel of all the basic kernels ($\KK^A = \frac{1}{R} \sum_{r=1}^R \frac{\KK_r}{Tr(\KK_r)}$). From $\ggamma^*$, it is easy to obtain the optimal weights for the single basic kernels $\KK_r$ by using the following formula
\begin{equation}
\label{eq:easyeta}
	\eta_r = \gamma^{*T} \YY \,\, (\KK_r/Tr(\KK_r)) \,\, \YY \gamma^*, \,\,\,\, \forall r=1,\dots,R.
\end{equation}

In the following sections, we will refer to this algorithm as EasyMKL\footnote{EasyMKL implementation: github.com/jmikko/EasyMKL}.
%and we have to provide it only $\KK^A$, the dataset $\XX$ and $\yy$ to evaluate equation \ref{eq:easyeta}, and the EasyMKL regularization hyper-parameter $\Lambda$.


\section{Our framework (devo finire di sistemare)}
In several problems solved using MKL paradigm, the number of kernels used can be very high and, when we use structured data as input, such as graph, the computational effort to calculare each of them can be expensive. Moreover each kernel requires an amount of memory that depends on the number of samples, making the MKL approach unpractical when the number of samples and the number of kernels are huge.

In order to avoid this problem, we propose a method to reduce the number of kernel matrices calculated, based on the robustness of weights vector $\eta$ over the number of training samples used.\\
In particular if $\eta$, learned using the whole set of data, is similar to the weights vector $\eta_s$ learned using the same kernels calculated on a subset of data, then we can use directly $\eta_s$ in our model.
In this way we can learn quickly the kernels combination, but we have to calculate each of them anyway.
We want to take a step forward by using $\eta_s$ to perform a kernels selection, aiming to build the complete model using only the top kernels.\\
The top kernels are those with the best margin-contribution defined in $\eta_s$.

In the second phase, we calculate the top kernel matrices using the whole set of data, and then we use them in combination. The kernels discarded can be considered as noise and irrelevant in combination, so their elimination [???].
In this way we calculate all kernels using a subset of data, and a subset of kernels using the whole training data.


\section{Experimental Results (devo finire di sistemare)}
Firstly, we performed some experiment to check the base of our method. In particular we want to observe how many samples are required to make $\eta$ and $\eta_s$ similar.
\figurename\ \ref{fig:weights} shows the weights vector obtained by EasyMKL with different number of training samples.

\begin{figure}[htb]
\includegraphics[width=\textwidth]{img/weights_GDD.png}
\caption{The weights vector learned by EasyMKL using $50\%$ and $25\%$ of training data. Blue regions represents the difference with the weights vector calculated on all data. }
\label{weights}
\end{figure}

However, in this context, two weights vectors are similar if the top kernels are the same. The exact (rescaled eventually) values are useless, we have to chose the same kernels using a subset of data.
The similarity metric used for comparison is the jaccard similarity score.
\figurename\ \ref{fig:jaccard} shows the jaccard score by considering different percentage of top kernels and different number of training data used.

\begin{figure}[htb]
\centering
\subfloat[a]{\includegraphics[width=0.42\textwidth]{img/CPDB_jaccard.png}}
\subfloat[b]{\includegraphics[width=0.42\textwidth]{img/GDD_jaccard.png}}
\caption{Average jaccard values between the indices of top kernels using a subset of training data and using the whole set.}
\label{fig:jaccard}
\end{figure}

The datasets used in our experiments are: [???]

In the first step of our method, we learn the weights vector $\eta_{10}$ using a subset $(10\%)$ of training data and the whole set of kernels. To do this, we perform a 10-fold cross validation using EasyMKL to combine kernels and SVM as base learner. The hyperparameters are $(\lambda,C)$. We chose the top $10\%$ kernels using the weights vector with the best result in the cv procedure.\\
In the latter step we calculate the complete matrix relative to the top kernels selected and use these kernels to fit the model. In particular we divided our approach in 3 sub-methods:
\begin{itemize}
\item{}use the EasyMKL algorithm to re-clculate the kernel with top kernels;
\item{}calculate the resulting kernel as the average;
\item{}use the same weights (rescaled) from the first learning step.
\end{itemize}
For testing, we compared our method with 3 hard baselines, that are:
\begin{itemize}
\item{}validation: we calculate all kernel matrices and select which has the best results in validation;
\item{}average: we calculate the kernel as the average of all base kernels;
\item{}combined: we use EasyMKL to combine the whole set of kernels;
\end{itemize}
In all baselined and in the latter training step of each proposed methods, we use a stratified nested 10-fold cross-validation to chose the best hyperparameters sets, that are $\lambda$ for models where we use EasyMKL and $C$ for all baselines and methods since we always use a SVM as base learner.


\subsection{Results}
[...]
\begin{table}[]
\centering
\caption{My caption}
\label{my-label}
\resizebox{\textwidth}{!}{
\begin{tabular}{l|c|lll|lll}
\hline
dataset & \# train & validation      & all (easy) & all (average) & subset\_easy & subset\_aver & subset\_w \\
\hline
CPDB    & 684 &$0.8753_{\pm0.029}$ & $0.8814_{\pm0.027}$ & $0.8809_{\pm0.029}$ & $0.8768_{\pm0.023}$ & $0.8850_{\pm0.027}$ & $0.8751_{\pm0.023}$ \\
\\
MUTAG   & 188 & \res{0.9023}{0.075} & \res{0.9310}{0.061} & \res{0.9309}{0.060} & \res{0.9292}{0.054} & \res{0.9292}{0.054} & \res{0.9331}{0.053}    \\
\\
CAS     & 4337 & \res{0.9189}{0.012} & - & \res{0.9209}{0.012} & \res{0.9195}{0.11} & \res{0.9161}{0.012} & \res{0.9185}{0.012}    \\
\\
GDD     & 1178 & \res{0.8647}{0.045} & \res{0.8719}{0.039} & \res{0.8675}{0.039} & \res{0.869}{0.041} & \res{0.8713}{0.043} & \res{0.8011}{0.036}    \\
\\
AIDS    & 1503 & \res{0.7797}{0.083} & \res{0.7775}{0.080} & \res{0.7712}{0.083} & \res{0.7845}{0.077} & \res{0.7690}{0.074} & \res{0.7673}{0.073}    \\
\\
NCI1    & 4110 & \res{0.8839}{0.043} & - & \res{0.8851}{0.041} & \res{0.8818}{0.043} & \res{0.8772}{0.047} & \res{0.8811}{0.044}    \\
\\
NCI109 & 4127 & \res{0.8728}{0.025} & - & \res{0.8713}{0.025} & \res{0.8670}{0.024}  & \res{0.8713}{0.025} & \res{0.8680}{0.022}  \\
\\
\hline
\end{tabular}}
\end{table}

\section{Conclusion}
...

% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************

\begin{footnotesize}

% IF YOU DO NOT USE BIBTEX, USE THE FOLLOWING SAMPLE SCHEME FOR THE REFERENCES
% ----------------------------------------------------------------------------

% ----------------------------------------------------------------------------

% IF YOU USE BIBTEX,
% - DELETE THE TEXT BETWEEN THE TWO ABOVE DASHED LINES
% - UNCOMMENT THE NEXT TWO LINES AND REPLACE 'Name_Of_Your_BibFile'

\bibliographystyle{unsrt}
\bibliography{donini,biblio}

\end{footnotesize}

% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************

\end{document}





