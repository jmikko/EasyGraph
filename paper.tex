\documentclass{esannV2}
\usepackage{subfig}
\usepackage{color}
\usepackage{url}

% My packages
\usepackage{ctable} % for \specialrule command
%\usepackage{tikz}
%\usepackage{pgfplots}
%\usepackage{filecontents}
%\usepackage{amsfonts}
%\usepackage{booktabs}
%\usepackage{amsthm}
\usepackage{amssymb,amsmath,array}
%\usepackage{color}
%\usepackage{algorithm}
%\usepackage{algorithmic}
%\usepackage{multirow}
%\usepackage{array}
%\usepackage{subfig}
%\usepackage{caption}
%\usepackage{graphicx}
%\usepackage{hyperref}
%\usepackage{algorithmicx}
%\usepackage{float}
%\usepackage{soul}
%\usepackage{amsfonts}
%\usepackage{subfigure}
\usepackage{amsbsy}
%\usepackage{setspace}
%\usepackage{hyperref}
%\usepackage[bookmarks=false]{hyperref}
%\usepackage{babel,blindtext}
\usepackage{color}

% My commands
\newcommand{\Real}{\mathbb{R}}
\newcommand{\Natu}{\mathbb{N}}
\newcommand{\dd}{{\bf d}}
\newcommand{\xx}{{\bf x}}
\newcommand{\yy}{{\bf y}}
\newcommand{\zz}{{\bf z}}
\newcommand{\ff}{{\bf f}}
\newcommand{\ww}{{\bf w}}
\newcommand{\vv}{{\bf v}}
\newcommand{\AAA}{{\bf A}}
\newcommand{\CC}{{\bf C}}
\newcommand{\DD}{{\bf D}}
\newcommand{\EE}{{\bf E}}
\newcommand{\GG}{{\bf G}}
\newcommand{\KK}{{\bf K}}
\newcommand{\YY}{{\bf Y}}
\newcommand{\MM}{{\bf M}}
\newcommand{\HH}{{\bf H}}
\newcommand{\II}{{\bf I}}
\newcommand{\XX}{{\bf X}}
%\newcommand{\NN}{{\bf N}}
\newcommand{\cU}{{\cal U}}
\newcommand{\cI}{{\cal I}}
\newcommand{\cR}{{\cal R}}
\newcommand{\cN}{{\cal N}}
\newcommand{\cQ}{{\mathcal Q}}
\newcommand{\1}{{\bf 1}}
\newcommand{\Exp}{\mathbb{E}}
\newcommand{\labelfun}{L}
\newcommand{\node}{v}
\newcommand{\cK}{{\cal K}}
%\newtheorem{lemma}{Lemma}
%\newtheorem{theorem}{Theorem}
%\newtheorem{corollary}{Corollary}
%\newtheorem{proposition}{Proposition}
%\newtheorem{definition}{Definition}
%\newtheorem{remark}{Remark}
\newcommand{\cC}{{\cal C}}
\newcommand{\R}{{\mathbb R}}
\newcommand{\N}{{\mathbb N}}
\newcommand{\ggamma}{\pmb{\gamma}}
\newcommand{\aalpha}{\pmb{\alpha}}
\newcommand{\bbeta}{\pmb{\beta}}
\newcommand{\eeta}{\pmb{\eta}}
\newcommand{\ssigma}{\pmb{\sigma}}
\newcommand{\llambda}{\pmb{\lambda}}
\newcommand{\mmu}{\pmb{\mu}}
\newcommand{\pphi}{\pmb{\phi}}

%***********************************************************************
% !!!! IMPORTANT NOTICE ON TEXT MARGINS !!!!!
%***********************************************************************
%
% Please avoid using DVI2PDF or PS2PDF converters: some undesired
% shifting/scaling may occur when using these programs
% It is strongly recommended to use the DVIPS converters, and to submit
% PS file. You may submit a PDF file if and only if you use ADOBE ACROBAT
% to convert your PS file to PDF.
%
% Check that you have set the paper size to A4 (and NOT to letter) in your
% dvi2ps converter, in Adobe Acrobat if you use it, and in any printer driver
% that you could use.  You also have to disable the 'scale to fit paper' option
% of your printer driver.
%
% In any case, please check carefully that the final size of the top and
% bottom margins is 5.2 cm and of the left and right margins is 4.4 cm.
% It is your responsibility to verify this important requirement.  If these margin requirements and not fulfilled at the end of your file generation process, please use the following commands to correct them.  Otherwise, please do not modify these commands.
%
\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}

%***********************************************************************
% !!!! USE OF THE esannV2 LaTeX STYLE FILE !!!!!
%***********************************************************************
%
% Some commands are inserted in the following .tex example file.  Therefore to
% set up your ESANN submission, please use this file and modify it to insert
% your text, rather than staring from a blank .tex file.  In this way, you will
% have the commands inserted in the right place.



\begin{document}
%style file for ESANN manuscripts
\title{Title}

%***********************************************************************
% AUTHORS INFORMATION AREA
%***********************************************************************
%\author{First author$^1$ and Second author$^2$
\author{A$^1$, B$^2$ and C$^3$
%
% Optional short acknowledgment: remove next line if non-needed
%\thanks{This is an optional funding source acknowledgement.}
%
% DO NOT MODIFY THE FOLLOWING '\vspace' ARGUMENT
\vspace{.3cm}\\
%
% Addresses and institutions (remove "1- " in case of a single institution)
1- Department of Mathematics, University of Padova\\
via Trieste 63, Padova, Italy
\vspace{.1cm}\\
2- Computational Statistics and Machine Learning (CSML) \\
Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy
\vspace{.1cm}\\
3- ... \\
...
}

%commento sta roba
%1- School of First Author - Dept of First Author \\
%Address of First Author's school - Country of First Author's
%school
%
% Remove the next three lines in case of a single institution
%\vspace{.1cm}\\
%2- School of Second Author - Dept of Second Author \\
%Address of Second Author's school - Country of Second Author's school\\
%}
%***********************************************************************
% END OF AUTHORS INFORMATION AREA
%***********************************************************************

\maketitle

%\renewcommand{\baselinestretch}{.95}

\begin{abstract}
...
\end{abstract}


\section{Introduction}
In different real-world domains, it may be easy to encode examples in a structured form. For example, in chemoinformatics there is a well known graph representation of the secondary structures of chemical compounds. Moreover, images can easily be represented as graph after segmentation, where adjacent objects (or regions) are liked.
In these cases, specific machine learning techniques have to be adopted.
Kernel methods are one option, where it suffices to adopt a kernel function defined for the kind of data under consideration. Then, standard (kernelized) learning algorithms (e.g. SVM) can be adopted.

With kernel methods, a user has usually to deal with a number of hyper-parameters to fix at hand. The values of these hyper-parameters can strongly affect the predictive performance of the method.
Usually, these parameters are:
\begin{enumerate}
 \item the choice of the kernel to adopt;
 \item the hyper-parameters of the selected kernel;
 \item the hyper-parameters of the learning algorithm.
\end{enumerate}
One commonly adopted and simple approach to fix these parameters is to fix a-priori some possible values for each parameter, and to test each possible combination on a validation set. This approach is commonly referred to as grid-search (from the parameter grid).
Then, only the best combination is used to predict the test set, in order to produce an unbiased error estimation of the method (if necessary).
In particular, for each point in the grid, a learning procedure have to be computed. In the case of kernel algorithms, a different kernel matrix has to be computed for the hyper-parameters at points 1 and 2.
This procedure may be very time consuming.
A possible alleviation to this problem has been proposed in~\cite{Massimo2016}, where Multiple Kernel Learning has been used for combining different kernel matrices in a single learning procedure, instead of selecting the best parameters configuration.
In this paper, we propose a new approach to parameter selection that, based on sub-sampling and Multiple Kernel Learning, is able to speedup the parameter selection phase by orders of manitude on real-world datasets.

\section{Kernel for structures and Multiple Kernel Learning}
When dealing with structured data, the choice of the kernel to adopt is a key step. Depending on the type of data under consideration, different proposals are available in literature, each one with a different trade-off between predictive performance and computational time (of course, this trade-off depends from the considered task).
In this paper we focus on graphs, i.e. we consider each example to be a different graph.
In this case, there are several graph kernels defined in literature based on random walks, shortest paths or subgraphs up to a fixed size[TODO]. The problem with these kernels is the high computational complexity, that makes them inapplicable to several real-world datasets.
Recently, different almost linear time graph kernels have been defined in literature, and these are the ones we focus o in this paper.
In the following we will give some details about the considered alternatives.
\textcolor{red}{TODO}\\
\begin{itemize}
 \item WL
 \item NSPDK
 \item ODD$_{ST}$
\end{itemize}

\subsection{Multiple Kernel Learning (MKL)}
\label{MKL}
MKL \cite{Bach2004,Gonen2011} is one of the most popular paradigms used to learn kernels in real world applications \cite{Bucak2014,Castro2014}. % Zien2007,Wang2015a,Sun
The kernels generated by these techniques are combinations of a prescribed set of basic kernels $\KK_1,...,\KK_R$ with a constraint in the form:
$
	H_R^q = \{ \xx \mapsto \textbf{w} \cdot \pphi_\KK(\xx) : \KK = \sum_{r=1}^R \eta_r \KK_r, \mmu \in \Psi_q, \|\textbf{w}\|_2 \leqslant 1 \}
$
with $\Psi_q = \{ \mmu : \mmu \succcurlyeq 0, \| \mmu \|_q = 1 \}$ and considering the function $\pphi_\KK$ as the feature mapping from the input space to the feature space. The value $q$ being the kind of mean used, is typically fixed to $1$ or $2$.

Using this formulation, we are studying the family of sums of kernels in the feature space. It is well know that the sum of two kernels can be seen as the concatenation of the features contained in both the RKHS \cite{Shawe-Taylor2004}. Extending the same idea, the weighted sum of a list of basic kernels can be seen as a weighted concatenation of all the features contained in all the RKHS (where the weights are the square roots of the learned weights $\eta_k$).

 %structured sparsity regularizers \cite{Micchelli2010,Micchelli2013}.
The problem of searching for a combinations of basic kernels can be rephrased as a regularization problem in which the prediction function is the sum of functions $f_r$ in the RKHS of kernel $\KK_r$ and the regularization term is the combination of the norms of the $f_r$ in the associated RKHS \cite{Micchelli2005a}. %,Micchelli2007
For example if $q=1$ this is a parametric version of the group Lasso \cite{Meier2009}, see e.g. \cite{Maurer2012}.

These algorithms are supported by several theoretical results that bound the \emph{estimation error} (i.e. the difference between the true error and the empirical margin error). These bounds exploit the \emph{Rademacher complexity} applied to the combination of kernels \cite{Maurer2012,Srebro2006,Cortes2009c,Hussain2011,Hussain2011a}. %,Kakade2009,Micchelli2005,Kloft2012

Existing MKL approaches can be divided in two main categories. In the first category, \emph{Fixed or Heuristic}, some fixed rule is applied to obtain the combination. They usually get results scalable with respect to the number of kernels combined but their effectiveness critically depends on the domain at hand. They use a parameterized combination function and find the parameters of this function generally by looking at some measure obtained from each kernel separately,  often giving a suboptimal solution (since no information sharing among the kernels is exploited).

On the other hand, the \emph{Optimization based} approaches learn the combination parameters by solving a single optimization problem directly integrated in the learning machine (e.g. structural risk based target function) or formulated as a different model (e.g. alignment, or other kernel similarity maximization) \cite{Rakotomamonjy2008,Bach2004,Varma2009}. %,Kloft2011,SorenSonnenburg2006,Vishwanathan2010,Xu2010

%*** good refs. [F. R. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning, 4:1?106, 2011.] \cite{Bach2012} ***

\subsubsection{EasyMKL}
\label{EasyMKL}
EasyMKL \cite{Aiolli2015} is a recent MKL algorithm able to combine sets of basic kernels by solving a simple quadratic optimization problem. Besides its proved empirical effectiveness, a clear advantage of EasyMKL compared to other MKL methods is its high scalability with respect to the number of kernels to be combined. Specifically, its computational complexity is constant in memory and linear in time.

EasyMKL finds the coefficients $\boldsymbol{\eta}$ that maximize the margin on the training set, where the margin is computed as the distance between the convex hulls of positive and negative examples. In particular, the general problem EasyMKL tries to optimize is the following:
\vspace{-0.4cm}
\begin{equation}
\label{eq:easymklotig}
\max_{\boldsymbol{\eta}:||\boldsymbol{\eta}||_2=1} \min_{\boldsymbol{\gamma} \in \Gamma} \mbox { }  \boldsymbol{\gamma}^{\top} \YY ( \sum_{r=0}^R \eta_r \KK_r) \YY \boldsymbol{\gamma} + \lambda ||\boldsymbol{\gamma}||^2.
\end{equation}
\vspace{-0.3cm}

where $\YY$ is a diagonal matrix with training labels on the diagonal, and $\lambda$ is a regularization hyper-parameter. The domain $\Gamma$ represents two probability distributions over the set of positive and negative examples of the training set, that is $\Gamma = \{\boldsymbol{\gamma} \in \Real_+^{\ell} \hspace{0.2cm} |  \sum_{y_i = +1} \boldsymbol{\gamma}_i = 1 , \sum_{y_i = -1} \boldsymbol{\gamma}_i = 1\}.
$
Note that any element $\boldsymbol{\gamma} \in \Gamma$ corresponds to a pair of points, the first in the convex hull of positive training examples and the second in the convex hull of negative training examples.
At the solution, the first term of the objective function represents the obtained margin, that is the (squared) distance between a point in the convex hull of positive examples and a point in the convex hull of negative examples, in the compounded feature space.

The objective function in Eq. \ref{eq:easymklotig} can be interpreted as the dual problem of a regularized empirical objective function using the kernel $\sum_{r=1}^R \eta_r \KK_r$. This equation is a minimax problem that can be reduced  to a simple quadratic problem with a technical derivation described  in \cite{Aiolli2015}. The solution of the quadratic problem is an optimal $\ggamma^*$ for the original min-max formulation. Due to the particular structure of EasyMKL, it is sufficient to provide the average kernel of all the basic kernels ($\KK^A = \frac{1}{R} \sum_{r=1}^R \frac{\KK_r}{Tr(\KK_r)}$). From $\ggamma^*$, it is easy to obtain the optimal weights for the single basic kernels $\KK_r$ by using the following formula
\begin{equation}
\label{eq:easyeta}
	\eta_r = \gamma^{*T} \YY \,\, (\KK_r/Tr(\KK_r)) \,\, \YY \gamma^*, \,\,\,\, \forall r=1,\dots,R.
\end{equation}

In the following sections, we will refer to this algorithm as EasyMKL\footnote{EasyMKL implementation: github.com/jmikko/EasyMKL}.
%and we have to provide it only $\KK^A$, the dataset $\XX$ and $\yy$ to evaluate equation \ref{eq:easyeta}, and the EasyMKL regularization hyper-parameter $\Lambda$.


\section{Our framework}
...

\section{Experimental Results}
...

\section{Conclusion}
...

% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************

\begin{footnotesize}

% IF YOU DO NOT USE BIBTEX, USE THE FOLLOWING SAMPLE SCHEME FOR THE REFERENCES
% ----------------------------------------------------------------------------

% ----------------------------------------------------------------------------

% IF YOU USE BIBTEX,
% - DELETE THE TEXT BETWEEN THE TWO ABOVE DASHED LINES
% - UNCOMMENT THE NEXT TWO LINES AND REPLACE 'Name_Of_Your_BibFile'

\bibliographystyle{unsrt}
\bibliography{donini,biblio}

\end{footnotesize}

% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************

\end{document}





