\documentclass{esannV2}
\usepackage{subfig}
\usepackage{color}
\usepackage{url}

%***********************************************************************
% !!!! IMPORTANT NOTICE ON TEXT MARGINS !!!!!
%***********************************************************************
%
% Please avoid using DVI2PDF or PS2PDF converters: some undesired
% shifting/scaling may occur when using these programs
% It is strongly recommended to use the DVIPS converters, and to submit
% PS file. You may submit a PDF file if and only if you use ADOBE ACROBAT
% to convert your PS file to PDF.
%
% Check that you have set the paper size to A4 (and NOT to letter) in your
% dvi2ps converter, in Adobe Acrobat if you use it, and in any printer driver
% that you could use.  You also have to disable the 'scale to fit paper' option
% of your printer driver.
%
% In any case, please check carefully that the final size of the top and
% bottom margins is 5.2 cm and of the left and right margins is 4.4 cm.
% It is your responsibility to verify this important requirement.  If these margin requirements and not fulfilled at the end of your file generation process, please use the following commands to correct them.  Otherwise, please do not modify these commands.
%
\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}

%***********************************************************************
% !!!! USE OF THE esannV2 LaTeX STYLE FILE !!!!!
%***********************************************************************
%
% Some commands are inserted in the following .tex example file.  Therefore to
% set up your ESANN submission, please use this file and modify it to insert
% your text, rather than staring from a blank .tex file.  In this way, you will
% have the commands inserted in the right place.



\begin{document}
%style file for ESANN manuscripts
\title{Title}

%***********************************************************************
% AUTHORS INFORMATION AREA
%***********************************************************************
%\author{First author$^1$ and Second author$^2$
\author{A$^1$, B$^2$ and C$^3$
%
% Optional short acknowledgment: remove next line if non-needed
%\thanks{This is an optional funding source acknowledgement.}
%
% DO NOT MODIFY THE FOLLOWING '\vspace' ARGUMENT
\vspace{.3cm}\\
%
% Addresses and institutions (remove "1- " in case of a single institution)
1- Department of Mathematics, University of Padova\\
via Trieste 63, Padova, Italy
\vspace{.1cm}\\
2- Computational Statistics and Machine Learning (CSML) \\
Istituto Italiano di Tecnologia, Via Morego 30, Genova, Italy
\vspace{.1cm}\\
3- ... \\
...
}

%commento sta roba
%1- School of First Author - Dept of First Author \\
%Address of First Author's school - Country of First Author's
%school
%
% Remove the next three lines in case of a single institution
%\vspace{.1cm}\\
%2- School of Second Author - Dept of Second Author \\
%Address of Second Author's school - Country of Second Author's school\\
%}
%***********************************************************************
% END OF AUTHORS INFORMATION AREA
%***********************************************************************

\maketitle

%\renewcommand{\baselinestretch}{.95}

\begin{abstract}
...
\end{abstract}


\section{Introduction}
In different real-world domains, it may be easy to encode examples in a structured form. For example, in chemoinformatics there is a well known graph representation of the secondary structures of chemical compounds. Moreover, images can easily be represented as graph after segmentation, where adjacent objects (or regions) are liked.
In these cases, specific machine learning techniques have to be adopted.
Kernel methods are one option, where it suffices to adopt a kernel function defined for the kind of data under consideration. Then, standard (kernelized) learning algorithms (e.g. SVM) can be adopted.

With kernel methods, a user has usually to deal with a number of hyper-parameters to fix at hand. The values of these hyper-parameters can strongly affect the predictive performance of the method.
Usually, these parameters are:
\begin{enumerate}
 \item the choice of the kernel to adopt;
 \item the hyper-parameters of the selected kernel;
 \item the hyper-parameters of the learning algorithm.
\end{enumerate}
One commonly adopted and simple approach to fix these parameters is to fix a-priori some possible values for each parameter, and to test each possible combination on a validation set. This approach is commonly referred to as grid-search (from the parameter grid).
Then, only the best combination is used to predict the test set, in order to produce an unbiased error estimation of the method (if necessary).
In particular, for each point in the grid, a learning procedure have to be computed. In the case of kernel algorithms, a different kernel matrix has to be computed for the hyper-parameters at points 1 and 2.
This procedure may be very time consuming.
A possible alleviation to this problem has been proposed in~\cite{Massimo2016}, where Multiple Kernel Learning has been used for combining different kernel matrices in a single learning procedure, instead of selecting the best parameters configuration.
In this paper, we propose a new approach to parameter selection that, based on sub-sampling and Multiple Kernel Learning, is able to speedup the parameter selection phase by orders of manitude on real-world datasets.
\section{Kernel for structures and Multiple Kernel Learning}
When dealing with structured data, the choice of the kernel to adopt is a key step. Depending on the type of data under consideration, different proposals are available in literature, each one with a different trade-off between predictive performance and computational time (of course, this trade-off depends from the considered task).
In this paper we focus on graphs, i.e. we consider each example to be a different graph.
In this case, there are several graph kernels defined in literature based on random walks, shortest paths or subgraphs up to a fixed size[TODO]. The problem with these kernels is the high computational complexity, that makes them inapplicable to several real-world datasets.
Recently, different almost linear time graph kernels have been defined in literature, and these are the ones we focus o in this paper.
In the following we will give some details about the considered alternatives.
\textcolor{red}{TODO}\\
\begin{itemize}
 \item WL
 \item NSPDK
 \item ODD$_{ST}$
\end{itemize}


...

\section{Our framework}
...

\section{Experimental Results}
...

\section{Conclusion}
...

% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************

\begin{footnotesize}

% IF YOU DO NOT USE BIBTEX, USE THE FOLLOWING SAMPLE SCHEME FOR THE REFERENCES
% ----------------------------------------------------------------------------

% ----------------------------------------------------------------------------

% IF YOU USE BIBTEX,
% - DELETE THE TEXT BETWEEN THE TWO ABOVE DASHED LINES
% - UNCOMMENT THE NEXT TWO LINES AND REPLACE 'Name_Of_Your_BibFile'

\bibliographystyle{unsrt}
\bibliography{biblio.bib}

\end{footnotesize}

% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************

\end{document}





